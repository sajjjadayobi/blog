<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Text Classification with Albert-Persian | Sajjad Ayoubiâ€™s Blog ğŸ˜‰</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Text Classification with Albert-Persian" />
<meta name="author" content="Sajjad Ayoubi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Text Classification with Transformers-LMs in TF 2.X &amp; PyTorch" />
<meta property="og:description" content="Text Classification with Transformers-LMs in TF 2.X &amp; PyTorch" />
<link rel="canonical" href="https://sajjjadayobi.github.io/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html" />
<meta property="og:url" content="https://sajjjadayobi.github.io/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html" />
<meta property="og:site_name" content="Sajjad Ayoubiâ€™s Blog ğŸ˜‰" />
<meta property="og:image" content="https://sajjjadayobi.github.io/blog/images/albert.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-23T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://sajjjadayobi.github.io/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html","@type":"BlogPosting","headline":"Text Classification with Albert-Persian","dateModified":"2021-01-23T00:00:00-06:00","datePublished":"2021-01-23T00:00:00-06:00","image":"https://sajjjadayobi.github.io/blog/images/albert.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://sajjjadayobi.github.io/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html"},"author":{"@type":"Person","name":"Sajjad Ayoubi"},"description":"Text Classification with Transformers-LMs in TF 2.X &amp; PyTorch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sajjjadayobi.github.io/blog/feed.xml" title="Sajjad Ayoubi's Blog ğŸ˜‰" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Sajjad Ayoubi&#39;s Blog ğŸ˜‰</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Text Classification with Albert-Persian</h1><p class="page-description">Text Classification with Transformers-LMs in TF 2.X &amp; PyTorch</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-23T00:00:00-06:00" itemprop="datePublished">
        Jan 23, 2021
      </time>â€¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Sajjad Ayoubi</span></span>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          
          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/sajjjadayobi/blog/blob/master/_notebooks/2021-01-23-Text-Classification-Albert.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#What's-ALBERT-Persian?">What&#39;s ALBERT-Persian? </a></li>
<li class="toc-entry toc-h1"><a href="#Dataset-(DigiMag)">Dataset (DigiMag) </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Load">Load </a></li>
<li class="toc-entry toc-h2"><a href="#Normalization-(Preprocessing)">Normalization (Preprocessing) </a>
<ul>
<li class="toc-entry toc-h3"><a href="#display-the-frequent-words-in-the-train-set">display the frequent words in the train set </a></li>
<li class="toc-entry toc-h3"><a href="#Finding-the-max-len">Finding the max len </a></li>
<li class="toc-entry toc-h3"><a href="#Distribution-of-Classes">Distribution of Classes </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Prepare-data-for-Albert-LM">Prepare data for Albert-LM </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Training">Training </a>
<ul>
<li class="toc-entry toc-h2"><a href="#PyTorch">PyTorch </a></li>
<li class="toc-entry toc-h2"><a href="#Tensorflow">Tensorflow </a></li>
<li class="toc-entry toc-h2"><a href="#How-can-we-achieve-better-results">How can we achieve better results </a></li>
<li class="toc-entry toc-h2"><a href="#HuggingFace-ğŸ¤—">HuggingFace ğŸ¤— </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#In-The-End">In The End </a></li>
</ul><body>
<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-23-Text-Classification-Albert.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What's-ALBERT-Persian?">
<a class="anchor" href="#What's-ALBERT-Persian?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What's ALBERT-Persian?<a class="anchor-link" href="#What's-ALBERT-Persian?"> </a>
</h2>
<p>A Lite BERT for Self-supervised Learning of Language Representations for the Persian Language<br></p>
<ul>
<li>thanks <a href="https://github.com/m3hrdadfi">Mehrdad Farahani</a> &amp; <a href="https://hooshvare.com/">Hoshvare</a> for sharing this
<a href="https://github.com/m3hrdadfi/albert-persian">albert-persian repo</a>
</li>
</ul>
<p>ALBERT-Persian trained on a massive amount of public corpora (Persian Wikidumps, MirasText) and six other manually crawled text data from a various type of websites (BigBang Page scientific, Chetor lifestyle, Eligasht itinerary, Digikala, Ted Talks general conversational, Books novels, storybooks, short stories from old to the contemporary era).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Dataset-(DigiMag)">
<a class="anchor" href="#Dataset-(DigiMag)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset (DigiMag)<a class="anchor-link" href="#Dataset-(DigiMag)"> </a>
</h1>
<ul>
<li>For this notebook, I'm going to use DigiMag dataset for text classification<ul>
<li>train len: <strong>6865</strong>  , valid len:<strong>767</strong> , test len: <strong>852</strong>
</li>
<li>it has <strong>7</strong> types for Magazines (7 classes)</li>
<li>thanks <a href="https://hooshvare.com/">Hooshvare</a> for sharing this</li>
</ul>
</li>
</ul>
<ul>
<li>this is an example of how to use (You can use whatever dataset you have)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>the training was on Google Colab</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Thu Feb 18 05:45:45 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   32C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install -q sentencepiece
<span class="o">!</span>pip install -q transformers
<span class="o">!</span>pip install -q tokenizers
<span class="c1"># for text processing</span>
<span class="o">!</span>pip -q install hazm
<span class="o">!</span>pip -q install clean-text<span class="o">[</span>gpl<span class="o">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 5.9MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 6.1MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 40.2MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 38.7MB/s 
  Building wheel for sacremoses (setup.py) ... done
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 5.0MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 5.4MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 17.9MB/s 
  Building wheel for nltk (setup.py) ... done
  Building wheel for libwapiti (setup.py) ... done
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 3.3MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 7.0MB/s 
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 6.9MB/s 
  Building wheel for ftfy (setup.py) ... done
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span> 

<span class="kn">from</span> <span class="nn">hazm</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="kn">from</span> <span class="nn">hazm</span> <span class="kn">import</span> <span class="n">WordTokenizer</span>
<span class="kn">from</span> <span class="nn">cleantext</span> <span class="kn">import</span> <span class="n">clean</span>
<span class="kn">import</span> <span class="nn">re</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Load">
<a class="anchor" href="#Load" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load<a class="anchor-link" href="#Load"> </a>
</h2>
<ul>
<li>the dataset is <a href="https://bit.ly/3ca4bm8">here</a> on Drive (open it with VPN)</li>
<li>add it on your drive and use it like the following</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/gdrive'</span><span class="p">)</span>
<span class="o">%</span><span class="k">cd</span> /gdrive/MyDrive
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Mounted at /gdrive
/gdrive/MyDrive
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>unzip digimag.zip
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Archive:  digimag.zip
   creating: digimag/
  inflating: digimag/dev.csv         
  inflating: digimag/train.csv       
  inflating: digimag/test.csv        
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>load train, test, and valid with Pandas</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'digimag/train.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">"	"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'digimag/dev.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">"	"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'digimag/test.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">"	"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># drop the label columns</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Unnamed: 0'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">eval_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Unnamed: 0'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Unnamed: 0'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>content</th>
      <th>label_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Ù†Ù…Ø§ÛŒØ´ ØªØ¨Ù„ÛŒØº Ø¯Ø± Ù„Ø§Ú©â€ŒØ§Ø³Ú©Ø±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Ø´Ú©Ø³Øª Justice League Ø¯Ø± Ø¨Ø§Ú©Ø³ Ø¢ÙÛŒØ³ Ù¾Ø³ Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯...</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Ú©Ù„Ø§Ø³ÛŒÚ© Ø¨ÛŒÙ†ÛŒØ› Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¯Ø± ÛŒÚ© Ø´Ø¨ Ø§ØªÙØ§Ù‚ Ø§ÙØªØ§Ø¯ ÙÛŒÙ„Ù…...</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Ø§Ù¾Ù„ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø³Ø±Ø§Øº Ø±Ù†Ø¯Ù‡ Ø±ÙØªÙ‡ Ú†Ø±Ø§Ú©Ù‡ Ø¢Ù¾Ú¯Ø±ÛŒØ¯ Ú©Ø±Ø¯Ù† Ø³Ø·...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø²Ø¡ Ø¨Ù‡ Ø¬Ø²Ø¡ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ori and the Blind ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Normalization-(Preprocessing)">
<a class="anchor" href="#Normalization-(Preprocessing)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalization (Preprocessing)<a class="anchor-link" href="#Normalization-(Preprocessing)"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our cleaning method includes these steps:</p>
<ul>
<li>fixing unicodes</li>
<li>removing specials like a phone number, email, url, new lines, ...</li>
<li>cleaning HTMLs</li>
<li>normalizer</li>
<li>tokenize and detokenize (for adding space)</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">WordTokenizer</span><span class="p">(</span><span class="n">join_verb_parts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">replace_hashtags</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">replace_IDs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">(</span><span class="n">remove_extra_spaces</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">persian_numbers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">persian_style</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">punctuation_spacing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">remove_diacritics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">affix_spacing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">token_based</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cleaning</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">clean</span><span class="p">(</span><span class="n">text</span><span class="p">,</span>
        <span class="n">fix_unicode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">to_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_urls</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_emails</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_phone_numbers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_numbers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_currency_symbols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_punct</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">replace_with_url</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
        <span class="n">replace_with_email</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
        <span class="n">replace_with_phone_number</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
        <span class="n">replace_with_number</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
        <span class="n">replace_with_currency_symbol</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span>
    
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'([Ø§-ÛŒ])\1{2,}'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'\1'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span> <span class="c1"># bbb+ -&gt; b at least 2 char</span>
    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">text_preprocessor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">cleaning</span><span class="p">(</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">tokenize</span> <span class="k">else</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_df</span><span class="p">[</span><span class="s1">'cleaned'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'test cleaned'</span><span class="p">)</span>
<span class="n">eval_df</span><span class="p">[</span><span class="s1">'cleaned'</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'valid cleaned'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>test cleaned
valid cleaned
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_df</span><span class="p">[</span><span class="s1">'cleaned'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'train cleaned'</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">[</span><span class="s1">'tokens'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train cleaned
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>content</th>
      <th>label_id</th>
      <th>cleaned</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Ù†Ù…Ø§ÛŒØ´ ØªØ¨Ù„ÛŒØº Ø¯Ø± Ù„Ø§Ú©â€ŒØ§Ø³Ú©Ø±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡...</td>
      <td>3</td>
      <td>Ù†Ù…Ø§ÛŒØ´ ØªØ¨Ù„ÛŒØº Ø¯Ø± Ù„Ø§Ú© Ø§Ø³Ú©Ø±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ú¯ÙˆØ´ÛŒ Ù‡Ø§ÛŒ Ù‡...</td>
      <td>[Ù†Ù…Ø§ÛŒØ´, ØªØ¨Ù„ÛŒØº, Ø¯Ø±, Ù„Ø§Ú©, Ø§Ø³Ú©Ø±ÛŒÙ†, ØªØ¹Ø¯Ø§Ø¯ÛŒ, Ø§Ø², Ú¯Ùˆ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Ø´Ú©Ø³Øª Justice League Ø¯Ø± Ø¨Ø§Ú©Ø³ Ø¢ÙÛŒØ³ Ù¾Ø³ Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯...</td>
      <td>5</td>
      <td>Ø´Ú©Ø³Øª justice league Ø¯Ø± Ø¨Ø§Ú©Ø³ Ø¢ÙÛŒØ³ Ù¾Ø³ Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯...</td>
      <td>[Ø´Ú©Ø³Øª, justice, league, Ø¯Ø±, Ø¨Ø§Ú©Ø³, Ø¢ÙÛŒØ³, Ù¾Ø³, Ø§Ø²...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Ú©Ù„Ø§Ø³ÛŒÚ© Ø¨ÛŒÙ†ÛŒØ› Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¯Ø± ÛŒÚ© Ø´Ø¨ Ø§ØªÙØ§Ù‚ Ø§ÙØªØ§Ø¯ ÙÛŒÙ„Ù…...</td>
      <td>5</td>
      <td>Ú©Ù„Ø§Ø³ÛŒÚ© Ø¨ÛŒÙ†ÛŒ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¯Ø± ÛŒÚ© Ø´Ø¨ Ø§ØªÙØ§Ù‚ Ø§ÙØªØ§Ø¯ ÙÛŒÙ„Ù… ...</td>
      <td>[Ú©Ù„Ø§Ø³ÛŒÚ©, Ø¨ÛŒÙ†ÛŒ, Ù‡Ù…Ù‡, Ú†ÛŒØ², Ø¯Ø±, ÛŒÚ©, Ø´Ø¨, Ø§ØªÙØ§Ù‚, Ø§Ù...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="display-the-frequent-words-in-the-train-set">
<a class="anchor" href="#display-the-frequent-words-in-the-train-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>display the frequent words in the train set<a class="anchor-link" href="#display-the-frequent-words-in-the-train-set"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">word_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_df</span><span class="o">.</span><span class="n">tokens</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">word_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'words'</span><span class="p">])</span>
<span class="n">top</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">'words'</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'number of words'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">top</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">top</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="mi">30</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">top</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">30</span><span class="p">]);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>number of words 80154
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgMAAAKrCAYAAABslI7UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZQldX338feHZdiGYWZYBgV0QBAVZIltDKI5Y+KCiEEUCUbzgMIziiiYEJ8YzZMYk2hiohESjE5CFIHHqCwxLggqIaKHaFowKkTZXMDIvgyjwrB8nz9uDTbtNDPDTN+6dev9OqcP91bVvfdTp5rpT//q11WpKiRJUn9t1HYASZLULsuAJEk9ZxmQJKnnLAOSJPWcZUCSpJ7bpO0Abdluu+1q8eLFbceQJGlovv71r99aVdtPX97bMrB48WImJyfbjiFJ0tAk+cHqlnuaQJKknuvtyMD9t9zOLX9/ZtsxJEn6Bdsf96qhfp4jA5Ik9ZxlQJKknrMMSJLUc5YBSZJ6zjIgSVLPWQYkSeo5y4AkST3XqzKQZGmSySSTt61Y3nYcSZJGQq/KQFUtq6qJqprYdu68tuNIkjQSxq4MJPmrJN9PcmLbWSRJ6oKxKgNJFgKvAfYF3pBkq5YjSZI08saqDFTV7cB5wJXALsD8dhNJkjT6xqoMAFTVscAzgJ8CP245jiRJI2+sykCSjZLsD3waOKWqHmw7kyRJo25sbmGc5GXAXwDXAydX1YdajiRJUieMTRmoqnOAc9rOIUlS14xNGVhXm2y/kO2Pe1XbMSRJat1YzRmQJEnrzjIgSVLPWQYkSeq53s4ZuP+Wm7n5A6e0HUOSNCQ7vO6EtiOMLEcGJEnqOcuAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOWAUmSes4yIElSz3W6DCRZnOT167D90iSTSSZvW7FiNqNJktQZnS4DVfV9YCLJsauWJZmTZPMZtl9WVRNVNbHt3LnDiilJ0kjrdBlovBN47ZTnvwyc0VIWSZI6p7NlIMmq+yrsA0wd878V2Hb4iSRJ6qYu36joJUn+BvgxcOyU5UcCk+1EkiSpezpbBqrqbOBsgAzsBrweOBQ4sM1skiR1SWfLwDRfBh4AzgV+qarubjmPJEmdMRZloKocCZAk6VHq7ARCSZK0YYzFyMCjscn2O7DD605oO4YkSa1zZECSpJ6zDEiS1HOWAUmSeq63cwbuu/l6/ufU3207hiRpPTz2+Pe2HWEsODIgSVLPWQYkSeo5y4AkST1nGZAkqecsA5Ik9ZxlQJKknrMMSJLUc5YBSZJ6rvNlIMmTk7woyQlJrk5ySpKtZth2aZLJJJO3rfjZsKNKkjSSOlEGkjwnyUdmWL0fcCjw+8AE8D3g/avbsKqWVdVEVU1sO3eL2QkrSVLHdKIMAJsBuyXZftWCJIuSHA28EzgDeAC4H7gI2LuNkJIkdVFX7k1wAbA/8JUkWwIPArcBFwOHVNUVSZYBVwG3ACe1FVSSpK7pRBmoqgLe1XzNtM2fAX82tFCSJI2JrpwmeJgkp7edQZKkcdHJMlBVR7WdQZKkcdHJMiBJkjYcy4AkST3XiQmEs2HTHXbhsce/t+0YkiS1zpEBSZJ6zjIgSVLPWQYkSeq53s4ZuOfma/jOqYe2HUOSOuVJx3+y7QiaBY4MSJLUc5YBSZJ6zjIgSVLPWQYkSeo5y4AkST3X+TKQ5Ia2M0iS1GWdLwOSJGn9WAYkSeq5sSsDSbZMctoM65YmmUwyeceKlcOOJknSSBq7MlBVP62qY2ZYt6yqJqpqYsHcOcOOJknSSBq7MgCQ5GtJtmo7hyRJXTCWZQC4H9iu7RCSJHXB2JWBJI8DdgX8k0NJktZC58tAVe0MkGTrJEcC/w78QVU90G4ySZK6YSxuYZzkJOAo4BLgJVX1Xy1HkiSpM8aiDFTVe4D3tJ1DkqQuGosy8GhsvsPuPOn4T7YdQ5Kk1nV+zoAkSVo/lgFJknrOMiBJUs9ZBiRJ6rneTiD8yS3XcOmyQ9qOIUmtOWDpp9uOoBHhyIAkST1nGZAkqecsA5Ik9ZxlQJKknrMMSJLUc5YBSZJ6zjIgSVLP9aoMJFmaZDLJ5B0rVrYdR5KkkdC5MpDkmCTXJjkzyRZTls9Pcm6S65p1c6a/tqqWVdVEVU0smPsLqyVJ6qVOlYEkmwEnA08HlgMvmrJ6CTAf2Au4ANh+2PkkSeqizpSBJAuBzwPnAF8H9gb+M8kbm03OBy4HLgO2rqoftRJUkqSO6UwZAJ4HfK+qjqqqXavqV4FFzXKq6t6qOonBCMGftBdTkqRu6dKNirYAatqyK4HFSd4K3AjsCRwEvGXI2SRJ6qwujQz8gqpaARwIXA/MAS4EDqiq01oNJklSh3RmZKCqPgx8eDXL7wbOGHYeSZLGRadHBiRJ0vrrzMjAhrbV9rtzwNJPtx1DkqTWOTIgSVLPWQYkSeo5y4AkST3X2zkDy2+9mgtOO7jtGJK01l5wzGfbjqAx5ciAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOdLwNJ5iZ5XZKL2s4iSVIXdbYMJNkiycnAZcBOwG+3HEmSpE7q8nUGDgP2AvauqpVth5Ekqas6OzIAXMhgRGDvtX1BkqVJJpNM3nW3/UGSJOhQGcjA1queV9WtwCuAf06yKMlmSb6Q5LIkR6zuPapqWVVNVNXENlvPGVZ0SZJGWmfKAPA84OSpC6rqG8CHgdcCzwduBJ4LHJfk14YdUJKkLupSGbgfeGqSPZpRgi2SHAocCVzLYP7DPVV1O3A5g/kEkiRpDTozgbCqLkpyGvBRYBHwM+BS4KSq+nySbYDfTXID8DXgT9tLK0lSd3SmDABU1QeAD8yw7i7g2cNNJElS93XpNIEkSZoFlgFJknrOMiBJUs91as7AhjRvuz14wTGfbTuGJEmtc2RAkqSeswxIktRzlgFJknqut3MG7rj1as7+0EFtx5DUAYe/+nNtR5BmlSMDkiT1nGVAkqSeswxIktRzlgFJknrOMiBJUs+NbBlobkUsSZJm2ciWAUmSNByWAUmSeq5XZSDJ0iSTSSaXr1jZdhxJkkZCp8pAkn2SXJrkqiS/P23dLkkWNY9/e3Wvr6plVTVRVRPz5s4ZRmRJkkZep8oA8BrgYmAf4N4MbJpkblVdX1U3NdsdmWSitZSSJHXIyJeBJE9M8hvN0/cAuwOXApdVVQHHAm+e9rLvAE8YXkpJkrpr5MsA8BIGBYDmt/+XA+8CTmzWLwBum/aaPYGbkCRJazSydy2sqp2bhxcB/5TkZ83zfYFnMhgRALgMeFuSs6rqtiS/BewCfGmogSVJ6qiRHxmoqkngxcCDwL3AacB+VfW1Zv3ngHOAi5JcBxwMvKiqHmwpsiRJnTKyIwNTVdUPgA8+wvr3Ae8bXiJJksbHyI8MSJKk2dWJkYHZsGC7PTj81Z9rO4YkSa1zZECSpJ6zDEiS1HOWAUmSeq63cwZuve0qPnT689uOIWlIXn3UhW1HkEaWIwOSJPWcZUCSpJ6zDEiS1HOWAUmSes4yIElSz41dGUiyOMmX284hSVJXjF0ZkCRJ68YyIElSz/WqDCRZmmQyyeSKu+9rO44kSSNhrMpAknmPtL6qllXVRFVNzN1602HFkiRppI1VGQDOBbZqO4QkSV0ybmXgVmDbtkNIktQlY1MGkiwEfhW4u+0skiR1SefLQJItkrwY+DLw3qq6vKqe1XYuSZK6otO3ME7ym8AfAV8DjqmqS1uOJElS53S6DFTVx4CPtZ1DkqQu63QZWB/bbftEXn3UhW3HkCSpdZ2fMyBJktaPZUCSpJ6zDEiS1HOWAUmSeq63Ewhvvv1qTjnrBW3HkEbGCa+8oO0IklriyIAkST1nGZAkqecsA5Ik9ZxlQJKknrMMSJLUc62XgSQ3tJ1BkqQ+a70MSJKkdvWqDCRZmmQyyeSK5SvbjiNJ0kjoVRmoqmVVNVFVE3PnzWk7jiRJI6FzZSDJmUmWTFt2XJKTkhzXPN8yyeNaCShJUseMbBlIsnGS7zaP90pyyapVQCXZJcl5AFX191X1nqr6+2abJwKnDT+1JEndMzJlIMn+Sc5f9byqHgB+kOR5wNOApyTZBlgM/AiYBzxp6ghAkq2TPB94P/CZYeaXJKmrRulGRUuAL0xb9lhgGfAAgx/wVwH/UlXXACT5O+CCpiTcB/wMmAT+tKrOR5IkrVHrZaCqdm4e3gs8I8kcBrneCtxfVbtO2fz/TnvtqcCpQwkqSdKYGpnTBMBHgE2B7wBXMBgVeF6riSRJ6oHWRwZWqaoVwG+1nUOSpL4ZmTIwbDss3IMTXnlB2zEkSWrdKJ0mkCRJLbAMSJLUc5YBSZJ6rrdzBv7njqt5+8df0HYMaSS8/Qjnz0h95siAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOdLANJLk6ye9s5JEkaB50sA5IkacOxDEiS1HO9KgNJliaZTDL50+Ur244jSdJIGPkykGSzJF9IclmSIx5hu/lJzk1yXZIzk8yZvk1VLauqiaqa2HLeL6yWJKmXunA54ucDNwJHAOckuXWG7ZYA84G9gMOB7YEfDSOgJEldNvIjAwwKyz1VdTtwOYMf9sBDowZvbJ6e36y/DNi6qiwCkiSthS6UgYuAPZPcACwGzpyybl/geQBVdW9VncRghOBPhpxRkqTOGvnTBFV1F/DsaYuXACSZCyxO8lYGpxL2BA4C3jLMjJIkdVkXRgZmVFUrgAOB64E5wIXAAVV1WqvBJEnqkJEfGViTqrobOKPtHJIkdVWnRwYkSdL66/zIwKP12AV78PYjLmg7hiRJrXNkQJKknrMMSJLUc5YBSZJ6rrdzBr5/59W8+ryD2o6hHvrQYZ9rO4IkPYwjA5Ik9ZxlQJKknrMMSJLUc5YBSZJ6zjIgSVLPjWUZSPLhJM9tO4ckSV0wlmVAkiStPcuAJEk916sykGRpkskkk/csX9l2HEmSRsJYXYEwyZOB3WZaX1XLgGUA2+2+TQ0rlyRJo6xzIwNJnpPkIzOs3g84dJh5JEnqus6VAWAzYLck269akGRRkqOBdwJntBVMkqQu6uJpgguA/YGvJNkSeBC4DbgYOKSqrkhyTIv5JEnqlM6Vgaoq4F3N10zbHD20QJIkdVwXTxM8TJLT284gSVKXdb4MVNVRbWeQJKnLOneaYENZPH8PPnTY59qOIUlS6zo/MiBJktaPZUCSpJ6zDEiS1HOWAUmSeq63EwivvvOHvPCTx7cdQ+vp/ENPbTuCJHWeIwOSJPWcZUCSpJ6zDEiS1HOWAUmSes4yIElSz3WyDCS5OMnubeeQJGkcdLIMSJKkDccyIElSz/WqDCRZmmQyyeTK5T9rO44kSSNhZMtAkicnedGU55snmfMI289Pcm6S65Kcubptq2pZVU1U1cSceVvMVnRJkjql1TKQ5DlJPjLD6v2AQ6c8Pxz480d4uyXAfGAv4AJg+w2RUZKkcdf2yMBmwG5JHvrBnWRRkqOBdwJnTNn2VmDbqS9OslmSNzZPzwcuBy4Dtq6qH81mcEmSxkXbZeAC4DPAV5LckOSHwOeAfYFDquqSKdseCUxOe/2+wPMAqureqjqJwQjBn8x2cEmSxkWrdy2sqgLe1Xz9giQbAU8G3gw8BTiued2SZv1cYHGStwI3AnsCBwFvme3skiSNi7ZHBh4myelTHm8MXAX8DXAJ8KyqetifAFTVCuBA4HpgDnAhcEBVnTa00JIkdVyrIwPTVdVRUx4/AKzxKoNVdTcPn1sgSZLWwUiVgWHaY/7jOP/QU9uOIUlS60bqNIEkSRo+y4AkST1nGZAkqecsA5Ik9VxvJxBefeePOfi8P2s7hh6Fzx72h21HkKSx4siAJEk9ZxmQJKnnLAOSJPWcZUCSpJ4byzKQ5OIka7yUsSRJGtMyIEmS1p5lQJKknutVGUiyNMlkksmVy3/SdhxJkkZC58tAks2SfCHJZUmOeKRtq2pZVU1U1cSceVsNK6IkSSNtHK5A+HzgRuAI4Jwkt7acR5KkTun8yACDQnNPVd0OXA7s1XIeSZI6ZRzKwEXAnkluABYDZ7YbR5Kkbun8aYKqugt49rTFS1qIIklSJ43DyIAkSVoPlgFJknrOMiBJUs91fs7Ao7XH/Mfw2cP+sO0YkiS1zpEBSZJ6zjIgSVLPWQYkSeq53s4ZuPrOm3nRuae0HUPr4DMvPaHtCJI0lhwZkCSp5ywDkiT1nGVAkqSe62QZSLJxkpcl2TTJPkl2ajuTJEld1ckyADwT2IbBLYtPBG5pN44kSd3VyTJQVZdU1T8B86vqmKpa2XYmSZK6qpNlQJIkbTiWAUmSeq5XZSDJ0iSTSSZX3rWi7TiSJI2EsSkDSbZLsrh5/PIkm0/fpqqWVdVEVU3M2WbusCNKkjSSOn854iQbA3Or6lbg1mbxEqCAs9vKJUlSV4zDyMDzgek3GfgO8IQWskiS1DnjUAYWALdNW7YncFMLWSRJ6pxOl4Gq2hn4BvDCJLsAJPl14IXAeW1mkySpKzo/Z6CqrkzyDuC8JAuBbwEvrqq7Wo4mSVIndL4MAFTVR4GPtp1DkqQu6vRpAkmStP7GYmTg0dhj/g585qUntB1DkqTWOTIgSVLPWQYkSeo5y4AkST3X2zkDV99xKy865x/bjqEpPvOyY9uOIEm95MiAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOWAUmSem6dy0CSBUn2mY0wkiRp+NaqDCS5OMm85hbBlwH/kOS9sxtNkiQNw9qODGxTVcuBlwIfqapnAM+dvViPTpLHJfl8kmuTvG8165cmmUwyuXL53W1ElCRp5KxtGdgkyWOAI4BPz2Ke9fVS4CbgycBVSeZOXVlVy6pqoqom5szbupWAkiSNmrUtA+8ALgCuqar/TLIbcPXsxVp7SXZI8r+ap6cD9wKXAzdV1Yr2kkmS1A1rVQaq6hNVtU9Vvb55fl1VvWx2o621JcAEQFXdUVXHAK8G3tpmKEmSuuIRb1SU5G+Bmml9VZ2wwROtu/8A/jjJm4CfAHsBvw68pdVUkiR1xJpGBiaBrwObA7/E4NTA1cB+wJzZjbZ2quqHwHOAu4EAZwNPq6rPtBpMkqSOeMSRgao6HSDJccCzqur+5vkHgEtmP97aqaqbgdPaziFJUhet7QTCBcC8Kc/nNsskSVLHPeLIwBR/AVye5N8YDMX/KvD22Qo1DHss2I7PvOzYtmNIktS6NZaBJBsB3wWe0XwB/H5V3TibwSRJ0nCssQxU1YNJTq2q/YFPDiGTJEkaorWdM/DFJC9LkllNI0mShm5ty8BrgU8AK5Pc3Xwtn8VckiRpSNZqAmFVjd2F/K+543YOOfustmP00qcPf2XbESRJU6ztXxOQ5DcY/BUBwMVVNco3LJIkSWtprU4TJPkL4ETgyubrxCTvms1gkiRpONZ2ZOBgYL+qehAgyekM7gz4B7MVTJIkDcfaTiAEmD/l8TYbOogkSWrH2o4MvBO4LMnF/PwKhN4VUJKkMbC2IwOHAP/E4A6GZwMHVNXHZi3VFEk+nOS5w/gsSZL6aG1HBk4Dng38BvAEBvcp+FJVnTxrySRJ0lCs7XUG/i3Jl4CnA88BXgfsBVgGJEnquLX908IvAl8BfpPBTYueXlVPms1ga8jz/iTrfAvlJEuTTCaZXLncCyhKkgRrP2fgm8BKYG9gH2DvJFvMWiogyZOTvGh166rq9VV1x7Tt5yc5N8l1Sc5MMmc1r1tWVRNVNTFn3rzZii5JUqesVRmoqt+pql8FXgrcBnwIuHN9PzzJc5J8ZIbV+wGHzvC6/5PkqGmLlzD488e9gAuA7dc3nyRJfbC2pwnekORjDC40dCiDvyx44Qb4/M2A3ZI89IM7yaIkRzP4c8YzZnjdncDiJJsleWOz7Pwm32XA1lX1ow2QT5Kksbe2f02wOfBe4OtVdf8G/PwLgP2BryTZEniQwcjDxcAhVXVFkmOmvqC5jfJhwDJgX+B5wN9W1b3ASUneDXwbeP8GzClJ0tha278m+OvZ+PCqKuBdzddM2xwNkGQTBj/8/xDYGPhXYAsGIwRvBW4E9gQOwgsiSZK01tblcsSzrrnnweqWP57BDZLeDnwceGFVPVBVK4ADgeuBOcCFDC6IdNpwEkuS1H1rfQvjYaiq6ZMCVy3/AfDEGdbdzcxzCyRJ0hqMVBkYpt0XLOTTh7+y7RiSJLVupE4TSJKk4bMMSJLUc5YBSZJ6zjIgSVLP9XYC4TV33MmLzz637Ri98qnDX9p2BEnSajgyIElSz1kGJEnqOcuAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HO9KgNJliaZTDK5cvldbceRJGkkdK4MJNksyReSXJbkiCnLn57kyiRfTLLH6l5bVcuqaqKqJubM22Z4oSVJGmGdKwPA84EbgecCxyX5tWb5XwO/D3wQOLGlbJIkdU4XL0e8CXBPVd2e5HJgrySvBa4G3g08ALyhzYCSJHVJF8vARcDvJrkB+Brwl8DvVdXj240lSVI3da4MVNVdwLNXPU+yGKi28kiS1HVdnDMgSZI2oM6NDExXVd8HFrccQ5KkznJkQJKknuv8yMCjtfuC+Xzq8Je2HUOSpNY5MiBJUs9ZBiRJ6jnLgCRJPdfbOQPX3LGcQ8/+XNsxxt4nDz+o7QiSpDVwZECSpJ6zDEiS1HOWAUmSes4yIElSz1kGJEnqubErA0m2S/LmJB9vO4skSV0wNmWgKQGnA5cAAV7XciRJkjphnK4zsBQoYK+qerDtMJIkdcXYjAwAZwMHALvMtEGSpUkmk0yuXH7X8JJJkjTCOlMGkjwuyeeTXJvkfUk2TrLVqvVVdRXwJuCTSbZY3XtU1bKqmqiqiTnzthlWdEmSRlpnygDwUuAm4MnAVcDxwElTN6iq84GvAS8fejpJkjpqpMtAkh2S/K/m6enAvcDlDErBCuAZSXZqtp2X5JXA84Hr2sgrSVIXjfoEwiXABPCRqroDOCbJLwN/D/wysCPwuSTzgeXAl4AjquprLeWVJKlzRr0M/Afwx0neBPwE2Av4deAtVfUA8M7mS5IkPUojfZqgqn4IPAe4m8G1A84GnlZVn2k1mCRJY2TURwaoqpuB09rOIUnSuBrpkQFJkjT7Rn5kYLbsvmAenzz8oLZjSJLUOkcGJEnqOcuAJEk9ZxmQJKnnejtn4No7VnDYOV9uO8bYO+9lz2o7giRpDRwZkCSp5ywDkiT1nGVAkqSeswxIktRznS0DSZYkObPtHJIkdV1ny4AkSdowWi0DSf4oyRPbzCBJUt+1ep2BqnpHm58vSZJaHBlIckCSbyW5MMm8aev+Msk1SS5Osngt32/bZvtvJDl4hm2WJplMMnnv8jvXfyckSRoDbZ4meAPwN8A3gdeuWphkAfBqYAnwx8Bjm+WbJsmU128C3Dfl+QnA14HnA+9e3QdW1bKqmqiqic3mzd9weyJJUofNehlI8uwkH1zNqrOAPwR+E1jZ/NY+r6ruaJafD/wW8NVm+yuBLae8/knAjc1nnAjsBRzYbL9sNvZFkqRxNIyRgS2AfacO9ze/4V/B4Df4e4F/Bd5RVcth8Bs8sA/wLOApzcu2BHZNsnGS5wInAR9r1r0OeHNV/UpV7VpVp8z6XkmSNCZmfQJhVV2YZA/g40m2AwI8APwQ+Brw/Kr6XpJLkvwdMAnsDBwEfAn4dvNW/xs4A5gHfAN4RVV9o1m3BVCzvS+SJI2jofw1QVWdCpy6hs2OBA4GHs+gAHywqm6Z8h6fBT47ayElSeqpkbmFcVU9AHzqUb528YZNI0lSf3gFQkmSem5kRgaG7QkL5nLey57VdgxJklrnyIAkST1nGZAkqecsA5Ik9ZxlQJKknuvtBMLr7vgZLz/nm23HGDmfeNk+bUeQJA2ZIwOSJPWcZUCSpJ6zDEiS1HOWAUmSes4yIElSz3WiDCSZm+R1SS5qO4skSeNmpMtAki2SnAxcBuwE/HbLkSRJGjujfp2Bw4C9gL2rauX6vlmSpcBSgC23e8z6vp0kSWNhpEcGgAsZjAjsvTYbJ9koyQlJdlrd+qpaVlUTVTWx2bwFGzKnJEmdNVJlIANbr3peVbcCrwD+OcmiKdttleTJzeODkmzfbP8g8E3go8NNLklSd41UGQCeB5w8dUFVfQP4MPBagCQLquonVfXfzSZPBY6asv3FwEZJ9hpGYEmSum7UysD9wFOT7NGMEmyR5FDgSODaZjTgX6a95jvAEwCSbJJkO2BH4KfDDC5JUleN1ATCqrooyWkMhvkXAT8DLgVOqqrPJ3kmcNu0l+0J3JRkY+AqBgXnvVX1vSFGlySps0aqDABU1QeAD8yw+jvA05LsVVVXJNmfwemDF1TVA8Buw8opSdK4GLky8Eiq6vYkxwIfTPJY4DrgqKq6ruVokiR1VqfKAEBVfR74fNs5JEkaF50rAxvKbgu24BMv26ftGJIktW7U/ppAkiQNmWVAkqSeswxIktRzlgFJknqutxMIr79zJSecd33bMUbCKYft0nYESVKLHBmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOWAUmSes4yIElSz418GUjyK0ne0HYOSZLG1ciXAeANwBOSbD91YZLNk7w5yby1faMkS5NMJpn82fLbN3hQSZK6aKTLQJLNqupVVfU7wPJm2XFJDqmqe4CbgPc3y/dI8u0kX0yy3+rer6qWVdVEVU1sMW/h0PZDkqRRNtJlALg0yTZJtgKubZbtBGzbPD4DOLAZHXgd8D7gROCsJP60lyRpLYxMGUiyVZKzpi2eBI4AJoAdk+wGPA64MclGwK7AFsBKBpdWvge4GribQWmQJElrMEr3JljC4If4VDsDBwEB3gF8GfgGcCVwHXAfcGJV3ZPkw8AngD8F/hn49lBSS5LUcaNUBjYGHpdkc+BB4FhgX2CPqvpps807pmy/eOqLq+pyYPch5JQkaayMUhk4HziYwW/0mwL/CTx7ShGQJEmzYGTKQFXdx2ASoCRJGqKRmUAoSZLaMTIjA8O2y/w5nHLYLm3HkCSpdY4MSJLUc5YBSZJ6zjIgSVLP9XbOwM133sep593UdoyRcJZSbCoAABTKSURBVPxhi9qOIElqkSMDkiT1nGVAkqSeswxIktRzlgFJknrOMiBJUs9ZBiRJ6rmRLwNJfiXJG9rOIUnSuBr5MgC8AXhCku3XtGGShUlOSrLpEHJJkjQWRroMJNmsql5VVb8DLG+WHZfkkNVtX1W3A1sBfzzD+y1NMplkcsXy22cttyRJXTLSZQC4NMk2SbYCrm2W7QRsm+Q3k1yT5IwkG095zV8DR6/uzapqWVVNVNXE3HkLZze5JEkdMTJlIMlWSc6atngSOAKYAHZMshvwOOBG4G3AkcAc4NeSrLq08t7APcNJLUlS943SvQmWAHdPW7YzcBAQ4B3Al4FvAF8ETgXOAeYD/wpMJPlY8x5LhxNZkqTuG6UysDHwuCSbAw8CxwL7AntU1U+bbd4xZfsPJvkHBuXg2qr6D+DxwwwsSdI4GJnTBMD5wA+BbwNXA78GPHtKEXiYJLsAZwK3AV8dVkhJksbNyIwMVNV9wOvWtF2SnYB/B+4EPgMcU1U1y/EkSRpbI1MG1lZV/QjYve0ckiSNi1E6TSBJklrQuZGBDWWH+Zty/GGL2o4hSVLrHBmQJKnnLAOSJPWcZUCSpJ7r7ZyBO++4n3PPvrXtGLPmpYdv13YESVJHODIgSVLPWQYkSeo5y4AkST1nGZAkqefGsgwk+XCS57adQ5KkLhjLMiBJktaeZUCSpJ6zDEiS1HOdvuhQkj8HbgeuqqpPJdkG2PIRtl8KLAXYbrudhxNSkqQR17kykOQA4Leq6o1V9bZpq58DvGim11bVMmAZwO5P2K9mL6UkSd3RxdMEC4F9kjx0vd0kC5O8BPhz4HOtJZMkqYM6NzIAfBZ4OvDVJJsBDwDLgf8Ajq2qS5O8uM2AkiR1SefKQFUV8Pbma6Ztjh5SHEmSOq+LpwkkSdIGZBmQJKnnLAOSJPVc5+YMbCjzF2zCSw/fbs0bSpI05hwZkCSp5ywDkiT1nGVAkqSeswxIktRzvZ1AePft9/NvZ93Sdoz18pxXbt92BEnSGHBkQJKknrMMSJLUc5YBSZJ6zjIgSVLPWQYkSeq5TpaBJBcn2X2GdUcn+bNhZ5Ikqas6WQYkSdKG06sykGRpkskkk3ctv63tOJIkjYRelYGqWlZVE1U1sc28bduOI0nSSOjUFQibuQDXtJ1DkqRxMrIjA0kuTzIvyVZJrl+1GKhp282Z9tJNgPuGkVGSpHEwEmUgyaIk35i2+D+B3wSeBuyYZFdgMXDDlNdtBXwrSaa87knAjbObWJKk8TESZQB4JnDJtGU7Af8XOAv4U+ArwDzg4inbbALMBXZOMifJy4EjgE/NdmBJksbFqMwZuBd4UvOb/n3AMcB+wB5V9dNmm3dM2X7JqgdJ3gpcAGwKXAq8oKr+ZxihJUkaB6NSBi4EDga+ySDTfwLPnlIEZlRVpwOnz248SZLG10iUgaq6H3hD2zkkSeqjkSgDbdh64SY855Xbtx1DkqTWjcoEQkmS1BLLgCRJPWcZkCSp5ywDkiT1XG8nEP701vu5/B9vbjvGetn/2B3ajiBJGgOODEiS1HOWAUmSes4yIElSz1kGJEnqOcuAJEk9ZxmQJKnnxqoMJNk8yZuTzGs7iyRJXdG5MpDkyUletLp1VXUPcBPw/hleuzTJZJLJO+6+bTZjSpLUGSNZBpI8J8lHZli9H3Bos82VST6XZO6U9WcAB65udKCqllXVRFVNLNh629mILklS54xkGQA2A3ZL8tA9hpMsSnI08E4GP/B/B/gj4L+AVyTZKMlGwK7AFsDKoaeWJKmDRvVyxBcA+wNfSbIl8CBwG3AxcEhVXZFkAfA3wALgPcBOwCXAfcCJzSkDSZK0BiNZBqqqgHc1XzNt86/Avyb5NHBtVV0PLB5OQkmSxseoniZ4mCSnr2bZ9kneC+wOnDf8VJIkjYdOlIGqOmrq8yTXAF8AAjyzqu5tJZgkSWNgJE8TrElV7d52BkmSxkUnRgYkSdLs6eTIwIaw5XabsP+xO7QdQ5Kk1jkyIElSz1kGJEnqOcuAJEk919s5Aytvuo/vv+/GtmM8aovftGPbESRJY8KRAUmSes4yIElSz1kGJEnqOcuAJEk9ZxmQJKnnLAOSJPWcZUCSpJ7rTBlI8sQkb13DNhslOSHJTsPKJUlS1418GUiyOMnngb8FTkxydZKXr27bqnoQ+Cbw0Rnea2mSySSTt/3kttkLLUlSh4x8GQD+B3glsA2wDFhSVZ9I8tQkX09yaZLHrNq4qi4GNkqy1/Q3qqplVTVRVRPbbrXtsPJLkjTSRv5yxFW1MsmfAx+sqg9NWfW/GYwA/AR4A/C2JJsA84EdgZ8OPawkSR008mWgcRDwumnLzgJOBxYCn0yyMXAVg9GO91bV94YbUZKkbupKGSgGP+QfeGhB1VeBJyX5O+CGqnoA2K2lfJIkdVZXysAFwD8keUtV3ZhkD+CxwC8DLwf2bjWdJEkd1oUJhABvAm4H/j3JDcCZwEXAHsDTq+qWNsNJktRlnRgZqKqfAL/bfEmSpA2oKyMDkiRplnRiZGA2zFm0KYvftGPbMSRJap0jA5Ik9ZxlQJKknrMMSJLUc72dM3DfTfdy419f03aMdbbj7+3edgRJ0phxZECSpJ6zDEiS1HOWAUmSes4yIElSz1kGJEnquU6VgSRvT3Js2zkkSRonnSoDkiRpw7MMSJLUcyNfBpIsTvL6tdx28yRvTjJvhvVLk0wmmbxtxe0bNqgkSR018mWgqr4PTMw0V6ApAHOabe8BbgLeP8N7Lauqiaqa2HbuwtmKLElSp4x8GWi8E3jtDOsOB/58yvMzgANnGh2QJEkPN9JlIMmqeyfsA6yYYbNbgW2TbJRkI2BXYAtg5RAiSpLUeSNdBoCXJLkeeAtw4gzbHAlMAjsB1wEXACc2pwwkSdIajPRdC6vqbODsKYu+CdCMADwZeDPwFOC4qvoZsHjYGSVJ6rqRLgOrk2Rj4LsMRgE+BiytKk8JSJL0KHWuDFTVA8DubeeQJGlcdK4MbCibLtqMHX/PTiFJ0qhPIJQkSbPMMiBJUs9ZBiRJ6jnLgCRJPdfbCYT33fRTbnrf19uOsc4WvelpbUeQJI0ZRwYkSeo5y4AkST1nGZAkqedaKwNJntTWZ0uSpJ8bahlIMifJK5KcB7x2mJ8tSZJWb2hlIMnewLnAQuALwAHD+mxJkjSzYY4MXAf8N3AisAtw5BA/W5IkzWCY1xk4CJgAnlpV967LC5McDtxfVf+yPgGSLAWWAuy8YMf1eStJksbGMEcG/g3YAdj/Ubz2JcDh6xugqpZV1URVTSzcasH6vp0kSWNh1spABrZe9byq7gCOAM5MstM6vM+cqnoV8NtJ5jTL3pHkadO2m5/k3CTXJTlz1baSJOmRzebIwPOAk6cuqKorgPcDxwMk+ask309y4ureIMnGwLeSBHgKcFGzajdg7rTNlwDzgb2AC4DtN8xuSJI03mazDNwPPDXJHs0oweZJDgFeBVybZCHwGmBf4A1JtkryuCSnrHqDqnoA+AHwXAbzDZ6SZBvgccCNSTZL8sZm8/OBy4HLgK2r6kezuG+SJI2NWZtAWFUXJfkH4P8BjwF+CvwH8Naq+hxAc72BK4FtGfxW/2Lgf6a91U7AMuAB4FTgKuAzVfXdJL/MYATib5tJiScleTfwbQYjEJIkaQ1m9a8JqmoZgx/kM60/NsnOwDeBHwMbA09IsgkwB/hD4L6q2nXKy/7vlMdXAouTvBW4EdiTwV8tvGWD7ogkSWOszcsRb5Rkf+DTwClV9SBwOrA18F3gCmBHBr/5r1ZVrQAOBK5nUB4uBA6oqtNmOb4kSWNjmNcZeEiSlwF/weCH+MlV9SGAqrqLdbwYUVXdDZyxwUNKktQTrZSBqjoHOKeNz15l00VbsuhNT1vzhpIkjTlvYSxJUs9ZBiRJ6jnLgCRJPdfKnIFRcN/Nd3PTKRe3HWOdLDphSdsRJEljyJEBSZJ6zjIgSVLPWQYkSeo5y4AkST1nGZAkqec6WQaS3NB2BkmSxkUny4AkSdpwLAOSJPVcr8pAkqVJJpNM3r7irrbjSJI0EsamDCTZJ8mlSa5K8vur26aqllXVRFVNLJy7zbAjSpI0ksamDACvAS4G9gHuTZJ240iS1A2dLgNJnpjkN5qn7wF2By4FLquqai+ZJEnd0fUbFb0EuB+gqq4HXp7kCOBE4EttBpMkqSs6WQaqaufm4UXAPyX5WfN8X+CZwLGtBJMkqYM6fZqgqiaBFwMPAvcCpwH7VdXXWg0mSVKHdHJkYKqq+gHwwbZzSJLUVZ0eGZAkSeuv8yMDj9amO2zNohOWtB1DkqTWOTIgSVLPWQYkSeo5y4AkST3X2zkD9998Fzef+qm2Y6zRDse/uO0IkqQx58iAJEk9ZxmQJKnnLAOSJPWcZUCSpJ6zDEiS1HOWAUmSem4kykCSuUlel+SitrNIktQ3rZaBJFskORm4DNgJ+O0280iS1EdtX3ToMGAvYO+qWjnbH5ZkKbAUYOcF28/2x0mS1Altnya4kMGIwN4b4s2SbJ7kzUnmrW59VS2rqomqmth27jYb4iMlSeq8oZaBDGy96nlV3Qq8AvjnJIumbLdVkic3jw9K8gu/xif5yyTXJLk4yeLm/e4BbgLeP7t7IknS+Bj2yMDzgJOnLqiqbwAfBl4LkGRBVf2kqv672eSpwFFTX5NkAfBqYAnwx8Bjp6w+AzhwptEBSZL0cMMuA/cDT02yRzNKsEWSQ4EjgWub0YB/mfaa7wBPgME5/yTzquoO4A+B84HfAr6aZKMkGwG7AlsAsz4HQZKkcTDUMlBVFwGnAR8Ffgj8F/BS4KSqOgtYANw27WV7Mhj6B3hHVS1v3msZsA/wLOApDOYeXAdcAJzYnDKQJElrMPS/JqiqDwAfmGH1d4CnJdmrqq5Isj+D0wcvaNZfkuTvgElgZ+Ag4EvAt6uqgMWzGl6SpDHU9p8WPkxV3Z7kWOCDSR7L4Df9o6rqumaTI4GDgccD3wY+WFW3tJNWkqTxMFJlAKCqPg98foZ1DwCfGm4iSZLG28iVgWHZZIdt2OH4F7cdQ5Kk1rV90SFJktSyDObd9U+Su4Hvtp1jSLYDbm07xBD0ZT+hP/vqfo6fvuzrqO7n46vqFy7k19vTBMB3q2qi7RDDkGSyD/val/2E/uyr+zl++rKvXdtPTxNIktRzlgFJknquz2VgWdsBhqgv+9qX/YT+7Kv7OX76sq+d2s/eTiCUJEkDfR4ZkCRJWAYkSeq9XpaBJAcl+W6Sa5K8pe08ayPJLkn+LcmVSa5IcmKzfGGSzye5uvnvgmZ5kpzS7OM3k/zSlPc6qtn+6iRHTVn+tCTfal5zSpIMf08fyrJxksuTfLp5vmuSrzbZPpZkTrN8s+b5Nc36xVPe4w+a5d9N8oIpy0fm+CeZn+TsJN9J8t9JDhjHY5rkd5rv228n+WiSzcflmCb5pyQ3J/n2lGWzfgxn+owh7+dfNd+730xyXpL5U9at07F6NN8Pw9zXKetOSlJJtmued/aYPkxV9eoL2Bi4FtgNmMPgNspPaTvXWuR+DPBLzeOtgasY3Lr53cBbmuVvAf6yeXwwcD4Q4FeArzbLFzK4AdRCBreMvg5Y0Kz7WrNtmte+sMX9/V3g/wGfbp5/HDiyefwB4Ljm8euBDzSPjwQ+1jx+SnNsNwN2bY75xqN2/IHTgWObx3OA+eN2TBncXvx7wBZTjuXR43JMgV8FfonB3VNXLZv1YzjTZwx5P58PbNI8/ssp+7nOx2pdvx+Gva/N8l2AC4AfANt1/Zg+bN+G9UGj8gUcAFww5fkfAH/Qdq5HsR+fBJ7H4CqKj2mWPYbBxZQAPgi8Ysr2323Wv4LB3R6Zul2z7jtTlj9suyHv287AF4FfAz7d/A9z65R/dB46hs3/mAc0jzdptsv047pqu1E6/sA2DH5IZtrysTqmDMrA9c0/ips0x/QF43RMGdw+feoPyVk/hjN9xjD3c9q6w4CzVncM1nSsHs3/423sK3A2sC/wfX5eBjp9TFd99fE0wap/mFa5oVnWGc0w2f7AV4FFVfXjZtWNwKLm8Uz7+UjLb1jN8ja8D/g/wIPN822BO6vq/ub51GwP7U+z/q5m+3Xd/zbsCtwCfCiDUyL/mGQrxuyYVtWPgL8Gfgj8mMEx+jrjeUxXGcYxnOkz2vIaBr/lwrrv56P5f3yokhwK/Kiq/mvaqrE4pn0sA52WZC5wDvCmqlo+dV0N6mSn/1Y0ySHAzVX19bazDMEmDIYi/76q9gd+wmBo8CFjckwXAIcyKD+PBbYCDmo11BAN4xi2/X2S5G3A/cBZbWWYTUm2BN4K/NGwPnPYx7SPZeBHDM77rLJzs2zkJdmUQRE4q6rObRbflOQxzfrHADc3y2faz0davvNqlg/bgcBvJPk+8M8MThWcDMxPsupeGlOzPbQ/zfptgNtY9/1vww3ADVX11eb52QzKwbgd0+cC36uqW6rqPuBcBsd5HI/pKsM4hjN9xlAlORo4BHhl8wMM1n0/b2Pdvx+G6QkMyux/Nf827QxclmRHxuWYDut8xKh8Mfht7DoGB3bVBJa92s61FrkDfAR437Tlf8XDJ5y8u3n8Ih4+qeVrzfKFDM5TL2i+vgcsbNZNn9RycMv7vISfTyD8BA+fXPT65vHxPHxy0cebx3vx8AlM1zGYvDRSxx+4BNizefz25niO1TEFngFcAWzZ5DgdeOM4HVN+cc7ArB/DmT5jyPt5EHAlsP207db5WK3r98Ow93Xauu/z8zkDnT6mD+3TsD5olL4YzP68isGs1re1nWctMz+LwZDRN4FvNF8HMzh39kXgauALU77ZApza7OO3gIkp7/Ua4Jrm69VTlk8A325e83cMYZLOGvZ5CT8vA7s1/wNd0/yjsVmzfPPm+TXN+t2mvP5tzb58lymz6Efp+AP7AZPNcf2X5h+NsTumwJ8A32mynMHgh8RYHFPgowzmQtzHYLTnmGEcw5k+Y8j7eQ2D8+Kr/k36wKM9Vo/m+2GY+zpt/ff5eRno7DGd+uXliCVJ6rk+zhmQJElTWAYkSeo5y4AkST1nGZAkqecsA5Ik9ZxlQJKknrMMSJLUc/8fhPDTy+lhZbgAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Finding-the-max-len">
<a class="anchor" href="#Finding-the-max-len" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finding the max len<a class="anchor-link" href="#Finding-the-max-len"> </a>
</h3>
<p>The contents have different lengths based on words! Detecting the most normal range could help us find the maximum length of the sequences for the preprocessing step. On the other hand, we suppose that the minimum word combination for having a meaningful article for our learning process is 50.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_df</span><span class="p">[</span><span class="s1">'words_len'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">tokens</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">data_gl_than</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">less_than</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">greater_than</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s1">'words_len'</span><span class="p">):</span>
    <span class="n">data_length</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">data_glt</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">data_length</span> <span class="k">if</span> <span class="n">greater_than</span> <span class="o">&lt;</span> <span class="n">length</span> <span class="o">&lt;=</span> <span class="n">less_than</span><span class="p">])</span>
    <span class="n">data_glt_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_glt</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_length</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Texts with word length of greater than </span><span class="si">{</span><span class="n">greater_than</span><span class="si">}</span><span class="s1"> and less than </span><span class="si">{</span><span class="n">less_than</span><span class="si">}</span><span class="s1"> includes </span><span class="si">{</span><span class="n">data_glt_rate</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">% of the whole!'</span><span class="p">)</span>

<span class="n">data_gl_than</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>   
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Texts with word length of greater than 50 and less than 10000.0 includes 99.55% of the whole!
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_df</span><span class="p">[</span><span class="s1">'words_len'</span><span class="p">]))</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
    <span class="n">title_text</span><span class="o">=</span><span class="s1">'Distribution of word counts within comments'</span><span class="p">,</span>
    <span class="n">xaxis_title_text</span><span class="o">=</span><span class="s1">'Word Count'</span><span class="p">,</span>
    <span class="n">yaxis_title_text</span><span class="o">=</span><span class="s1">'Frequency'</span><span class="p">,</span>
    <span class="n">bargap</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">bargroupgap</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<meta charset="utf-8">

    <div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>
                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>    
            <div id="b1d71f95-7255-428d-bca9-419a46b0a3b1" class="plotly-graph-div" style="height:525px; width:100%;"></div>
            <script type="text/javascript">
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById("b1d71f95-7255-428d-bca9-419a46b0a3b1")) {
                    Plotly.newPlot(
                        'b1d71f95-7255-428d-bca9-419a46b0a3b1',
                        [{"type": "histogram", "x": [237, 376, 726, 1799, 2059, 690, 561, 440, 442, 1590, 480, 243, 422, 859, 925, 1773, 3006, 678, 412, 305, 1587, 447, 1769, 1519, 396, 470, 1101, 218, 185, 277, 359, 403, 705, 841, 255, 345, 281, 430, 3224, 460, 509, 563, 722, 54, 1943, 373, 1314, 2580, 500, 571, 633, 377, 367, 457, 1299, 423, 743, 568, 156, 275, 385, 1323, 424, 512, 209, 304, 559, 1732, 247, 449, 1859, 542, 1101, 2233, 2429, 306, 433, 1720, 111, 293, 64, 956, 928, 268, 92, 1168, 307, 1396, 820, 259, 95, 445, 789, 282, 384, 451, 1412, 121, 790, 278, 516, 959, 846, 296, 2139, 314, 247, 160, 219, 920, 550, 299, 647, 80, 309, 435, 320, 323, 469, 800, 270, 82, 262, 1898, 370, 327, 843, 375, 435, 440, 323, 175, 246, 282, 1013, 260, 821, 436, 1282, 91, 300, 1051, 308, 636, 550, 212, 192, 637, 6226, 2396, 347, 1316, 1146, 2808, 379, 290, 4106, 557, 335, 531, 218, 576, 374, 332, 1126, 272, 289, 355, 1077, 1425, 1253, 54, 652, 238, 327, 217, 427, 467, 401, 264, 435, 79, 1561, 462, 422, 1377, 328, 246, 948, 376, 516, 413, 358, 702, 1344, 1181, 234, 462, 1785, 1461, 495, 429, 401, 273, 241, 273, 341, 2943, 222, 371, 244, 128, 557, 1829, 240, 240, 76, 490, 136, 303, 4664, 456, 950, 6792, 589, 378, 1166, 408, 230, 77, 448, 1127, 928, 51, 260, 341, 624, 338, 524, 1255, 327, 242, 100, 84, 427, 90, 337, 531, 1090, 262, 251, 343, 2558, 2045, 875, 534, 503, 550, 411, 366, 75, 462, 333, 1812, 53, 274, 295, 4752, 583, 231, 1148, 1238, 757, 348, 3356, 2139, 319, 278, 309, 350, 49, 1707, 102, 349, 143, 996, 591, 3254, 2219, 2369, 624, 535, 341, 292, 1357, 282, 376, 140, 7727, 1677, 1547, 2902, 696, 564, 249, 233, 616, 558, 263, 277, 271, 71, 1529, 405, 351, 434, 343, 324, 392, 342, 371, 457, 363, 343, 284, 356, 1437, 67, 379, 279, 2446, 307, 668, 261, 332, 1178, 191, 333, 829, 1024, 740, 248, 434, 49, 1823, 233, 196, 261, 356, 588, 644, 320, 262, 168, 299, 964, 32, 1876, 117, 177, 2463, 65, 361, 438, 346, 837, 427, 4339, 1589, 624, 304, 318, 413, 224, 231, 3311, 356, 371, 448, 577, 447, 401, 324, 1170, 359, 238, 224, 1602, 1162, 320, 434, 226, 575, 368, 453, 239, 370, 711, 75, 225, 368, 1118, 287, 913, 72, 333, 370, 371, 813, 347, 648, 550, 296, 99, 724, 458, 809, 484, 1128, 1186, 245, 559, 359, 62, 140, 350, 336, 345, 1590, 1056, 1104, 367, 197, 584, 513, 795, 1086, 756, 224, 379, 314, 928, 475, 578, 686, 2791, 315, 495, 352, 318, 473, 402, 69, 574, 1735, 359, 855, 268, 628, 514, 1994, 343, 282, 219, 1252, 899, 1241, 276, 1037, 867, 355, 781, 281, 357, 576, 429, 259, 612, 350, 264, 312, 78, 502, 394, 864, 351, 993, 208, 488, 731, 90, 1919, 426, 282, 536, 211, 451, 280, 1663, 87, 333, 526, 323, 319, 344, 186, 383, 66, 1201, 2951, 291, 372, 327, 1197, 1542, 309, 142, 394, 1060, 332, 514, 262, 296, 48, 317, 474, 346, 246, 316, 685, 273, 1527, 322, 312, 731, 809, 1345, 308, 923, 1714, 451, 381, 728, 461, 48, 207, 1002, 564, 383, 675, 251, 620, 1173, 69, 2097, 604, 300, 688, 206, 429, 768, 348, 2014, 273, 488, 298, 598, 241, 424, 255, 347, 280, 579, 478, 258, 319, 256, 571, 5194, 1396, 307, 1100, 657, 108, 283, 531, 650, 416, 404, 460, 1356, 84, 450, 274, 338, 238, 324, 420, 313, 1791, 1200, 1439, 267, 3876, 341, 248, 381, 344, 327, 693, 641, 450, 1835, 2864, 212, 834, 251, 1161, 276, 1423, 454, 60, 1055, 338, 585, 210, 617, 618, 260, 346, 1037, 725, 528, 530, 329, 320, 229, 393, 495, 506, 914, 374, 984, 858, 197, 588, 723, 494, 1310, 404, 1734, 1870, 1710, 1299, 416, 222, 468, 1043, 241, 236, 1079, 177, 1322, 394, 248, 1658, 297, 427, 1556, 166, 476, 1146, 540, 277, 276, 1419, 260, 280, 282, 602, 213, 356, 435, 1758, 235, 490, 1261, 260, 110, 48, 724, 1039, 1652, 1461, 1013, 1475, 848, 230, 247, 1828, 902, 570, 260, 2198, 334, 314, 536, 1013, 354, 2460, 54, 1078, 415, 600, 450, 328, 428, 1099, 371, 1218, 213, 1538, 4722, 87, 217, 1900, 363, 640, 245, 1049, 323, 428, 252, 1255, 2602, 198, 426, 820, 1313, 243, 357, 1483, 384, 230, 999, 1272, 469, 343, 425, 61, 189, 531, 357, 1563, 313, 399, 815, 325, 197, 277, 272, 3273, 780, 577, 105, 390, 516, 60, 252, 4470, 759, 366, 509, 298, 1696, 1639, 907, 76, 66, 423, 67, 292, 531, 329, 411, 361, 416, 1973, 807, 231, 1330, 463, 997, 200, 2393, 614, 418, 360, 293, 894, 375, 1803, 1383, 338, 321, 673, 161, 815, 449, 360, 978, 343, 499, 444, 71, 437, 197, 298, 449, 289, 212, 149, 478, 985, 489, 56, 616, 613, 365, 1843, 306, 360, 328, 408, 512, 342, 315, 422, 346, 615, 445, 325, 319, 107, 1524, 468, 420, 1724, 351, 1157, 351, 275, 573, 341, 385, 481, 203, 324, 63, 258, 1267, 196, 571, 1066, 335, 628, 1795, 535, 2299, 734, 966, 1265, 681, 271, 263, 1406, 336, 350, 494, 199, 363, 816, 628, 293, 2010, 357, 441, 422, 227, 250, 1953, 284, 381, 1525, 225, 1115, 244, 547, 438, 387, 550, 670, 612, 1134, 1932, 397, 386, 900, 2541, 295, 1113, 231, 1814, 751, 1773, 275, 819, 77, 239, 3544, 220, 827, 1102, 415, 1064, 1037, 396, 292, 62, 526, 65, 506, 798, 296, 71, 263, 141, 1271, 750, 281, 633, 619, 507, 256, 238, 533, 1600, 345, 128, 2838, 482, 586, 81, 727, 479, 251, 527, 58, 533, 513, 760, 430, 757, 391, 278, 330, 376, 1900, 70, 968, 243, 91, 1619, 182, 556, 307, 397, 3099, 278, 242, 329, 403, 882, 262, 1177, 364, 949, 506, 465, 77, 220, 2584, 162, 347, 256, 169, 195, 234, 1747, 289, 482, 347, 530, 457, 347, 586, 372, 280, 914, 732, 883, 330, 281, 2603, 301, 636, 62, 332, 767, 273, 2154, 254, 2164, 407, 345, 1309, 368, 647, 64, 428, 769, 472, 689, 228, 264, 1081, 942, 440, 572, 793, 218, 473, 259, 1084, 277, 422, 188, 422, 398, 609, 366, 350, 548, 1170, 86, 513, 254, 1957, 1146, 894, 279, 1661, 339, 303, 231, 422, 1671, 3035, 1276, 273, 747, 368, 1379, 302, 332, 350, 949, 1111, 594, 561, 649, 588, 193, 634, 936, 883, 460, 285, 286, 87, 1400, 3792, 488, 261, 2155, 247, 491, 76, 4701, 435, 1616, 713, 561, 580, 359, 1376, 580, 552, 262, 72, 313, 963, 401, 391, 1509, 340, 418, 233, 264, 365, 223, 1662, 47, 500, 706, 261, 2222, 255, 398, 207, 693, 1168, 433, 787, 491, 1571, 261, 282, 796, 1071, 324, 73, 462, 1132, 164, 232, 347, 338, 336, 106, 521, 6188, 2741, 450, 250, 376, 86, 385, 591, 648, 351, 57, 606, 408, 33, 465, 411, 503, 1184, 967, 910, 316, 71, 289, 198, 982, 326, 403, 167, 235, 131, 244, 628, 524, 86, 502, 891, 318, 535, 73, 72, 956, 238, 362, 313, 974, 326, 180, 545, 277, 261, 450, 74, 370, 855, 300, 1692, 1060, 852, 375, 1049, 1047, 80, 99, 576, 613, 2242, 474, 532, 46, 324, 349, 388, 297, 1269, 497, 296, 376, 845, 361, 296, 394, 53, 456, 1267, 527, 311, 826, 338, 236, 369, 839, 342, 344, 247, 461, 267, 226, 478, 200, 1214, 820, 289, 501, 896, 1052, 1535, 89, 262, 443, 953, 867, 785, 529, 63, 502, 533, 182, 455, 288, 433, 322, 258, 216, 1843, 398, 634, 1206, 418, 399, 245, 494, 220, 720, 96, 876, 625, 1007, 1676, 140, 85, 1327, 624, 523, 280, 313, 421, 297, 275, 722, 301, 1656, 259, 1580, 679, 255, 976, 642, 676, 325, 550, 319, 667, 93, 1136, 301, 872, 245, 626, 1125, 1374, 1406, 275, 1560, 335, 368, 574, 435, 356, 462, 225, 2625, 1289, 405, 338, 96, 625, 330, 1311, 302, 241, 542, 1664, 311, 355, 618, 234, 362, 992, 368, 77, 2445, 780, 91, 368, 369, 409, 202, 767, 371, 312, 309, 190, 318, 285, 1401, 2467, 2129, 503, 365, 329, 594, 453, 353, 398, 922, 286, 316, 997, 64, 1163, 1697, 329, 398, 253, 531, 448, 474, 361, 327, 762, 1165, 270, 191, 943, 367, 391, 455, 468, 279, 728, 377, 424, 870, 2458, 4328, 371, 580, 384, 114, 262, 540, 355, 429, 923, 675, 686, 28, 251, 585, 221, 151, 353, 231, 405, 509, 214, 390, 692, 397, 342, 232, 857, 267, 126, 263, 1393, 255, 265, 674, 437, 581, 58, 130, 273, 334, 292, 313, 140, 1277, 228, 369, 328, 168, 305, 721, 208, 1697, 1046, 66, 246, 918, 53, 216, 315, 565, 1845, 311, 296, 450, 71, 327, 1999, 591, 1736, 889, 359, 858, 364, 522, 59, 158, 446, 205, 381, 290, 541, 50, 407, 761, 87, 261, 1018, 938, 326, 440, 291, 271, 633, 225, 196, 499, 438, 84, 263, 373, 471, 73, 526, 90, 744, 269, 450, 272, 1399, 229, 588, 585, 367, 440, 64, 353, 311, 429, 481, 56, 295, 184, 342, 51, 549, 409, 562, 740, 418, 399, 515, 351, 51, 282, 869, 362, 421, 406, 134, 331, 1283, 237, 319, 571, 245, 279, 1192, 790, 881, 242, 1551, 560, 714, 666, 1147, 3185, 203, 292, 587, 230, 335, 546, 568, 70, 2383, 1073, 322, 1539, 1466, 530, 942, 1333, 381, 237, 1171, 1291, 754, 282, 1009, 292, 987, 2008, 221, 1005, 270, 790, 300, 746, 299, 2362, 968, 469, 263, 1817, 856, 1140, 513, 324, 66, 351, 404, 330, 77, 67, 362, 339, 420, 307, 354, 2488, 67, 421, 785, 251, 436, 262, 299, 65, 409, 230, 1455, 394, 1483, 1496, 922, 2824, 313, 1294, 1111, 404, 1287, 368, 61, 204, 4269, 239, 185, 330, 515, 437, 2192, 297, 280, 839, 314, 266, 357, 2266, 288, 307, 278, 654, 125, 391, 382, 539, 355, 322, 1082, 578, 47, 2407, 70, 73, 1214, 331, 745, 574, 153, 402, 323, 395, 65, 69, 353, 225, 986, 209, 223, 1115, 2347, 130, 270, 283, 381, 1534, 234, 245, 1043, 1199, 376, 436, 302, 315, 631, 769, 246, 1068, 725, 1261, 439, 158, 510, 554, 269, 875, 481, 720, 682, 481, 378, 496, 540, 385, 240, 376, 587, 75, 198, 215, 1628, 682, 532, 584, 59, 55, 824, 551, 413, 293, 232, 268, 862, 509, 359, 405, 527, 882, 493, 383, 980, 303, 380, 59, 397, 711, 1344, 954, 94, 1138, 452, 273, 435, 1040, 466, 244, 436, 267, 333, 519, 1771, 319, 407, 372, 426, 355, 325, 1208, 1504, 372, 1719, 298, 370, 347, 1093, 573, 317, 101, 252, 421, 297, 207, 492, 958, 303, 3080, 226, 76, 7514, 248, 253, 281, 459, 600, 78, 438, 179, 268, 401, 301, 1574, 813, 784, 154, 931, 88, 2320, 326, 183, 1821, 259, 551, 267, 1551, 352, 252, 322, 57, 351, 381, 755, 609, 705, 1026, 729, 508, 2479, 4199, 289, 4712, 144, 335, 51, 279, 245, 338, 370, 300, 1017, 179, 893, 54, 990, 315, 319, 469, 619, 407, 702, 893, 330, 1797, 1738, 332, 370, 273, 1544, 1007, 1337, 1481, 354, 305, 696, 367, 2946, 350, 1216, 144, 369, 266, 48, 494, 81, 346, 1906, 1594, 344, 52, 239, 236, 345, 408, 214, 434, 237, 415, 427, 84, 1975, 417, 315, 276, 325, 972, 280, 2816, 1244, 630, 636, 318, 244, 308, 1862, 2178, 572, 1649, 525, 1540, 1109, 539, 1321, 281, 264, 152, 1417, 392, 3464, 617, 390, 333, 321, 409, 1808, 329, 215, 197, 411, 500, 348, 392, 462, 564, 294, 676, 452, 745, 715, 270, 1541, 226, 1052, 234, 400, 1322, 398, 392, 343, 123, 339, 614, 481, 1560, 460, 250, 1661, 328, 944, 363, 307, 934, 939, 1450, 352, 259, 413, 988, 267, 1391, 317, 449, 342, 1222, 67, 338, 387, 232, 283, 57, 332, 359, 329, 294, 4006, 276, 317, 1575, 1850, 1596, 197, 774, 332, 369, 3000, 353, 344, 410, 417, 1366, 565, 291, 579, 366, 356, 741, 387, 496, 144, 162, 119, 593, 497, 770, 371, 458, 860, 356, 286, 72, 178, 818, 283, 365, 264, 1117, 249, 441, 706, 1416, 415, 234, 277, 123, 787, 329, 277, 747, 719, 399, 191, 221, 378, 314, 256, 439, 232, 809, 370, 267, 348, 1246, 263, 450, 534, 191, 220, 134, 569, 232, 850, 340, 264, 328, 984, 974, 213, 70, 121, 4681, 766, 237, 376, 212, 493, 1176, 193, 1087, 820, 7069, 2180, 765, 1354, 233, 322, 282, 771, 60, 751, 262, 1011, 333, 621, 360, 1208, 590, 827, 2061, 436, 391, 372, 466, 1627, 516, 238, 431, 378, 403, 266, 632, 1695, 1558, 1026, 390, 332, 319, 77, 2571, 913, 1059, 644, 243, 540, 73, 1723, 1206, 325, 49, 60, 1122, 769, 498, 294, 294, 386, 549, 133, 296, 231, 268, 441, 796, 419, 255, 308, 507, 449, 329, 253, 1542, 798, 554, 309, 151, 999, 219, 1340, 449, 385, 144, 467, 532, 112, 842, 616, 280, 74, 347, 1291, 163, 543, 443, 92, 1719, 403, 896, 925, 452, 280, 699, 339, 931, 824, 1155, 455, 340, 85, 953, 553, 216, 271, 621, 125, 712, 353, 261, 266, 414, 299, 487, 264, 439, 432, 323, 267, 674, 498, 1113, 1206, 255, 159, 51, 141, 457, 890, 567, 364, 348, 351, 231, 282, 2605, 1145, 554, 311, 258, 885, 300, 373, 257, 505, 485, 270, 347, 74, 572, 313, 330, 494, 117, 444, 311, 75, 283, 326, 1689, 354, 1768, 440, 320, 369, 960, 451, 371, 364, 407, 3379, 278, 435, 254, 63, 401, 445, 333, 159, 296, 84, 414, 336, 252, 493, 89, 914, 506, 503, 440, 115, 1544, 1213, 477, 717, 979, 231, 325, 365, 550, 230, 334, 347, 538, 298, 1370, 326, 383, 920, 417, 1366, 60, 70, 356, 334, 806, 1441, 884, 348, 383, 100, 369, 71, 437, 167, 492, 243, 3650, 50, 558, 1447, 427, 1448, 1252, 327, 78, 315, 261, 60, 1236, 252, 391, 229, 332, 391, 493, 426, 237, 237, 52, 1210, 3042, 913, 1023, 322, 477, 80, 234, 998, 340, 420, 714, 475, 1073, 896, 930, 238, 68, 567, 3626, 622, 201, 4198, 359, 626, 1885, 307, 303, 274, 111, 483, 312, 442, 322, 267, 348, 69, 667, 1364, 186, 341, 340, 658, 1260, 952, 686, 285, 214, 344, 377, 455, 387, 55, 379, 234, 345, 428, 410, 364, 360, 266, 740, 676, 466, 1210, 751, 319, 403, 637, 523, 907, 333, 244, 428, 1177, 1945, 387, 519, 301, 1766, 124, 292, 1786, 307, 877, 562, 1328, 770, 1067, 297, 504, 1076, 70, 221, 255, 355, 772, 264, 1823, 502, 1778, 251, 2033, 891, 1679, 287, 232, 353, 493, 248, 333, 258, 1524, 298, 229, 106, 514, 469, 263, 302, 312, 209, 339, 371, 738, 404, 300, 203, 730, 499, 511, 352, 2422, 352, 655, 511, 567, 346, 354, 756, 140, 687, 240, 71, 1046, 81, 464, 324, 1747, 238, 403, 525, 396, 609, 306, 288, 501, 267, 791, 340, 174, 326, 770, 333, 339, 282, 257, 1124, 250, 270, 255, 11039, 387, 599, 428, 2323, 281, 492, 249, 1322, 947, 60, 198, 867, 1686, 577, 338, 453, 187, 1048, 454, 250, 58, 852, 582, 1742, 68, 442, 1075, 583, 332, 63, 199, 329, 419, 1745, 276, 474, 325, 741, 269, 2867, 973, 248, 316, 1879, 391, 606, 2051, 439, 256, 246, 708, 551, 433, 435, 310, 631, 938, 261, 500, 151, 273, 216, 958, 417, 309, 207, 953, 480, 578, 315, 314, 463, 566, 958, 173, 308, 237, 1258, 397, 621, 1442, 216, 886, 1840, 55, 57, 347, 165, 529, 1065, 334, 509, 263, 1319, 929, 387, 702, 186, 5966, 302, 1378, 1519, 97, 268, 396, 279, 346, 504, 1521, 211, 196, 328, 785, 698, 349, 327, 782, 316, 1059, 1814, 586, 518, 540, 386, 1013, 661, 172, 248, 1259, 539, 399, 245, 500, 786, 1561, 1181, 558, 665, 249, 344, 531, 334, 292, 106, 128, 408, 1530, 367, 5261, 620, 449, 987, 259, 320, 305, 223, 1288, 288, 256, 267, 643, 1109, 611, 343, 596, 759, 1401, 860, 371, 1829, 638, 2160, 2189, 269, 298, 394, 266, 260, 4878, 343, 306, 121, 1428, 349, 338, 1634, 384, 215, 1274, 150, 56, 2022, 1182, 255, 508, 2380, 274, 406, 419, 1533, 263, 494, 335, 268, 324, 440, 99, 1866, 334, 288, 226, 279, 547, 319, 263, 267, 262, 291, 528, 385, 55, 1270, 1731, 366, 1210, 255, 387, 272, 2100, 480, 253, 554, 340, 1069, 303, 630, 630, 1183, 449, 314, 317, 482, 89, 332, 241, 323, 257, 272, 205, 462, 385, 1751, 665, 517, 87, 1568, 533, 238, 2151, 370, 547, 60, 263, 2693, 707, 2532, 1173, 287, 779, 533, 1120, 2523, 351, 669, 252, 338, 2456, 403, 1659, 135, 342, 1079, 619, 471, 943, 214, 2634, 750, 578, 1171, 279, 390, 592, 540, 291, 56, 744, 346, 747, 70, 2026, 1195, 296, 231, 850, 449, 775, 690, 410, 458, 287, 231, 373, 280, 369, 341, 331, 279, 3628, 551, 931, 577, 285, 976, 286, 314, 640, 485, 1672, 434, 422, 330, 335, 328, 225, 685, 1327, 369, 533, 62, 639, 465, 639, 863, 1327, 2137, 429, 381, 1668, 335, 2260, 2604, 399, 1113, 895, 1015, 333, 430, 514, 444, 691, 480, 125, 659, 737, 520, 427, 258, 445, 378, 479, 207, 7477, 330, 587, 1156, 504, 379, 499, 458, 188, 278, 2838, 338, 304, 528, 1068, 394, 425, 1134, 442, 24, 426, 771, 346, 313, 928, 303, 401, 3287, 1549, 685, 306, 771, 232, 102, 83, 655, 1030, 467, 60, 630, 535, 60, 702, 339, 1557, 540, 135, 299, 336, 251, 1345, 170, 328, 356, 1773, 207, 323, 2205, 737, 362, 252, 338, 3699, 301, 229, 119, 325, 339, 293, 325, 368, 848, 3687, 1110, 603, 598, 356, 84, 652, 359, 64, 347, 556, 319, 543, 456, 1465, 210, 229, 336, 46, 1996, 1655, 484, 61, 1875, 656, 578, 342, 99, 402, 153, 422, 609, 315, 534, 255, 501, 289, 284, 284, 433, 631, 512, 1799, 886, 333, 279, 76, 89, 651, 346, 218, 417, 120, 467, 411, 1051, 399, 61, 234, 335, 1106, 410, 1203, 304, 459, 375, 942, 2236, 880, 83, 356, 1159, 451, 429, 310, 1002, 286, 543, 422, 227, 349, 365, 238, 83, 654, 930, 304, 445, 284, 789, 738, 296, 789, 837, 393, 445, 347, 411, 529, 422, 196, 336, 244, 272, 694, 111, 372, 51, 1506, 437, 597, 615, 1687, 246, 641, 967, 430, 1862, 488, 542, 1038, 2202, 1668, 829, 1653, 462, 80, 304, 1991, 319, 318, 225, 521, 572, 4931, 303, 389, 1422, 713, 1554, 226, 172, 2337, 1302, 551, 2467, 554, 90, 289, 1271, 233, 1462, 224, 378, 324, 473, 2276, 549, 329, 227, 819, 466, 203, 908, 387, 169, 435, 375, 1490, 418, 241, 897, 424, 459, 53, 1215, 606, 340, 601, 279, 192, 1699, 2682, 248, 1075, 256, 3969, 561, 675, 401, 355, 325, 96, 226, 69, 2175, 1545, 525, 1066, 488, 366, 250, 389, 66, 57, 252, 1604, 244, 374, 798, 792, 142, 437, 885, 241, 1499, 2245, 572, 1528, 1935, 488, 874, 592, 797, 603, 1131, 452, 977, 480, 576, 230, 562, 7235, 707, 92, 462, 727, 867, 710, 325, 600, 451, 349, 2403, 2534, 77, 442, 729, 1579, 1196, 1577, 245, 198, 347, 385, 425, 402, 320, 51, 524, 1770, 374, 130, 1224, 1494, 783, 607, 955, 1274, 788, 1592, 1321, 347, 1018, 380, 408, 540, 358, 317, 1753, 170, 64, 1482, 459, 525, 219, 570, 65, 53, 232, 466, 344, 132, 491, 383, 770, 348, 105, 238, 321, 159, 388, 1799, 781, 58, 517, 391, 284, 228, 350, 280, 351, 695, 318, 60, 614, 1140, 663, 410, 647, 516, 1346, 2454, 537, 314, 364, 1166, 57, 430, 697, 925, 68, 498, 1799, 728, 380, 390, 1006, 323, 1231, 2197, 715, 275, 494, 353, 1655, 508, 232, 586, 941, 256, 198, 518, 391, 1498, 313, 803, 1591, 1939, 356, 321, 64, 706, 372, 213, 57, 536, 1342, 482, 145, 1750, 201, 2223, 284, 364, 350, 280, 465, 472, 273, 557, 407, 598, 266, 323, 2935, 260, 416, 2562, 241, 1367, 448, 374, 112, 527, 454, 308, 290, 451, 2960, 378, 48, 1095, 616, 63, 2024, 597, 778, 752, 778, 593, 1123, 389, 630, 495, 310, 356, 503, 1370, 1213, 304, 3097, 523, 1097, 392, 1990, 354, 316, 941, 369, 380, 427, 869, 393, 1490, 1170, 389, 445, 314, 256, 360, 551, 243, 714, 1418, 338, 522, 192, 329, 1551, 444, 176, 429, 259, 604, 390, 1781, 1708, 547, 300, 2074, 578, 303, 419, 371, 474, 2658, 1011, 438, 2363, 564, 93, 489, 2039, 911, 1425, 396, 798, 466, 595, 1571, 352, 384, 641, 61, 314, 2045, 410, 243, 593, 297, 132, 332, 2190, 433, 373, 1133, 495, 841, 2048, 1346, 199, 253, 282, 1152, 1140, 995, 733, 1061, 442, 280, 1126, 1321, 263, 367, 320, 1016, 305, 80, 175, 279, 2128, 915, 637, 377, 1083, 321, 339, 233, 1591, 351, 224, 394, 310, 211, 476, 1523, 257, 554, 465, 686, 207, 276, 474, 600, 1161, 190, 1049, 1680, 264, 84, 266, 319, 1045, 243, 208, 4331, 835, 304, 435, 525, 662, 750, 420, 436, 1463, 246, 1113, 60, 301, 357, 2699, 573, 2206, 425, 327, 736, 359, 416, 610, 284, 467, 538, 213, 244, 731, 1842, 79, 721, 591, 2617, 558, 338, 378, 1765, 307, 219, 241, 144, 2470, 705, 506, 280, 588, 2962, 644, 2130, 356, 1183, 117, 1367, 302, 289, 162, 78, 141, 297, 489, 130, 1259, 1284, 252, 1442, 455, 283, 68, 263, 812, 1727, 339, 257, 939, 1496, 2432, 1149, 684, 915, 967, 285, 920, 105, 445, 454, 279, 1599, 586, 582, 281, 1253, 402, 1011, 395, 635, 603, 650, 1245, 319, 405, 506, 452, 191, 765, 327, 1561, 260, 73, 185, 160, 239, 97, 1233, 2398, 948, 2328, 195, 416, 1642, 325, 700, 48, 1134, 491, 543, 426, 844, 871, 287, 286, 1841, 614, 1788, 319, 1042, 350, 310, 272, 97, 405, 474, 253, 318, 899, 2037, 258, 520, 271, 87, 260, 989, 59, 238, 62, 250, 1064, 564, 1406, 706, 491, 2364, 76, 319, 293, 1747, 226, 224, 1441, 1057, 1814, 2184, 392, 594, 251, 424, 293, 345, 202, 206, 524, 281, 715, 324, 454, 480, 2841, 558, 458, 623, 648, 259, 541, 236, 1064, 301, 357, 1009, 567, 213, 1304, 520, 813, 828, 606, 2566, 878, 551, 546, 364, 248, 1118, 387, 1879, 492, 395, 615, 340, 652, 596, 1898, 430, 278, 60, 476, 325, 472, 381, 1288, 64, 254, 58, 1249, 88, 1345, 246, 1370, 874, 365, 929, 745, 1091, 1730, 560, 851, 1218, 389, 530, 100, 341, 2354, 66, 419, 1494, 894, 2023, 472, 551, 60, 443, 1894, 867, 70, 1061, 674, 2808, 275, 488, 333, 1613, 62, 1054, 3347, 1796, 215, 352, 555, 351, 75, 323, 328, 304, 598, 75, 729, 471, 809, 1037, 307, 206, 342, 672, 664, 365, 844, 438, 203, 275, 240, 572, 476, 202, 217, 717, 69, 1695, 514, 51, 229, 442, 1659, 515, 795, 287, 799, 228, 1833, 238, 501, 527, 497, 939, 395, 257, 315, 464, 103, 622, 76, 161, 1295, 1679, 1098, 925, 271, 131, 808, 461, 64, 763, 2044, 291, 418, 427, 1140, 403, 544, 365, 930, 281, 101, 264, 77, 405, 341, 360, 576, 291, 128, 520, 502, 330, 1068, 179, 571, 402, 877, 68, 360, 539, 398, 406, 675, 749, 5315, 190, 297, 348, 120, 358, 738, 1810, 99, 474, 558, 479, 297, 417, 251, 242, 306, 1350, 980, 573, 393, 590, 204, 2133, 871, 423, 1239, 589, 1081, 522, 508, 4769, 341, 478, 336, 312, 288, 835, 548, 2087, 3361, 309, 1133, 411, 675, 239, 248, 458, 201, 61, 401, 357, 64, 1255, 542, 809, 1369, 416, 276, 416, 945, 998, 1551, 96, 1705, 615, 1196, 1339, 325, 5363, 230, 1146, 228, 394, 695, 274, 702, 359, 340, 1294, 94, 387, 1518, 789, 623, 844, 58, 74, 419, 1068, 559, 536, 362, 215, 1868, 354, 617, 390, 896, 408, 901, 564, 530, 537, 499, 474, 866, 1098, 788, 886, 297, 63, 311, 1046, 64, 351, 562, 275, 274, 596, 2082, 336, 353, 1391, 417, 1502, 279, 235, 721, 2160, 1073, 95, 130, 351, 388, 435, 241, 345, 420, 310, 339, 1784, 1125, 368, 1581, 1039, 155, 198, 147, 381, 208, 539, 2101, 570, 439, 610, 287, 217, 520, 290, 67, 1855, 282, 186, 93, 71, 83, 854, 362, 429, 428, 597, 325, 403, 304, 387, 405, 465, 331, 1421, 288, 1058, 325, 1467, 955, 373, 510, 502, 665, 342, 494, 80, 1353, 319, 369, 985, 462, 3347, 7479, 1195, 182, 361, 432, 340, 282, 1865, 612, 538, 253, 1070, 350, 443, 810, 374, 594, 446, 313, 517, 618, 346, 276, 214, 1458, 771, 462, 341, 374, 826, 346, 255, 302, 318, 323, 243, 504, 6803, 296, 434, 315, 590, 351, 299, 884, 247, 1056, 1397, 422, 67, 60, 309, 345, 899, 157, 365, 366, 673, 1092, 227, 358, 212, 840, 315, 241, 253, 374, 1042, 292, 550, 4561, 298, 350, 1293, 326, 1395, 306, 77, 847, 287, 263, 748, 226, 67, 299, 811, 249, 796, 275, 360, 403, 576, 238, 77, 2349, 585, 285, 374, 1195, 478, 410, 2924, 3228, 1474, 1476, 4500, 398, 971, 252, 379, 103, 451, 72, 261, 735, 498, 1145, 282, 934, 520, 146, 918, 91, 439, 533, 638, 418, 343, 1415, 407, 487, 309, 1573, 269, 604, 1648, 949, 1484, 291, 2277, 974, 67, 278, 360, 59, 596, 1011, 516, 361, 1299, 916, 406, 478, 389, 1964, 232, 265, 260, 239, 266, 1353, 1970, 589, 99, 531, 1454, 123, 283, 73, 236, 154, 174, 799, 136, 289, 234, 279, 704, 390, 387, 350, 209, 290, 297, 288, 286, 1362, 1041, 236, 301, 538, 568, 830, 524, 566, 265, 290, 391, 462, 321, 400, 310, 63, 217, 76, 305, 718, 62, 452, 350, 1182, 1121, 604, 284, 325, 926, 1109, 278, 1181, 518, 620, 223, 542, 262, 988, 268, 1658, 329, 5440, 1178, 418, 775, 275, 1320, 665, 3618, 198, 1144, 833, 2181, 1260, 2493, 601, 255, 2493, 1857, 238, 485, 692, 313, 482, 585, 291, 685, 443, 54, 1126, 309, 348, 1218, 1484, 324, 451, 67, 270, 269, 208, 553, 506, 315, 220, 1201, 284, 1669, 1098, 221, 336, 241, 513, 270, 382, 1029, 406, 1712, 839, 313, 984, 871, 1624, 565, 501, 967, 753, 515, 840, 499, 578, 1856, 366, 304, 203, 302, 2840, 432, 286, 249, 311, 3205, 496, 654, 454, 651, 317, 570, 259, 238, 1256, 77, 678, 1216, 372, 75, 1395, 233, 303, 321, 1375, 271, 2047, 3449, 330, 344, 300, 205, 262, 1335, 123, 477, 507, 654, 367, 399, 315, 67, 375, 245, 343, 1914, 228, 1316, 371, 100, 408, 1518, 1169, 2791, 2084, 258, 463, 314, 546, 995, 241, 211, 580, 248, 285, 493, 312, 378, 336, 307, 1736, 1926, 288, 357, 1574, 300, 75, 1516, 303, 173, 274, 340, 324, 189, 689, 394, 1698, 241, 335, 3134, 1322, 794, 364, 363, 648, 683, 289, 285, 4407, 464, 545, 312, 300, 290, 356, 184, 247, 329, 403, 502, 262, 401, 209, 219, 3600, 273, 244, 509, 2688, 292, 78, 408, 362, 325, 737, 342, 504, 601, 438, 518, 608, 709, 741, 1384, 2542, 230, 246, 1085, 380, 1911, 1042, 705, 298, 153, 373, 601, 592, 283, 96, 585, 328, 336, 986, 220, 247, 625, 393, 846, 346, 367, 623, 353, 198, 334, 617, 1583, 326, 441, 390, 4247, 225, 78, 460, 616, 1178, 356, 1296, 208, 298, 942, 402, 620, 425, 215, 69, 726, 337, 873, 265, 421, 718, 246, 307, 866, 1763, 1319, 1212, 319, 698, 434, 365, 283, 339, 410, 1167, 378, 1326, 1720, 289, 266, 269, 1461, 1190, 1232, 357, 413, 495, 1972, 2033, 2354, 434, 425, 696, 335, 249, 1768, 2435, 209, 526, 4863, 279, 462, 228, 348, 1516, 1477, 440, 280, 1046, 435, 308, 3114, 475, 581, 386, 336, 258, 502, 220, 524, 232, 868, 1650, 314, 280, 369, 306, 282, 361, 738, 298, 64, 412, 457, 662, 370, 325, 735, 310, 708, 97, 366, 807, 360, 463, 711, 2603, 372, 265, 276, 3258, 1676, 1282, 357, 1231, 531, 2912, 1000, 430, 377, 249, 559, 2023, 235, 220, 262, 448, 125, 414, 211, 1294, 124, 329, 389, 310, 399, 577, 680, 220, 352, 359, 311, 245, 764, 377, 70, 25, 1901, 948, 390, 1028, 343, 348, 339, 485, 305, 312, 724, 408, 607, 567, 461, 2119, 942, 414, 80, 182, 302, 197, 342, 998, 396, 470, 286, 506, 1555, 328, 463, 482, 337, 80, 795, 642, 631, 2230, 291, 311, 1312, 625, 459, 286, 1041, 1370, 324, 1086, 280, 226, 681, 245, 1373, 1926, 730, 248, 347, 1718, 238, 1545, 915, 2528, 521, 479, 748, 233, 716, 221, 1454, 1475, 339, 74, 539, 2370, 941, 137, 60, 3305, 237, 508, 255, 565, 370, 1696, 364, 369, 2399, 1530, 168, 523, 3553, 382, 312, 866, 1268, 783, 251, 1308, 233, 612, 511, 142, 345, 1195, 1641, 875, 270, 1584, 313, 312, 652, 611, 791, 395, 414, 286, 292, 1663, 322, 367, 5421, 370, 334, 313, 334, 130, 490, 2965, 561, 1251, 235, 1470, 451, 433, 3346, 389, 268, 2331, 2246, 345, 923, 285, 232, 380, 249, 98, 284, 418, 260, 2194, 2254, 281, 122, 516, 715, 1770, 47, 1378, 294, 379, 264, 1854, 1363, 782, 930, 473, 3268, 2175, 543, 249, 243, 392, 1262, 1523, 363, 135, 421, 902, 349, 910, 72, 257, 380, 368, 390, 294, 647, 618, 1934, 298, 637, 1432, 241, 897, 790, 1064, 168, 288, 569, 505, 445, 408, 384, 428, 1810, 2685, 572, 1819, 288, 1184, 332, 662, 870, 465, 335, 1192, 379, 317, 825, 341, 1096, 1197, 53, 388, 466, 306, 816, 587, 406, 311, 640, 1901, 1390, 102, 858, 563, 70, 479, 515, 287, 262, 389, 182, 469, 717, 353, 603, 250, 169, 4021, 362, 240, 427, 437, 698, 468, 458, 519, 1922, 232, 331, 1434, 308, 870, 244, 618, 705, 349, 378, 349, 543, 591, 418, 2900, 337, 487, 64, 213, 332, 449, 197, 2706, 989, 172, 756, 631, 348, 2431, 265, 339, 477, 385, 639, 996, 398, 632, 1215, 502, 1212, 1737, 611, 334, 100, 233, 481, 623, 62, 936, 1957, 254, 757, 1834, 300, 47, 113, 391, 1870, 1019, 564, 379, 90, 427, 294, 427, 527, 309, 440, 243, 1139, 338, 703, 254, 900, 742, 308, 359, 310, 584, 264, 798, 314, 1612, 236, 811, 464, 289, 442, 259, 492, 530, 611, 325, 418, 355, 148, 437, 492, 268, 1151, 243, 2235, 690, 414, 1325, 327, 557, 233, 769, 526, 1470, 437, 451, 236, 178, 305, 248, 1251, 351, 1393, 387, 1410, 987, 455, 1878, 1133, 801, 239, 491, 165, 301, 1038, 2296, 983, 1367, 250, 377, 908, 236, 888, 618, 306, 2181, 2310, 1444, 502, 361, 954, 212, 1149, 1185, 552, 37, 571, 872, 543, 223, 629, 372, 364, 332, 219, 387, 399, 1879, 306, 469, 73, 634, 361, 269, 301, 1012, 347, 301, 351, 493, 68, 360, 44, 506, 274, 256, 79, 391, 374, 1986, 324, 280, 398, 499, 91, 188, 1229, 1085, 695, 386, 242, 555, 344, 770, 120, 446, 232, 315, 1245, 442, 757, 1034, 415, 249, 432, 1458, 396, 307, 308, 483, 324, 711, 530, 348, 286, 243, 438, 2118, 350, 2048, 55, 260, 952, 1154, 47, 803, 546, 55, 443, 368, 140, 776, 457, 514, 1128, 211, 532, 334, 650, 1134, 993, 456, 1125, 652, 322, 237, 457, 347, 355, 315, 514, 337, 310, 1075, 432, 689, 319, 329, 345, 2178, 2118, 1209, 400, 2122, 448, 1686, 775, 2100, 268, 429, 309, 201, 1143, 1249, 261, 88, 841, 286, 1855, 962, 297, 375, 402, 307, 1156, 415, 416, 282, 395, 571, 1587, 345, 524, 715, 661, 848, 923, 469, 1267, 1102, 667, 257, 308, 1220, 475, 398, 817, 394, 1145, 890, 244, 332, 561, 3608, 414, 184, 839, 65, 817, 541, 314, 1476, 544, 221, 1741, 1315, 349, 461, 239, 596, 69, 396, 582, 623, 319, 524, 180, 97, 201, 357, 657, 452, 283, 398, 143, 899, 311, 212, 244, 369, 251, 221, 359, 2424, 692, 84, 277, 1413, 515, 278, 633, 745, 407, 319, 329, 800, 755, 323, 77, 4253, 1128, 424, 877, 678, 57, 295, 817, 413, 591, 139, 387, 1610, 374, 540, 580, 332, 63, 308, 471, 394, 1032, 265, 122, 436, 278, 1118, 1001, 278, 255, 373, 486, 750, 206, 693, 1201, 787, 1633, 87, 311, 564, 460, 716, 449, 471, 763, 894, 51, 1614, 354, 2074, 988, 1354, 1324, 497, 701, 249, 2423, 568, 827, 570, 440, 299, 256, 406, 267, 1357, 961, 256, 1020, 540, 927, 653, 267, 1131, 540, 779, 243, 234, 1362, 253, 379, 1610, 1620, 1756, 296, 2549, 362, 1599, 379, 1034, 1668, 354, 64, 314, 255, 256, 184, 1431, 233, 1824, 1574, 178, 315, 386, 155, 447, 496, 613, 313, 267, 367, 311, 2293, 304, 488, 1801, 633, 366, 132, 256, 278, 1042, 359, 356, 191, 389, 1369, 247, 62, 444, 246, 4188, 471, 398, 671, 1063, 88, 2068, 452, 293, 83, 298, 531, 563, 292, 2402, 1207, 998, 272, 808, 307, 160, 876, 522, 346, 710, 290, 269, 532, 494, 707, 930, 659, 1857, 150, 360, 351, 109, 2248, 201, 552, 394, 1662, 254, 169, 97, 305, 579, 409, 228, 320, 1802, 2608, 428, 409, 1776, 429, 582, 343, 158, 585, 124, 1219, 945, 1874, 909, 258, 218, 770, 2182, 895, 776, 370, 281, 268, 174, 548, 522, 614, 372, 2444, 261, 841, 2591, 104, 297, 707, 153, 681, 5144, 240, 235, 1365, 420, 361, 466, 308, 459, 693, 1884, 323, 245, 1104, 429, 1273, 1999, 1478, 906, 173, 833, 1511, 1531, 1020, 91, 108, 1619, 470, 3896, 524, 1735, 384, 590, 232, 60, 478, 223, 231, 1995, 408, 317, 259, 232, 398, 654, 371, 439, 2149, 337, 487, 2987, 863, 723, 469, 50, 440, 257, 2341, 509, 220, 273, 287, 341, 2523, 1343, 362, 253, 578, 1632, 122, 708, 3178, 361, 760, 3058, 1005, 347, 48, 464, 303, 497, 341, 252, 2477, 908, 618, 2845, 421, 681, 326, 243, 277, 179, 315, 1776, 1457, 870, 1018, 1208, 403, 1702, 425, 584, 1198, 433, 326, 715, 556, 216, 77, 1456, 108, 292, 195, 345, 330, 604, 576, 2118, 797, 636, 297, 453, 377, 64, 359, 216, 1965, 326, 1012, 359, 1015, 3298, 273, 108, 297, 1720, 391, 259, 601, 669, 476, 2329, 768, 526, 379, 549, 256, 1004, 2210, 161, 1089, 332, 1375, 372, 318, 266, 1139, 1414, 279, 262, 601, 388, 360, 340, 2186, 1363, 305, 427, 1381, 525, 125, 399, 192, 76, 1929, 289, 254, 69, 388, 792, 546, 1841, 648, 427, 169, 255, 409, 745, 53, 1178, 443, 240, 580, 588, 665, 336, 282, 607, 1354, 331, 627, 211, 372, 326, 214, 296, 253, 620, 1251, 363, 877, 61, 337, 1000, 496, 410, 1761, 611, 417, 230, 305, 614, 298, 2533, 420, 1453, 765, 482, 341, 2506, 127, 794, 495, 285, 535, 343, 1498, 752, 185, 365, 467, 75, 384, 70, 609, 331, 401, 449, 2108, 297, 393, 808, 443, 149, 397, 984, 651, 693, 1404, 267, 352, 2053, 150, 502, 554, 339, 2969, 89, 563, 768, 654, 412, 553, 756, 435, 4236, 200, 603, 840, 375, 723, 3014, 1108, 286, 355, 303, 315, 728, 713, 1617, 383, 346, 455, 308, 481, 286, 297, 374, 331, 538, 2768, 1595, 203, 367, 417, 1131, 408, 348, 561, 219, 273, 73, 1267, 250, 598, 430, 365, 372, 1284, 2646, 1070, 1504, 319, 281, 448, 463, 266, 336, 389, 954, 1066, 258, 2306, 618, 442, 3885, 320, 1571, 243, 304, 668, 1266, 672, 310, 123, 1077, 254, 323, 327, 74, 350, 59, 1082, 666, 1444, 247, 57, 365, 434, 358, 482, 337, 378, 421, 697, 1259, 2063, 250, 1845, 240, 465, 422, 568, 2000, 407, 316, 340, 1182, 354, 271, 2244, 358, 581, 264, 325, 521, 1292, 262, 335, 269, 369, 1184, 755, 59, 409, 680, 401, 343, 226, 1023, 291, 1016, 606, 844, 1447, 1806, 507, 200, 317, 373, 330, 372, 825, 1693, 232, 817, 353, 1065, 1639, 399, 551, 322, 294, 304, 808, 629, 1267, 277, 228, 1119, 365, 250, 410, 864, 1694, 1292, 1054, 1286, 210, 714, 892, 2654, 568, 801, 314, 731, 285, 298, 369, 374, 726, 316, 251, 1533, 197, 115, 245, 1004, 312, 615, 1033, 442, 1327, 1413, 309, 1707, 730, 842, 362, 979, 284, 81, 530, 510, 365, 619, 534, 3399, 947, 347, 364, 706, 337, 320, 323, 591, 227, 447, 791, 1152, 55, 850, 2179, 234, 320, 1707, 196, 460, 304, 249, 356, 297, 309, 869, 61, 483, 447, 977, 788, 1823, 1180, 241, 183, 3716, 1540, 237, 1028, 914, 327, 418, 334, 1510, 326, 116, 425, 131, 77, 61, 319, 66, 375, 289, 406, 2250, 492, 892, 503, 230, 1372, 474, 1571, 796, 431, 452, 286, 424, 2046, 203, 868, 572, 1825, 421, 474, 401, 54, 317, 1099, 1091, 617, 415, 1309, 509, 458, 373, 267, 986, 275, 2175, 1613, 314, 342, 804, 956, 1062, 255, 236, 142, 345, 362, 2548, 387, 314, 774, 307, 173, 457, 305, 114, 2139, 81, 433, 304, 6054, 1506, 604, 110, 675, 1013, 640, 271, 1756, 298, 361, 1370, 399, 1270, 428, 580, 476, 5286, 917, 594, 397, 618, 288, 305, 354, 1074, 1218, 311, 331, 976, 259, 2502, 958, 253, 755, 391, 1049, 244, 463, 281, 1013, 1049, 430, 230, 863, 1044, 1171, 1165, 59, 2713, 328, 1455, 293, 353, 3104, 389, 325, 994, 829, 255, 281, 2104, 43, 335, 858, 244, 933, 321, 1591, 677, 422, 448, 4209, 820, 331, 242, 368, 1755, 2326, 457, 325, 525, 491, 687, 1050, 519, 64, 876, 293, 142, 318, 268, 533, 402, 353, 368, 204, 500, 280, 254, 902, 763, 1359, 369, 310, 54, 2540, 319, 1529, 1140, 365, 918, 266, 405, 308, 1456, 169, 340, 344, 949, 1940, 881, 164, 345, 321, 1446, 562, 1210, 79, 269, 910, 473, 248, 1087, 1817, 335, 325, 414, 158, 822, 145, 288, 446, 886, 1743, 3023, 96, 1545, 310, 799, 247, 348, 753, 289, 207, 265, 411, 463, 2105, 275, 1133, 456, 263, 112, 326, 266, 355, 247, 616, 1399, 299, 410, 273, 1012, 305, 234, 399, 1339, 905, 679, 254, 221, 324, 233, 1495, 116, 341, 326, 2590, 277, 293, 1221, 438, 463, 274, 329, 149, 1684, 306, 146, 1472, 1778, 1443, 261, 2651, 260, 448, 448, 265, 54, 405, 424, 897, 332, 1903, 594, 1484, 80, 823, 561, 392, 651, 62, 1142, 318, 351, 243, 380, 658, 2501, 337, 792, 63, 377, 381, 233, 362, 482, 300, 262, 740, 350, 745, 264, 394, 588, 451, 595, 388, 341, 76, 384, 596, 249, 619, 1123, 6525, 160, 1622, 420, 1527, 506, 1254, 729, 304, 395, 376, 1148, 781, 298, 73, 236, 301, 4801, 1917, 308, 281, 551, 540, 950, 121, 355, 68, 884, 307, 1106, 263, 644, 378, 272, 280, 1211, 4585, 967, 1893, 342, 470, 1429, 565, 861, 171, 124, 395, 79, 332, 376, 309, 289, 1561, 613, 458, 497, 369, 595, 245, 623, 387, 392, 377, 120, 240, 444, 316, 250, 499, 275, 435, 386, 233, 994, 326, 328, 405, 794, 322, 1185, 301, 154, 411, 298, 1251, 536, 546, 597, 1544, 2739, 264, 330, 778, 280, 56, 1519, 248, 70, 330, 416, 363, 345, 1689, 260, 114, 306, 399, 73, 351, 397, 428, 438, 339, 56, 1010, 1034, 369, 877, 57, 325, 814, 1645, 271, 363, 382, 987, 219, 309, 454, 553, 261, 270, 1173, 53, 233, 710, 883, 2508, 1454, 317, 300, 286, 705, 427, 222, 494, 1598, 611, 894, 494, 307, 1114, 223, 1817, 301, 493, 267, 318, 811, 214, 302, 323, 417, 210, 139, 1622, 59, 226, 410, 576, 369, 581, 361, 1407, 101, 734, 1367, 484, 314, 415, 349, 321, 498, 368, 205, 308, 438, 379, 335, 509, 638, 488, 391, 281, 369, 2553, 201, 442, 607, 1093, 616, 608, 266, 357, 245, 996, 1039, 257, 342, 830, 540, 324, 150, 951, 423, 340, 303, 1008, 1926, 1207, 207, 267, 626, 479, 757, 349, 502, 363, 629, 1737, 737, 571, 961, 825, 489, 523, 1345, 658, 303, 580, 389, 1442, 685, 444, 351, 938, 652, 368, 663, 2141, 499, 238, 762, 236, 111, 408, 444, 432, 1793, 637, 782, 356, 340, 259, 3514, 2020, 2108, 296, 1197, 606, 154, 94, 255, 289, 1091, 79, 303, 442, 400, 355, 280, 267, 1333, 770, 370, 143, 336, 311, 331, 236, 2156, 994, 604, 558, 337, 234, 1467, 6287, 344, 1464, 559, 265, 1024, 4228, 680, 459, 1711, 202, 449, 2531, 304, 354, 866, 611, 600, 351, 2983, 1465, 560, 240, 224, 605, 372, 565, 478, 368, 207, 819, 494, 723, 1045, 248, 328, 322, 489, 270, 76, 5408, 372, 71, 341, 2599, 1568, 294, 491, 129, 219, 2867, 590, 1744, 464, 1132, 263, 415, 1443, 295, 358, 338, 223, 1228, 1042, 3733, 795, 586, 583, 313, 263, 406, 1027, 933, 1628, 344, 306, 503, 264, 992, 343, 402, 35, 598, 124, 1175, 389, 952, 286, 270, 1161, 273, 591, 1400, 544, 896, 386, 609, 774, 2880, 384, 377, 273, 489, 733, 1962, 1144, 682, 1393, 1878, 851, 771, 340, 333, 312, 1231, 407, 394, 327, 1303, 347, 236, 424, 47, 309, 765, 381, 561, 1145, 60, 267, 266, 1302, 848, 711, 512, 1394, 613, 99, 518, 542, 68, 366, 3186, 338, 1488, 852, 1726, 253, 326, 326, 1507, 314, 802, 336, 330, 243, 265, 2301, 387, 62, 316, 409, 413, 379, 708, 835, 1311, 302, 290, 64, 412, 347, 289, 841, 1293, 255, 305, 337, 405, 478, 436, 489, 224, 301, 557, 299, 72, 275, 1126, 225, 400]}],
                        {"bargap": 0.2, "bargroupgap": 0.2, "template": {"data": {"bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "bar"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "barpolar"}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "choropleth": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "choropleth"}], "contour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "contour"}], "contourcarpet": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "contourcarpet"}], "heatmap": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmap"}], "heatmapgl": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmapgl"}], "histogram": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "histogram"}], "histogram2d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2d"}], "histogram2dcontour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2dcontour"}], "mesh3d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "mesh3d"}], "parcoords": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "parcoords"}], "pie": [{"automargin": true, "type": "pie"}], "scatter": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter"}], "scatter3d": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter3d"}], "scattercarpet": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattercarpet"}], "scattergeo": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergeo"}], "scattergl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergl"}], "scattermapbox": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattermapbox"}], "scatterpolar": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolar"}], "scatterpolargl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolargl"}], "scatterternary": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterternary"}], "surface": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "surface"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}]}, "layout": {"annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]], "sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}, "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "geo": {"bgcolor": "white", "lakecolor": "white", "landcolor": "#E5ECF6", "showlakes": true, "showland": true, "subunitcolor": "white"}, "hoverlabel": {"align": "left"}, "hovermode": "closest", "mapbox": {"style": "light"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "ternary": {"aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "title": {"x": 0.05}, "xaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}, "yaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}}}, "title": {"text": "Distribution of word counts within comments"}, "xaxis": {"title": {"text": "Word Count"}}, "yaxis": {"title": {"text": "Frequency"}}},
                        {"responsive": true}
                    ).then(function(){
                            
var gd = document.getElementById('b1d71f95-7255-428d-bca9-419a46b0a3b1');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            </script>
        </div>


</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">minlim</span><span class="p">,</span> <span class="n">maxlim</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="mf">1e4</span>
<span class="c1"># remove comments with the length of fewer than 50 words</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'size of the data before remove: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">))</span>
<span class="n">train_df</span><span class="p">[</span><span class="s1">'words_len'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">'words_len'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">len_t</span><span class="p">:</span> <span class="n">len_t</span> <span class="k">if</span> <span class="n">minlim</span> <span class="o">&lt;</span> <span class="n">len_t</span> <span class="o">&lt;=</span> <span class="n">maxlim</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">'words_len'</span><span class="p">])</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'size of the data after remove: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>size of the data before remove:  6896
size of the data after remove:  6865
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Distribution-of-Classes">
<a class="anchor" href="#Distribution-of-Classes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distribution of Classes<a class="anchor-link" href="#Distribution-of-Classes"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">topic_freq</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">label_id</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Bar</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">topic_freq</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">topic_freq</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
    <span class="n">title_text</span><span class="o">=</span><span class="s1">'Distribution of each class'</span><span class="p">,</span>
    <span class="n">xaxis_title_text</span><span class="o">=</span><span class="s1">'classes'</span><span class="p">,</span>
    <span class="n">yaxis_title_text</span><span class="o">=</span><span class="s1">'Frequency'</span><span class="p">,</span>
    <span class="n">bargap</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">bargroupgap</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<meta charset="utf-8">

    <div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>
                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>    
            <div id="f2da18d0-be39-4155-b7fd-63beafc1b10a" class="plotly-graph-div" style="height:525px; width:100%;"></div>
            <script type="text/javascript">
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById("f2da18d0-be39-4155-b7fd-63beafc1b10a")) {
                    Plotly.newPlot(
                        'f2da18d0-be39-4155-b7fd-63beafc1b10a',
                        [{"type": "bar", "x": [3, 0, 5, 2, 6, 1, 4], "y": [2245, 1575, 1342, 1299, 206, 101, 97]}],
                        {"bargap": 0.2, "bargroupgap": 0.2, "template": {"data": {"bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "bar"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "barpolar"}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "choropleth": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "choropleth"}], "contour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "contour"}], "contourcarpet": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "contourcarpet"}], "heatmap": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmap"}], "heatmapgl": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmapgl"}], "histogram": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "histogram"}], "histogram2d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2d"}], "histogram2dcontour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2dcontour"}], "mesh3d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "mesh3d"}], "parcoords": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "parcoords"}], "pie": [{"automargin": true, "type": "pie"}], "scatter": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter"}], "scatter3d": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter3d"}], "scattercarpet": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattercarpet"}], "scattergeo": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergeo"}], "scattergl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergl"}], "scattermapbox": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattermapbox"}], "scatterpolar": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolar"}], "scatterpolargl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolargl"}], "scatterternary": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterternary"}], "surface": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "surface"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}]}, "layout": {"annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]], "sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}, "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "geo": {"bgcolor": "white", "lakecolor": "white", "landcolor": "#E5ECF6", "showlakes": true, "showland": true, "subunitcolor": "white"}, "hoverlabel": {"align": "left"}, "hovermode": "closest", "mapbox": {"style": "light"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "ternary": {"aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "title": {"x": 0.05}, "xaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}, "yaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}}}, "title": {"text": "Distribution of each class"}, "xaxis": {"title": {"text": "classes"}}, "yaxis": {"title": {"text": "Frequency"}}},
                        {"responsive": true}
                    ).then(function(){
                            
var gd = document.getElementById('f2da18d0-be39-4155-b7fd-63beafc1b10a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            </script>
        </div>


</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prepare-data-for-Albert-LM">
<a class="anchor" href="#Prepare-data-for-Albert-LM" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prepare data for Albert-LM<a class="anchor-link" href="#Prepare-data-for-Albert-LM"> </a>
</h2>
<ul>
<li>using transformers tokenizer</li>
<li>tiny and fast BERT-ish Language Model (just 70MB)<ul>
<li>the pre-trained model is from <a href="https://github.com/m3hrdadfi/albert-persian">Merhdad Farhani</a>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>this is the latest version from official repository</li>
<li>the new version removes half-space and newlines in its tokenizer (you don't have to handle it in your cleaning)</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">class_number</span> <span class="o">=</span> <span class="mi">7</span>
<span class="c1"># this is the latest version from official repository</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"HooshvareLab/albert-fa-zwnj-base-v2"</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AlbertTokenizerFast</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AlbertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Tokenization</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">cleaned</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'train tokenized'</span><span class="p">)</span>
<span class="n">test_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">test_df</span><span class="o">.</span><span class="n">cleaned</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'test tokenized'</span><span class="p">)</span>
<span class="n">valid_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">eval_df</span><span class="o">.</span><span class="n">cleaned</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'valid tokenized'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train tokenized
test tokenized
valid tokenized
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>create config for our classification task</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="c1"># create a dict for classes</span>
<span class="n">label2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">class_number</span><span class="p">))}</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">label2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="o">**</span><span class="p">{</span><span class="s1">'label2id'</span><span class="p">:</span> <span class="n">label2id</span><span class="p">,</span> <span class="s1">'id2label'</span><span class="p">:</span> <span class="n">id2label</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'label2id: </span><span class="si">{</span><span class="n">label2id</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'id2label: </span><span class="si">{</span><span class="n">id2label</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
label2id: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}
id2label: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training">
<a class="anchor" href="#Training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training<a class="anchor-link" href="#Training"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PyTorch">
<a class="anchor" href="#PyTorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>PyTorch<a class="anchor-link" href="#PyTorch"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AlbertForSequenceClassification</span>
<span class="c1"># set device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cpu'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DigiMagDs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encodings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="n">encodings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># create datasets</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">DigiMagDs</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">)</span>
<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">DigiMagDs</span><span class="p">(</span><span class="n">valid_encodings</span><span class="p">,</span> <span class="n">eval_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">)</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">DigiMagDs</span><span class="p">(</span><span class="n">test_encodings</span><span class="p">,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">)</span>

<span class="c1"># create Dataloders</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>My PyTorch Trainer</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span><span class="o">,</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="k">class</span> <span class="nc">TorchTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">train_dl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">valid_dl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">clear_output</span><span class="p">()</span>
        <span class="n">valid_acc</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch </span><span class="si">%2d</span><span class="s1">/</span><span class="si">%2d</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
            <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_model</span><span class="p">()</span>
            <span class="n">valid_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_model</span><span class="p">()</span>
            <span class="n">time_elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1"> Metrics: | train_acc: </span><span class="si">%.3f</span><span class="s1"> | valid_acc: </span><span class="si">%.3f</span><span class="s1"> |'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">valid_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">  Epoch complete in: </span><span class="si">%.0f</span><span class="s1">m </span><span class="si">%.0f</span><span class="s1">s </span><span class="se">\n</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">time_elapsed</span> <span class="o">//</span> <span class="mi">60</span><span class="p">,</span> <span class="n">time_elapsed</span> <span class="o">%</span> <span class="mi">60</span><span class="p">))</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">avg_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># forward</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
            <span class="c1"># backward</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="c1"># statistics of model training and print</span>
            <span class="n">avg_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">avg_loss</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">avg_acc</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"</span><span class="se">\r</span><span class="s2">  Train_Step: </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2"> | runing_loss: </span><span class="si">%.4f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">))</span>

        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">avg_acc</span><span class="p">])</span> <span class="o">/</span> <span class="n">N</span>

    <span class="k">def</span> <span class="nf">valid_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">avg_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">):</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
                <span class="n">avg_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">avg_loss</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">avg_acc</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"</span><span class="se">\r</span><span class="s2">  Valid_Step: </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2"> | runing_loss: </span><span class="si">%.4f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">))</span>

        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">avg_acc</span><span class="p">])</span> <span class="o">/</span><span class="n">N</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AlbertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># AdamW is just adam with fixing on weight decay (don't worry about it)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch  1/ 2
--------------------
  Train_Step: 427/426 | runing_loss: 0.3749
  Valid_Step: 47/47 | runing_loss: 0.2160
 Metrics: | train_acc: 0.893 | valid_acc: 0.941 |

  Epoch complete in: 11m 4s 

Epoch  2/ 2
--------------------
  Train_Step: 427/426 | runing_loss: 0.2136
  Valid_Step: 47/47 | runing_loss: 0.2005
 Metrics: | train_acc: 0.942 | valid_acc: 0.935 |

  Epoch complete in: 10m 55s 

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We achieve 94% just in 2 epochs <ul>
<li>and without any hyperparameters optimization</li>
<li>Not bad ha?</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tensorflow">
<a class="anchor" href="#Tensorflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tensorflow<a class="anchor-link" href="#Tensorflow"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFAlbertForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>creat tf_dataset</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="nb">dict</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">),</span> <span class="n">train_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">))</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'train dataset created'</span><span class="p">)</span>

<span class="n">test_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="nb">dict</span><span class="p">(</span><span class="n">test_encodings</span><span class="p">),</span> <span class="n">test_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">))</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'test dataset created'</span><span class="p">)</span>

<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="nb">dict</span><span class="p">(</span><span class="n">valid_encodings</span><span class="p">),</span> <span class="n">eval_df</span><span class="o">.</span><span class="n">label_id</span><span class="p">))</span>
<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">valid_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'valid dataset created'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train dataset created
test dataset created
valid dataset created
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>the original model is based on PyTorch</li>
<li>If you need use Albert-Persian in TF you have to use <code>from_pt=True</code>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAlbertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># compile model</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">,</span> <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">])</span> <span class="c1"># you can also use any keras loss fn</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "tf_albert_for_sequence_classification_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
albert (TFAlbertMainLayer)   multiple                  11683584  
_________________________________________________________________
dropout_18 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  5383      
=================================================================
Total params: 11,688,967
Trainable params: 11,688,967
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1/2
426/426 [==============================] - 920s 2s/step - loss: 0.5785 - acc: 0.8232 - val_loss: 0.2331 - val_acc: 0.9295
Epoch 2/2
426/426 [==============================] - 875s 2s/step - loss: 0.2243 - acc: 0.9400 - val_loss: 0.1981 - val_acc: 0.9348
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'test loss: '</span><span class="p">,</span> <span class="n">metrics</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'test acc: '</span><span class="p">,</span> <span class="n">metrics</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>53/53 [==============================] - 30s 562ms/step - loss: 0.2437 - acc: 0.9387
test loss:  0.2437356561422348
test acc:  0.9386792182922363
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-can-we-achieve-better-results">
<a class="anchor" href="#How-can-we-achieve-better-results" aria-hidden="true"><span class="octicon octicon-link"></span></a>How can we achieve better results<a class="anchor-link" href="#How-can-we-achieve-better-results"> </a>
</h2>
<ul>
<li>with hyper parameters tuning<ul>
<li>more epoch</li>
<li>different batch size</li>
<li>bigger max len </li>
<li>...</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="HuggingFace-ğŸ¤—">
<a class="anchor" href="#HuggingFace-%F0%9F%A4%97" aria-hidden="true"><span class="octicon octicon-link"></span></a>HuggingFace ğŸ¤—<a class="anchor-link" href="#HuggingFace-%F0%9F%A4%97"> </a>
</h2>
<ul>
<li>you can also train it with HuggingFace Trainer &amp; TFTrainer</li>
<li>for more details see <a href="https://huggingface.co/transformers/main_classes/trainer.html">this</a>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="In-The-End">
<a class="anchor" href="#In-The-End" aria-hidden="true"><span class="octicon octicon-link"></span></a>In The End<a class="anchor-link" href="#In-The-End"> </a>
</h1>
<ul>
<li>thanks for your attention</li>
<li>sorry, my English is not great but you can get the idea</li>
<li>write your comments and ideas below<ul>
<li>and say about my mistakes or ...</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>

<script type="application/vnd.jupyter.widget-state+json">
{"54cec73d094547bbacd4815d4c00df42": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_d316adf9ebe44588911978aeadf99da4", "IPY_MODEL_498b3e9c437e4dde8b75e6d85d01a545"], "layout": "IPY_MODEL_e50f4dcd25eb434391e9ede8fc0c413d"}}, "e50f4dcd25eb434391e9ede8fc0c413d": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d316adf9ebe44588911978aeadf99da4": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_8092f8184aea440692882c11a286f2ae", "max": 760, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_ea3c09d1c63c47b68a7b7f26902b84ce", "value": 760}}, "498b3e9c437e4dde8b75e6d85d01a545": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3b841ecf971a4cb6879401e1d0b54865", "placeholder": "\u200b", "style": "IPY_MODEL_58a70ab48c7d424eb7f78cadcc159e62", "value": " 760/760 [17:30&lt;00:00, 1.38s/B]"}}, "ea3c09d1c63c47b68a7b7f26902b84ce": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "8092f8184aea440692882c11a286f2ae": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "58a70ab48c7d424eb7f78cadcc159e62": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "3b841ecf971a4cb6879401e1d0b54865": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}
</script>


</body>
  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sajjjadayobi/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Sajjad Ayoubi</li>
          <li><a class="u-email" href="mailto:sadeveloper360@gmail.com">sadeveloper360@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sajjjadayobi" title="sajjjadayobi"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ayoubi_sajjad" title="ayoubi_sajjad"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
