{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkK1QqCbtK02"
   },
   "source": [
    "# A little Framework on Top of PyTorch\n",
    "> for learning how Keras, Lightning & Fastai work\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- image: images/narm.png\n",
    "- comments: true\n",
    "- author: Sajjad Ayoubi\n",
    "- categories: [implementation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHLxBFWR9CsB"
   },
   "source": [
    "## inspiration\n",
    "\n",
    "- one month ago I was thinking about frameworks like Keras, Lightning & Fastai\n",
    "  - these are on top of main frameworks like Tensorflow & PyTorch\n",
    "- I found that the implementation is best for finding out `How`\n",
    "- this was my inspiration\n",
    "  - I looked to lighting and Keras \n",
    "  - and try to choose the best part from those\n",
    "- the implementation is on `PyTorch`\n",
    "- you can look at this and write your one little FrameWork\n",
    "- I can't and don't want to create something like Keras from scratch\n",
    "  - the main thing in these frameworks are **Training Loop** and **Callbacks**\n",
    "  - we go through to implement these\n",
    "\n",
    "- it was a little challenge for my weekend\n",
    "  - I learned much from this project\n",
    "  - and now I know that what Keras and Lightning does\n",
    "  - the best thing that I learned was\n",
    "    - the source code is the King !!!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87XITB1nd4YK"
   },
   "source": [
    "## Impelementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwXQwtgo9Y3-"
   },
   "source": [
    "- first of all, we need to add some basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pVOqSo6M9YBG"
   },
   "outputs": [],
   "source": [
    "import time, sys, gc\n",
    "from copy import deepcopy\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCMpAu72d_kx"
   },
   "source": [
    "### Trainer\n",
    "the most important thing is Training Loop\n",
    "\n",
    "Model.fit in Keras, trainer in lighting\n",
    "- I wanna create a Trainer class that gives some stuff like:\n",
    "    - Model, training dataset, optimizer, and loss function\n",
    "    - and does fit functionality like Sklearn and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll8z5Sf9eqHg"
   },
   "source": [
    "the `__init__` function is huge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tucrG3pM9lu9"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_ds, valid_ds, optimizer, loss, \n",
    "                 train_bs=32, valid_bs=32, scheduler=None, scheduler_type='epoch', \n",
    "                 metrcis=[], workers=4, fp16=False, grad_clip_value='inf'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          model: your model that you want to train (nn.Moudle)\n",
    "          train_ds: any DataStructer that works with torch.Dataloder (DataSet, list, ...)\n",
    "          optimizer: nn.optim\n",
    "          loss: nn.losses or any function\n",
    "          train_bs: training batch size (int)\n",
    "          valid_bs: validation batch size (int)\n",
    "          scheduler: torch.optim.lr_scheduler\n",
    "          metrics: a list of function for custom matrix (list)\n",
    "          scheduler_type: scheduler stratigy ('epoch', 'batch')\n",
    "          grad_clip_value: value for gradient cilipping (int or 'inf')\n",
    "          workers: num workers in dataloders (int)\n",
    "          fp16: automatic mixed percision training (bool)\n",
    "        \n",
    "        Params:\n",
    "          params: a dict for saving details like (loss, step number, epoch number, ...)\n",
    "          stop_training: a bool for stopping training proccess \n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.loss = loss\n",
    "        self.metrcis = metrcis\n",
    "        self.grad_clip_value = grad_clip_value\n",
    "        self.fp16 = fp16\n",
    "        self.scaler = GradScaler(enabled=fp16)\n",
    "        self.workers = workers\n",
    "        self.stop_training = False\n",
    "        self.train_bs = train_bs\n",
    "        self.valid_bs = valid_bs\n",
    "        self.train_ds = train_ds\n",
    "        self.valid_ds = valid_ds \n",
    "        self.train_dl = self.get_train_dataloder()\n",
    "        self.params = {'train_steps': len(self.train_dl)}\n",
    "        if valid_ds==None:\n",
    "            self.valid_dl = None\n",
    "        else:\n",
    "            self.valid_dl = self.get_valid_dataloder()\n",
    "            self.params['valid_steps'] = len(self.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrXk7beFBKlu"
   },
   "source": [
    "- the first functionality of Trainer is to Create DataLoader from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP_YTOw7BTez"
   },
   "outputs": [],
   "source": [
    "    def get_train_dataloder(self):\n",
    "        # create a DataLoder for train \n",
    "        return DataLoader(self.train_ds, batch_size=self.train_bs, shuffle=True, num_workers=self.workers)      \n",
    "      \n",
    "    def get_valid_dataloder(self): \n",
    "        # create a DataLoder for valid\n",
    "        return DataLoader(self.valid_ds, batch_size=self.valid_bs, shuffle=False, num_workers=self.workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-kcoFRaCXcf"
   },
   "source": [
    "- the advantage of this implementation is you can override these functions\n",
    "- see the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKI-jCQIBtxc"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloder(self):\n",
    "      # I override this and change the shuffle to False\n",
    "      return DataLoader(self.train_ds, batch_size=self.train_bs, shuffle=False, num_workers=self.workers)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQXP_sQkC-sE"
   },
   "source": [
    "- `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDZcmhMzDENq"
   },
   "outputs": [],
   "source": [
    "    def fit(self, epochs=1, device='cuda', callbacks=[]):\n",
    "        \"\"\"\n",
    "        it trains your nn for any epochs that you want\n",
    "        \n",
    "        Args:\n",
    "            epochs: number of epochs for training (int)\n",
    "            device: running device (torch.device)\n",
    "            callbacks: list of callbacks that you want (list)\n",
    "        \"\"\"\n",
    "        # set params\n",
    "        if self.stop_training: return\n",
    "        self.params['epochs'] = epochs\n",
    "        self.change_device(device)\n",
    "        self.state = CallbackRunner(callbacks, trainer=self)\n",
    "        self.state.on_train_start\n",
    "        # start train and valid epoch\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.params['epoch'] = epoch\n",
    "            self.state.on_epoch_start\n",
    "            # train\n",
    "            self.state.on_train_epoch_start\n",
    "            self.train_epoch() # train_for_one_epoch_on_train_dl\n",
    "            self.state.on_train_epoch_end\n",
    "            # schedule the learning rate\n",
    "            if self.scheduler != None and self.scheduler_type == 'epoch':\n",
    "                self.scheduler.step()\n",
    "            # valid\n",
    "            if self.valid_dl!=None:\n",
    "                self.state.on_valid_epoch_start\n",
    "                self.valid_epoch() # train_for_one_epoch_on_valid_dl\n",
    "                self.state.on_valid_epoch_end    \n",
    "              \n",
    "            self.state.on_epoch_end\n",
    "            if self.stop_training: \n",
    "                break\n",
    "        self.state.on_train_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ju2UTl4GReC"
   },
   "source": [
    "- one of the utils is\n",
    "    - `change_device` function for simply changing device at the start of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-8Xy6QoGX1T"
   },
   "outputs": [],
   "source": [
    "    def change_device(self, device='cuda'):\n",
    "        \"\"\" change device of (model, loss, optimizer)\n",
    "        Args:\n",
    "            device: running device (torch.device)\n",
    "        \"\"\"\n",
    "        # save device to self\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.loss.to(device)\n",
    "        self.optimizer.load_state_dict(self.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OkqX8NGFc2M"
   },
   "source": [
    "- now we need a `train_epoch` function\n",
    "  - train model with train dataloder for one epoch\n",
    "  - we save all things with `self.prarms`\n",
    "- Keras and Lightning also can train on TPUs and has distrubuted training\n",
    "  - plus some other features\n",
    "  \n",
    "- now we need a train_epoch function\n",
    "    - train model with train Dataloader for one epoch\n",
    "    - we save all things with self.prarms\n",
    "- Keras and Lightning also can train on TPUs and has distributed training\n",
    "    - plus some other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VF8j_P70FiUo"
   },
   "outputs": [],
   "source": [
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        train the model for one epoch, it has:\n",
    "          callbacks state changer\n",
    "          device change for batch\n",
    "          autocast for mixed percision\n",
    "          backward \n",
    "          params saver\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        for step, batch in enumerate(self.train_dl, 0):\n",
    "          self.params['step'] = step\n",
    "          self.optimizer.zero_grad()\n",
    "          # set device for this batch\n",
    "          batch = [item.to(self.device) for item in batch]\n",
    "          self.params['batch'] = batch\n",
    "          self.state.on_train_batch_start\n",
    "\n",
    "          if self.device == 'cuda': # GPU + FP16\n",
    "              with autocast(enabled=self.fp16):\n",
    "                  loss, metrics = self.train_step(batch) # train_for_one_batch\n",
    "              # bawkward + gradient_cliping\n",
    "              self.backward_step(loss)\n",
    "              #  schedule_lr\n",
    "              if self.scheduler != None and self.scheduler_type == 'batch':\n",
    "                  self.scaler.step(self.scheduler)\n",
    "              self.scaler.update()\n",
    "\n",
    "          else: # like the above but on CPU\n",
    "              loss, metrics = self.train_step(batch)\n",
    "              self.backward_step(loss)\n",
    "              if self.scheduler != None and self.scheduler_type == 'batch':\n",
    "                  self.scheduler.step()\n",
    "\n",
    "          # save stats\n",
    "          self.params['loss'] = loss\n",
    "          self.params['metrics'] = metrics\n",
    "          self.state.on_train_batch_end\n",
    "          if self.stop_training: \n",
    "              break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `backward_step` is normally the same except for things like GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward_step(self, loss):\n",
    "        if self.device == 'cuda':\n",
    "            self.scaler.scale(loss).backward()\n",
    "            clip_grad_norm_(self.model.parameters(), self.grad_clip_value)\n",
    "            self.scaler.step(self.optimizer)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(self.model.parameters(), self.grad_clip_value)\n",
    "            self.optimizer.step()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2YFFGmSHxaW"
   },
   "source": [
    "- we have the same function for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogZszo-GH2FK"
   },
   "outputs": [],
   "source": [
    "    def valid_epoch(self):\n",
    "      \"\"\"\n",
    "      train model for one epoch on valid_ds, it has:\n",
    "          callbacks state changer\n",
    "          device change for batch\n",
    "          autocast for mixed percision\n",
    "          params saver\n",
    "      \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(self.valid_dl, 0):\n",
    "              self.params['step'] = step\n",
    "              batch = [item.to(self.device) for item in batch]\n",
    "              self.params['batch'] = batch\n",
    "              self.state.on_valid_batch_start\n",
    "\n",
    "              if self.device == 'cuda': # GPU + FP16\n",
    "                  with autocast(enabled=self.fp16):\n",
    "                      self.params['loss'], self.params['metrics'] = self.valid_step(batch) # train_for_one_step\n",
    "              else: # CPU\n",
    "                self.params['loss'], self.params['metrics'] = self.valid_step(batch)\n",
    "              \n",
    "              self.state.on_valid_batch_end\n",
    "              if self.stop_training: \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd_6kzCPIPLV"
   },
   "source": [
    "- now we need normal `train_step` & `valid_step`\n",
    "    - these function gave a batch of data and do one train step on it\n",
    "    - Keras & Lightning have the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVMWKwqfIW4Y"
   },
   "outputs": [],
   "source": [
    "    def train_step(self, batch):\n",
    "        \"\"\" train on one batch and compute metrics\n",
    "        Args:\n",
    "            batch: training batch (From Dataloder)\n",
    "        Returns:\n",
    "            loss: any loss that you compute\n",
    "            metrics: come from compute_metrics function\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, y)\n",
    "        metrcis = self.compute_metrics(out, y)\n",
    "        return loss, metrcis\n",
    "\n",
    "    def valid_step(self, batch):\n",
    "        \"\"\" train on one batch of valid and compute metrics\n",
    "        Args:\n",
    "            batch: training batch (From Dataloder)\n",
    "        Returns:\n",
    "            loss: any loss that you compute\n",
    "            metrics: come from compute_metrics function\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, y)\n",
    "        metrcis = self.compute_metrics(out, y)\n",
    "        return loss, metrcis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJgB5kf2IsDc"
   },
   "source": [
    "the main advantage is you can Override `train_step` & `valid_step`\n",
    "  - and create your own `Trainer`\n",
    "  - just like `Keras SubClassing` and `pl.LightningModule`\n",
    "- see the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bA9bmWoI28k"
   },
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer(Trainer):\n",
    "    def train_step(self, batch):\n",
    "        x, _ = batch # we don't need y for AEs\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, x) # compute reconstraction loss\n",
    "        metrcis = self.compute_metrics(out, y)\n",
    "        return loss, metrcis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYivL9bKAp5"
   },
   "source": [
    "- the second utils function is \n",
    "- `compute_metrics`:\n",
    "  - for computing all metrics that you pass to Trainer\n",
    "  - it saves those in a dict with name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq-kgHmFKBVs"
   },
   "outputs": [],
   "source": [
    "    def compute_metrics(self, pred, y):\n",
    "      \"\"\" Compute metrics that you passed in __init__\n",
    "      Args:\n",
    "          pred: prediction of the model\n",
    "          y: gronth truth\n",
    "      Returns:\n",
    "        res: a dict{metrics_name: result}\n",
    "      \"\"\"\n",
    "      res = {}\n",
    "      for f in self.metrcis:\n",
    "        # use name of that functin for Logging\n",
    "        res[f.__name__] = f(pred, y).item()\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkmgmMLkimZ"
   },
   "source": [
    "- now we have a really simple training that can work with these:\n",
    "  - metrics \n",
    "  - callbacks\n",
    "  - cpu, cuda\n",
    "  - fp16, fp32\n",
    "  - gradient cillping\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ct-e-x3kgZt"
   },
   "source": [
    "## CallBacks\n",
    "- the second important thing in frameworks is Callbacks\n",
    "- with the concept of callback, we can add anything that we want without\n",
    "    - changing the training process\n",
    "    - or any knowledge from Trainer\n",
    "- I think the callbacks are really great\n",
    "    - you can test new ideas (for example Adaptive Gradient Clipping)\n",
    "    - just with creating a Custom Callback and adding it to fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf3XFFV9LAw7"
   },
   "source": [
    "- we need a Basic callback class\n",
    "  - we can override from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtdu7bstLli7"
   },
   "outputs": [],
   "source": [
    "# Basic Class\n",
    "class Callback():\n",
    "    \"\"\" Base class for creating callbacks:\n",
    "          with self in this class and its child\n",
    "          you can access to all property from Trainer\n",
    "    \"\"\"\n",
    "    def set_trainer(self, trainer):\n",
    "        # get trainer object\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.trainer, attr)\n",
    "\n",
    "    # fit\n",
    "    def on_train_start(self): pass\n",
    "    def on_train_end(self): pass\n",
    "    def on_epoch_start(self): pass\n",
    "    def on_epoch_end(self): pass\n",
    "    # train\n",
    "    def on_train_epoch_start(self): pass\n",
    "    def on_train_epoch_end(self): pass\n",
    "    def on_train_batch_start(self): pass\n",
    "    def on_train_batch_end(self): pass\n",
    "    # valid\n",
    "    def on_valid_epoch_start(self): pass\n",
    "    def on_valid_epoch_end(self): pass\n",
    "    def on_valid_batch_start(self): pass\n",
    "    def on_valid_batch_end(self): pass\n",
    "    def on_valid_step_start(self): pass\n",
    "    def on_valid_step_end(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ0ZlgpSMKOv"
   },
   "source": [
    "- we need a CallBack Handler\n",
    "  - it gives a list of callbacks and runs all of them\n",
    "  - in each state like `on_train_start`\n",
    "  - and it has some `default_callbacks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PItIw3s3MWDl"
   },
   "outputs": [],
   "source": [
    "class CallbackRunner():\n",
    "    # callbacks runner (with default_callbacks)\n",
    "    def __init__(self, callbacks, trainer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          callbacks: list of callbacks (list)\n",
    "          trainer: trainer object (Trainer)\n",
    "        \"\"\"\n",
    "        # set StatsAvg, Reporter callbacks to default_callbacks\n",
    "        default_callbacks = [StatsAvg(), Reporter(after_step=1)]\n",
    "        self.callbacks = default_callbacks + callbacks\n",
    "        for cb in self.callbacks:\n",
    "          # set trainer object to all callbacks\n",
    "          cb.set_trainer(trainer)\n",
    "\n",
    "    # run all state with one function\n",
    "    def __getattr__(self, attr):\n",
    "        for cb in self.callbacks:\n",
    "          getattr(cb, attr)()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBlNpLZqOLb5"
   },
   "source": [
    "- StatsAvg is the most important callbacks that we need\n",
    "    - it keeps track of metrics and loss and computes the average of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FxsnW9OMHQ"
   },
   "outputs": [],
   "source": [
    "class StatsAvg(Callback):\n",
    "  # inner class for computing AVG with mommentom  \n",
    "  class AvgRunner():\n",
    "    def __init__(self, mom=None):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, step=1):\n",
    "        self.val = val\n",
    "        self.sum += val * step\n",
    "        self.count += step\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "  # helper functions\n",
    "  def __init__(self, mom=None):\n",
    "      self.mom = mom\n",
    "\n",
    "  def update_all(self, items, step):\n",
    "      for avg, item in zip(self.metrcis_runners, items):\n",
    "          avg.update(item, step)\n",
    "\n",
    "  def get_avgs(self, names):\n",
    "      return {n:a.avg for a, n in zip(self.metrcis_runners, names)}\n",
    "  \n",
    "\n",
    "  # valid section\n",
    "  def on_valid_epoch_start(self):\n",
    "    self.metrcis_runners = [self.AvgRunner(self.mom) for i in range(len(self.metrcis))]\n",
    "    self.loss_runner = self.AvgRunner(self.mom)\n",
    "\n",
    "  def on_valid_batch_end(self):\n",
    "    self.loss_runner.update(self.params['loss'].item(), self.valid_bs)\n",
    "    self.params['loss'] = self.loss_runner.avg\n",
    "    self.update_all(self.params['metrics'].values(), self.valid_bs)\n",
    "    self.params['metrics'] = self.get_avgs(self.params['metrics'].keys()) \n",
    "        \n",
    "  def on_valid_epoch_end(self):\n",
    "    del self.metrcis_runners\n",
    "    del self.loss_runner\n",
    "\n",
    "  # train section\n",
    "  def on_train_epoch_start(self):\n",
    "    self.metrcis_runners = [self.AvgRunner(self.mom) for i in range(len(self.metrcis))]\n",
    "    self.loss_runner = self.AvgRunner(self.mom)\n",
    "\n",
    "  def on_train_batch_end(self):\n",
    "    self.loss_runner.update(self.params['loss'].item(), self.train_bs)\n",
    "    self.params['loss'] = self.loss_runner.avg\n",
    "    self.update_all(self.params['metrics'].values(), self.train_bs)\n",
    "    self.params['metrics'] = self.get_avgs(self.params['metrics'].keys())     \n",
    "\n",
    "  def on_train_epoch_end(self):\n",
    "    del self.metrcis_runners\n",
    "    del self.loss_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89zouMUtPBEK"
   },
   "source": [
    "- after having the AvgStats we need a Reporter\n",
    "    - for printing stats like loss and metrics during training\n",
    "- in creating callbacks order is really important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w32V-nyFPKsD"
   },
   "outputs": [],
   "source": [
    "class Reporter(Callback):\n",
    "    def __init__(self, after_step=1):\n",
    "        self.after_step = after_step\n",
    "\n",
    "    def show(self, phase='train'):\n",
    "        report = '   loss: ' + str(round(self.params['loss'], 4))\n",
    "        report += '  ' + str('   ').join([f\"{k}: {round(v, 3)}\" for k, v in self.params['metrics'].items()])\n",
    "        sys.stdout.write(f\"\\r {phase}_steps: {self.params['step']+1}/{self.params[f'{phase}_steps']}\" + report)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    def on_valid_epoch_end(self): \n",
    "        print('', end='\\n') \n",
    "\n",
    "    def on_valid_batch_end(self): \n",
    "        if self.params['step']%self.after_step==0:\n",
    "            self.show('valid')\n",
    "\n",
    "    def on_train_batch_end(self):\n",
    "        if self.params['step']%self.after_step==0:\n",
    "            self.show('train')\n",
    "\n",
    "    def on_train_epoch_start(self): \n",
    "        print('\\n Epoch %2d/%2d' % (self.params['epoch'], self.params['epochs']))\n",
    "        print('-' * 75)\n",
    "        self.t0 = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        t1 = time.time() - self.t0\n",
    "        print('  time: %.0fm %.0fs' % (t1//60, t1%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB51IugtPvyD"
   },
   "source": [
    "- so far we saw default callbacks but\n",
    "  - we can create any callbacks that we want\n",
    "- look at the following\n",
    "  - a new callback for learning rate finding\n",
    "  - it's a child of `Callback` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGpsZp1oQBn4"
   },
   "outputs": [],
   "source": [
    "class LRFinder(Callback):\n",
    "    def __init__(self, min_lr=1e-6, max_lr=5e-1, mom=0.9, update_steps=1):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.mom = mom # Make loss smoother using momentum\n",
    "        self.update_steps = update_steps # update lr after X batch\n",
    "                \n",
    "    def on_train_start(self):\n",
    "        n_iter = self.params['train_steps']*self.params['epochs']    \n",
    "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, num=n_iter//self.update_steps+1)\n",
    "        self.losses = []\n",
    "        self.best_loss = 0\n",
    "\n",
    "    def on_train_batch_end(self):\n",
    "        # in callbacks we have access to all params from Trainer\n",
    "        loss = self.params['loss']\n",
    "        step = self.params['step']\n",
    "        if step!=0:\n",
    "            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n",
    "        if step==0 or loss < self.best_loss: \n",
    "            self.best_loss = loss\n",
    "        if step%self.update_steps==0:\n",
    "            self.losses.append(loss)            \n",
    "            # update lr\n",
    "            lr = self.learning_rates[step//self.update_steps]            \n",
    "            self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        # Stop criteria\n",
    "        if loss > self.best_loss*1.1:\n",
    "            self.trainer.stop_training = True         \n",
    "            \n",
    "    def on_train_end(self): \n",
    "        print('\\n')\n",
    "        plt.figure(dpi=80)\n",
    "        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n",
    "        plt.title(f'I think we find it: {min(self.losses)/10}')\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iRio9E8QlzM"
   },
   "source": [
    "### Test: MNIST Classification\n",
    "- now it's ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCqrJhsNR1br"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), (255))])\n",
    "train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VQMSi1tSfkn"
   },
   "source": [
    "- you can write any custom function for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZehVpZ6XSIFc"
   },
   "outputs": [],
   "source": [
    "# write your custom metrics\n",
    "def acc(pred, y):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    return torch.mean((pred == y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oMm0MfPSZJe"
   },
   "source": [
    "- we create model, loss, optimizer & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT8GMwTHSU7T"
   },
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    model = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 128), nn.ReLU(), nn.Linear(128, 10))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=.9)\n",
    "    return model, loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqg-uzj7SohI"
   },
   "source": [
    "- now the create a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89Qb4DBRSnLk"
   },
   "outputs": [],
   "source": [
    "model, loss, optimizer, scheduler = compile_model()\n",
    "learner = Trainer(model=model, train_ds=train_ds, optimizer=optimizer, loss=loss, metrcis=[acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPRPpIbaTXoh"
   },
   "source": [
    "- we need to create `lr_finder`\n",
    "- and train it just for one epoch\n",
    "  - you can see the acc right!!! \n",
    "  - and lr_finder works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 11787,
     "status": "ok",
     "timestamp": 1617631109954,
     "user": {
      "displayName": "Sajjad Ayobi",
      "photoUrl": "",
      "userId": "16298903796317944191"
     },
     "user_tz": -270
    },
    "id": "yAmacE3IS34l",
    "outputId": "f922baf3-1f5a-47ea-e7a4-25f3f19efdd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch  1/ 1\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 389/469   loss: 2.255  acc: 54.095  time: 0m 10s\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder = LRFinder(min_lr=1e-5, max_lr=1e1)\n",
    "learner.fit(epochs=1, callbacks=[lr_finder], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25445,
     "status": "ok",
     "timestamp": 1617631363089,
     "user": {
      "displayName": "Sajjad Ayobi",
      "photoUrl": "",
      "userId": "16298903796317944191"
     },
     "user_tz": -270
    },
    "id": "4KcZfEasVKYj",
    "outputId": "76d865aa-b486-4c87-8e77-807945f760ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch  1/ 2\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 469/469   loss: 0.9052  acc: 0.741  time: 0m 13s\n",
      "\n",
      " Epoch  2/ 2\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 469/469   loss: 0.3651  acc: 0.897  time: 0m 12s\n"
     ]
    }
   ],
   "source": [
    "learner.fit(epochs=2, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9FIuiKdV6Ra"
   },
   "source": [
    "- every thigs is right and now we can create any Trainer\n",
    "  - imagine I want to add a new feature to my Trainer\n",
    "  - the feature is something like `model.summary` in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HtSbxAbUlYq"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def summary(self):\n",
    "        summary(self.model.cpu(), input_size=(self.train_ds[0][0].shape), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1617631760338,
     "user": {
      "displayName": "Sajjad Ayobi",
      "photoUrl": "",
      "userId": "16298903796317944191"
     },
     "user_tz": -270
    },
    "id": "aIt94u2sUsKy",
    "outputId": "bed1567a-b3a4-406a-bef2-cf0a1aaa6be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 128]         100,480\n",
      "              GELU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model, loss, optimizer, scheduler = compile_model()\n",
    "\n",
    "learner = CustomTrainer(model=model, train_ds=train_ds, valid_ds=test_ds, optimizer=optimizer, loss=loss, metrcis=[acc])\n",
    "learner.summary() # new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44728,
     "status": "ok",
     "timestamp": 1617631808399,
     "user": {
      "displayName": "Sajjad Ayobi",
      "photoUrl": "",
      "userId": "16298903796317944191"
     },
     "user_tz": -270
    },
    "id": "0YNQPBBWXI79",
    "outputId": "c8d4fef8-b547-4e07-d485-f5e4f71567ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch  1/ 3\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 469/469   loss: 0.9163  acc: 0.742  time: 0m 12s\n",
      " valid_steps: 157/157   loss: 0.4111  acc: 0.887\n",
      "\n",
      " Epoch  2/ 3\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 469/469   loss: 0.3669  acc: 0.897  time: 0m 12s\n",
      " valid_steps: 157/157   loss: 0.3186  acc: 0.911\n",
      "\n",
      " Epoch  3/ 3\n",
      "---------------------------------------------------------------------------\n",
      " train_steps: 469/469   loss: 0.3193  acc: 0.909  time: 0m 12s\n",
      " valid_steps: 157/157   loss: 0.2934  acc: 0.915\n"
     ]
    }
   ],
   "source": [
    "learner.fit(epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-zlcvjlX9-M"
   },
   "source": [
    "- for practice you can add these functions to your Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnFUPX4uYEfS"
   },
   "outputs": [],
   "source": [
    "    def most_worse(self): pass\n",
    "    def freeze(self): pass\n",
    "    def unfreaze(self): pass\n",
    "    def save(self): pass\n",
    "    def load(self): pass\n",
    "    def resume(self): pass\n",
    "    def evaluate(self): pass\n",
    "    def repuodicible(self): pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdup3UlmXu89"
   },
   "source": [
    "## All in one: Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZiLVHEBXVWr"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_ds, valid_ds=None, train_bs=128, valid_bs=64, optimizer=None, loss=None, \n",
    "                 scheduler=None, scheduler_type='epoch', metrcis=[], workers=4, fp16=False, grad_clip_value='inf'):\n",
    "        \n",
    "        # without valid\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.loss = loss\n",
    "        self.metrcis = metrcis\n",
    "        self.grad_clip_value = grad_clip_value\n",
    "        self.fp16 = fp16\n",
    "        self.scaler = GradScaler(enabled=fp16)\n",
    "        self.workers = workers\n",
    "        self.stop_training = False # check\n",
    "        # prepare dls\n",
    "        self.train_bs = train_bs\n",
    "        self.valid_bs = valid_bs\n",
    "        self.train_ds = train_ds\n",
    "        self.valid_ds = valid_ds \n",
    "        self.train_dl = self.get_train_dataloder()\n",
    "        self.params = {'train_steps': len(self.train_dl)}\n",
    "        if valid_ds==None:\n",
    "            self.valid_dl = None\n",
    "        else:\n",
    "            self.valid_dl = self.get_valid_dataloder()\n",
    "            self.params['valid_steps'] = len(self.valid_dl)\n",
    "    \n",
    "\n",
    "        \n",
    "    # training \n",
    "    def fit(self, epochs=1, device='cuda', callbacks=[]):\n",
    "        if self.stop_training: return\n",
    "        self.params['epochs'] = epochs\n",
    "        self.change_device(device)\n",
    "        self.state = CallbackRunner(callbacks, trainer=self)\n",
    "        self.state.on_train_start\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.params['epoch'] = epoch\n",
    "            self.state.on_epoch_start\n",
    "            # train\n",
    "            self.state.on_train_epoch_start\n",
    "            self.train_epoch()\n",
    "            self.state.on_train_epoch_end\n",
    "            if self.scheduler != None and self.scheduler_type == 'epoch':\n",
    "                self.scheduler.step()\n",
    "            # valid\n",
    "            if self.valid_dl!=None:\n",
    "                self.state.on_valid_epoch_start\n",
    "                self.valid_epoch()\n",
    "                self.state.on_valid_epoch_end    \n",
    "              \n",
    "            self.state.on_epoch_end\n",
    "            if self.stop_training: \n",
    "                break\n",
    "        self.state.on_train_end\n",
    "        \n",
    "\n",
    "        \n",
    "    def train_epoch(self):\n",
    "      self.model.train()\n",
    "      for step, batch in enumerate(self.train_dl, 0):\n",
    "          self.params['step'] = step\n",
    "          self.optimizer.zero_grad()\n",
    "          batch = [item.to(self.device) for item in batch]\n",
    "          self.params['batch'] = batch\n",
    "          self.state.on_train_batch_start\n",
    "             \n",
    "          if self.device == 'cuda': # GPU + FP16\n",
    "              with autocast(enabled=self.fp16):\n",
    "                  loss, metrics = self.train_step(batch)\n",
    "              \n",
    "              self.scaler.scale(loss).backward()\n",
    "              clip_grad_norm_(self.model.parameters(), self.grad_clip_value)\n",
    "              self.scaler.step(self.optimizer)\n",
    "              if self.scheduler != None and self.scheduler_type == 'batch':\n",
    "                  self.scaler.step(self.scheduler)\n",
    "              self.scaler.update()\n",
    "\n",
    "          else: # CPU\n",
    "              loss, metrics = self.train_step(batch)\n",
    "              loss.backward()\n",
    "              clip_grad_norm_(self.model.parameters(), self.grad_clip_value)\n",
    "              self.optimizer.step()\n",
    "              if self.scheduler != None and self.scheduler_type == 'batch':\n",
    "                  self.scheduler.step()\n",
    "\n",
    "          # stats\n",
    "          self.params['loss'] = loss\n",
    "          self.params['metrics'] = metrics\n",
    "          self.state.on_train_batch_end\n",
    "          if self.stop_training: \n",
    "              break\n",
    "\n",
    "    \n",
    "    def valid_epoch(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(self.valid_dl, 0):\n",
    "              self.params['step'] = step\n",
    "              batch = [item.to(self.device) for item in batch]\n",
    "              self.params['batch'] = batch\n",
    "              self.state.on_valid_batch_start\n",
    "\n",
    "              if self.device == 'cuda': # GPU + FP16\n",
    "                  with autocast(enabled=self.fp16):\n",
    "                      self.params['loss'], self.params['metrics'] = self.valid_step(batch)\n",
    "              else:\n",
    "                self.params['loss'], self.params['metrics'] = self.valid_step(batch)\n",
    "              \n",
    "              self.state.on_valid_batch_end\n",
    "              if self.stop_training: \n",
    "                break\n",
    "        \n",
    "        \n",
    "\n",
    "    # -------------------------------------------------------------------------------- overrider\n",
    "    def train_step(self, batch):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, y)\n",
    "        metrcis = self.compute_metrics(out, y)\n",
    "        return loss, metrcis\n",
    "\n",
    "    def valid_step(self, batch):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, y)\n",
    "        metrcis = self.compute_metrics(out, y)\n",
    "        return loss, metrcis\n",
    "\n",
    "    def get_train_dataloder(self): \n",
    "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.train_bs, shuffle=True, num_workers=self.workers)      \n",
    "      \n",
    "    def get_valid_dataloder(self): \n",
    "        return torch.utils.data.DataLoader(self.valid_ds, batch_size=self.valid_bs, shuffle=False, num_workers=self.workers)\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------- utils\n",
    "    def change_device(self, device='cuda'):\n",
    "        self.model.to(device)\n",
    "        self.loss.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer.load_state_dict(self.optimizer.state_dict())\n",
    "\n",
    "\n",
    "    def compute_metrics(self, pred, y):\n",
    "        res = {}\n",
    "        for f in self.metrcis:\n",
    "            res[f.__name__] = f(pred, y).item()\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        summary(self.model.cpu(), input_size=(self.train_ds[0][0].shape), device='cpu')\n",
    "\n",
    "\n",
    "    def lr_finder(self, device='cuda', epochs=1, min_lr=1e-6, max_lr=1e1):\n",
    "        init_weights = deepcopy(model.state_dict())\n",
    "        # off valid epoch\n",
    "        valid_dl = self.valid_dl\n",
    "        self.valid_dl = None\n",
    "        self.fit(epochs=epochs, device=device, callbacks=[LRFinder(min_lr=min_lr, max_lr=max_lr)])\n",
    "        # reset training\n",
    "        self.model.load_state_dict(init_weights)\n",
    "        self.valid_dl = valid_dl\n",
    "        self.stop_training = False\n",
    "\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.optimizer.param_groups[0]['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-lf4wWFodGp"
   },
   "source": [
    "- I hope you learned somthing and enjoed"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkOdhDndCusjLwcHRYBqYT",
   "collapsed_sections": [],
   "name": "narm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
