{
  
    
        "post0": {
            "title": "Mixed Precision Training (PyTorch, TF2.X)",
            "content": "Most deep learning frameworks, including PyTorch &amp; Tensorflow, train with 32-bit floating point (FP32) arithmetic by default. However this is not essential to achieve full accuracy for many deep learning models. NVIDIA researchers developed a methodology for mixed-precision training, which combined single-precision (FP32) with half-precision (e.g. FP16) format when training a network, and achieved the same accuracy as FP32 training using the same hyperparameters, with additional performance benefits on NVIDIA GPUs: . Shorter training time; | Lower memory requirements, enabling larger batch sizes, larger models, or larger inputs. | . How Mixed Precision works . the original image is from NVIDIA Developer video . Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. Older GPUs offer no math performance benefit for using mixed precision, however memory and bandwidth savings can enable some speedups. You can look up the compute capability for your GPU at NVIDIA&#39;s CUDA GPU web page. Examples of GPUs that will benefit most from mixed precision include RTX GPUs, the V100, and the A100. . For many real-world models, mixed precision also allows you to double the batch size without running out of memory, as float16 tensors take half the memory | . Note: If running this guide in Google Colab, the GPU runtime typically has a P100 connected. The P100 has compute capability 6.0 and is not expected to show a significant speedup. | . Performance Benchmarks . FP16 on NVIDIA V100 vs. FP32 on V100 | . AMP with FP16 is the most performant option for DL training on the V100. In Table 1, we can observe that for various models, AMP on V100 provides a speedup of 1.5x to 5.5x over FP32 on V100 while converging to the same final accuracy. . . Figure 2. Performance of mixed precision training on NVIDIA 8xV100 vs. FP32 training on 8xV100 GPU. Bars represent the speedup factor of V100 AMP over V100 FP32. The higher the better. . !nvidia-smi -L . GPU 0: Tesla K80 (UUID: GPU-5bd1bf20-1646-572e-c986-9bc958d3acef) . Gradient Scaling . If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost. . To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero. . Each parameter’s gradient should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate. . we have tow steps . multiply the loss by some large number | after computing grads, we rescale the gradients with divide by that number to bring them back to their correct values | . | we can do it with something like this: (it&#39;s pseudocode) . loss = model(inputs) # We assume `grads` are float32. We do not want to divide float16 gradients grads = compute_gradient(loss*512, model.weights) grads /= 512 # then update the weights . | . Choosing a loss scale can be tricky. If the loss scale is too low, gradients may still underflow to zero. If too high, the opposite the problem occurs: the gradients may overflow to infinity. . don&#39;t wory about it framworks like PyTorch &amp; TF set it dynamicly for you | . PyTorch . For the PyTorch 1.6 release, developers at NVIDIA and Facebook moved mixed precision functionality into PyTorch core as the AMP package, torch.cuda.amp . amp stands for auto mixed persicion | . import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torchvision.models import mobilenet_v2 . the data pipeline is as usual | . transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), (255))]) train_ds = datasets.CIFAR10(&#39;./&#39;, download=True, transform=transform) train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4) . Files already downloaded and verified . from torch.cuda.amp import GradScaler # for gradient and loss sclaing from torch.cuda.amp import autocast # Casts operations in float16 &amp; 32 automaticly . define a new model as usual | . device = &#39;cuda&#39; model = mobilenet_v2() model.classifier = nn.Linear(1280, 10) model.to(device) # create optimizer and loss optimizer = optim.Adam(model.parameters(), lr=0.005) loss_fn = nn.CrossEntropyLoss().to(device) . Training . autocast() has no effect outside regions where it’s enabled | . fp16 = True # defince scaler for loss and grad scaleing # Creates once at the beginning of training scaler = GradScaler(enabled=fp16) loss_avg = 0.0 for i, (inputs, labels) in enumerate(train_dl): optimizer.zero_grad() # Casts operations to mixed precision with autocast(enabled=fp16): outputs = model(inputs.to(device)) loss = loss_fn(outputs, labels.to(device)) loss_avg = (loss_avg * i + loss) / (i+1) # Scales the loss, and calls backward() # to create scaled gradients scaler.scale(loss).backward() # Unscales gradients and calls # or skips optimizer.step() scaler.step(optimizer) scaler.update() # simple logging if i%100==0: print(&#39;[%d, %4d] loss: %.4f&#39; %(i+1, len(train_dl), loss_avg)) . [1, 391] loss: 2.4166 [101, 391] loss: 2.1503 [201, 391] loss: 1.9664 [301, 391] loss: 1.8769 . all together | . def train(fp16=True, device=&#39;cuda&#39;): scaler = GradScaler(enabled=fp16) loss_avg = 0.0 for i, (inputs, labels) in enumerate(train_dl): optimizer.zero_grad() # set_to_none=True here can modestly improve performance # Casts operations to mixed precision with autocast(enabled=fp16): outputs = model(inputs.to(device)) loss = loss_fn(outputs, labels.to(device)) loss_avg = (loss_avg * i + loss.item()) / (i+1) # Scales the loss, and calls backward() # to create scaled gradients scaler.scale(loss).backward() # you can use grad norm as usual torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=&#39;inf&#39;) # Unscales gradients and calls # or skips optimizer.step() scaler.step(optimizer) scaler.update() if i%100==0: print(&#39;[%d, %4d] loss: %.4f&#39; %(i, len(train_dl), loss_avg)) . train(fp16=True) . [0, 391] loss: 1.4084 [100, 391] loss: 1.4822 [200, 391] loss: 1.4601 [300, 391] loss: 1.4331 . train(fp16=False) . [0, 391] loss: 1.2830 [100, 391] loss: 1.2331 [200, 391] loss: 1.2164 [300, 391] loss: 1.2011 . Saving/Resuming . To save/resume Amp-enabled runs with bitwise accuracy, use scaler.state_dict and scaler.load_state_dict. | . When saving, save the scaler state dict alongside the usual model and optimizer state dicts. Do this either at the beginning of an iteration before any forward passes, or at the end of an iteration after scaler.update(). | . For More details Torch doc on Mixed precision . Keras &amp; TF 2.X . import tensorflow as tf from tensorflow.keras.applications import MobileNetV2 from tensorflow.keras.layers import Dense, Activation . To use mixed precision in Keras, you need to set a tf.keras.mixed_precision Policy, typically referred to as a dtype policy. Dtype policies specify the dtypes layers will run in. In this guide, you will construct a policy from the string &#39;mixed_float16&#39; and set it as the global policy. This will cause subsequently created layers to use mixed precision with a mix of float16 and float32. | . from tensorflow.keras import mixed_precision # set global dtype for all keras.layers mixed_precision.set_global_policy(&#39;mixed_float16&#39;) # default is float32, if you use TPUs change it to mixed_bfloat16 . Computations are done in float16 for performance | but variables must be kept in float32 for numeric stability. | . print(&#39;Compute dtype: &#39;, mixed_precision.global_policy().compute_dtype) print(&#39;Variable dtype: &#39;, mixed_precision.global_policy().variable_dtype) . Compute dtype: float16 Variable dtype: float32 . This example cast the input data from int8 to float32. We don&#39;t cast to float16 since the division by 255 is on the CPU, which runs float16 operations slower than float32 operations. In this case, the performance difference in negligible, but in general you should run input processing math in float32 if it runs on the CPU. The first layer of the model will cast the inputs to float16, as each layer casts floating-point inputs to its compute dtype. | . (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() x_train = x_train.astype(&#39;float32&#39;) / 255 x_test = x_test.astype(&#39;float32&#39;) / 255 . Each layer has a policy and uses the global policy by default which is float16 You can override the dtype of any layer to be float32 by passing dtype=&#39;float32&#39; | . | Very small toy models typically do not benefit from mixed precision, because overhead from the TensorFlow runtime typically dominates the execution time, making any performance improvement on the GPU negligible | . model = tf.keras.Sequential() model.add(MobileNetV2(include_top=False, input_shape=(32, 32, 3))) model.add(Dense(10)) # use global policy which is float16 # If your model ends in softmax, make sure it is float32. And regardless of what your model ends in, make sure the output is float32. model.add(Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;)) . Even if the model does not end in a softmax, the outputs should still be float32 for computing loss in float32 we need outputs on float32 | . | . if you have not acsses to your last layer (like in applications), use thisoutputs = keras.layers.Activation(&#39;linear&#39;, dtype=&#39;float32&#39;)(outputs) . | . model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) history = model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2) . Epoch 1/2 1250/1250 [==============================] - 72s 47ms/step - loss: 1.9050 - val_loss: 7.5213 Epoch 2/2 1250/1250 [==============================] - 55s 44ms/step - loss: 1.7229 - val_loss: 7.7043 . def train(fp16=True, epochs=1): # set floating point if fp16: mixed_precision.set_global_policy(&#39;mixed_float16&#39;) else: mixed_precision.set_global_policy(&#39;float32&#39;) # create &amp; compile model model = tf.keras.Sequential() model.add(MobileNetV2(include_top=False, input_shape=(32, 32, 3))) model.add(Dense(10)) model.add(Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;)) # last layer must be float32 model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) # training model.fit(x_train, y_train, epochs=epochs, batch_size=64) . train(fp16=True) . 782/782 [==============================] - 57s 55ms/step - loss: 1.6211 . train(fp16=False) . 782/782 [==============================] - 34s 37ms/step - loss: 1.6675 . Custom Training Loop (float16) . TensorFlow dynamically determines the gradient scale so you do not have to choose one manually. If you use keras.Model.fit, gradient scaling is done for you so you do not have to do any extra work. If you use a custom training loop, you must explicitly use the special optimizer wrapper keras.mixed_precision.LossScaleOptimizer in order to use loss (and grad) scaling | . If you use a custom training loop with mixed_float16 . you need to wrap your optimizer with a tf.keras.mixed_precision.LossScaleOptimizer | Then call optimizer.get_scaled_loss to scale the loss, | and optimizer.get_unscaled_gradients to unscale the gradients. | . | see the following example . | . class Fp16Training(tf.keras.Model): def train_step(self, data): x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # scale loss with optimizer scaled_loss = optimizer.get_scaled_loss(loss) # used scaled loss for compute gradient scaled_gradients = tape.gradient(scaled_loss, self.trainable_variables) # unscaled gradients to default value for stable training grads = optimizer.get_unscaled_gradients(scaled_gradients) self.optimizer.apply_gradients(zip(grads, self.trainable_variables)) # as usally self.compiled_metrics.update_state(y, y_pred) return {m.name: m.result() for m in self.metrics} # write it as usally def test_step(self, data): pass . model = tf.keras.Sequential() model.add(MobileNetV2(include_top=False, input_shape=(32, 32, 3))) model.add(Dense(10)) # last layer or outputs must be float32 if use from_logits=True set dtype in last Dense model.add(Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;)) # use custom trainig loop cuistom_model = Fp16Training(model.inputs, model.outputs) . optimizer = keras.optimizers.Adam() optimizer = mixed_precision.LossScaleOptimizer(optimizer) # compile model cuistom_model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=optimizer) . cuistom_model.fit(x_train, y_train, batch_size=32, epochs=1) . 1563/1563 [==============================] - 74s 42ms/step - loss: 1.9542 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2de9545090&gt; . For More details TF doc on Mixed precision . Performance . modern NVIDIA GPUs use a special hardware unit called Tensor Cores that can multiply float16 matrices very quickly. However, Tensor Cores requires certain dimensions of tensors to be a multiple of 8 | . Matmul dimensions are not Tensor Core-friendly. Make sure matmuls’ participating sizes are multiples of 8. (For NLP models with encoders/decoders, this can be subtle. Also, convolutions used to have similar size constraints for Tensor Core use, but for CuDNN versions 7.3 and later, no such constraints exist | . I think that now we need to define models like this: | . batch_size = 8*4 layer_input = 8*20 layer_output = 8*40 channel_number = 8*64 .",
            "url": "https://sajjjadayobi.github.io/blog/tips/2021/03/06/mixed-precision.html",
            "relUrl": "/tips/2021/03/06/mixed-precision.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Reproducibility in Deep Learning",
            "content": "Reproducibility ?! . deep learning training processes are stochastic in nature, During development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, also for comparing different things and evaluate new tricks and ideas we need to train our neural nets in a deterministic way | In the process of training a neural network, there are multiple stages where randomness is used, for example . random initialization of weights of the network before the training starts. | regularization, dropout, which involves randomly dropping nodes in the network while training. | optimization process like SGD or Adam also include random initializations. | . | we will see that how can we use Frameworks in a deterministic way . | note in deterministic training you are a bit slow than stochastic | . PyTorch . Mnist classification with Reproducibilityfrom PyTorch Team:Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds, also Deterministic operations are often slower than nondeterministic operations . | . the following works with all models (maybe not LSTMs I didn’t check that) | . import numpy as np import random, os import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms . create dataloder | . transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), (255))]) train_ds = datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform) # if you set augmentations set worker_init_fn=(random.seed(0)) and num_workers=0 in dataloder train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4) . the following works with all models | . def torch_seed(seed=0): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.cuda.manual_seed_all(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) . . def train(reproducibility=True, n_run=2, device=&#39;cuda&#39;): for n in range(n_run): print(&#39;run number: &#39;, n+1) # set seed before create your model if reproducibility: torch_seed(seed=0) # compile model model = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 128), nn.GELU(), nn.Linear(128, 10)).to(device) loss_fn = nn.CrossEntropyLoss().to(device) optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.0) # training loop loss_avg = 0.0 for i, data in enumerate(train_dl): inputs, labels = data optimizer.zero_grad() outputs = model(inputs.to(device)) loss = loss_fn(outputs, labels.to(device)) loss_avg = (loss_avg * i + loss) / (i+1) loss.backward() optimizer.step() if i%850==0: print(&#39;[%d, %4d] loss: %.4f&#39; %(i+1, len(train_dl), loss_avg)) . train(reproducibility=False) . run number: 1 [1, 1875] loss: 2.2943 [851, 1875] loss: 0.8099 [1701, 1875] loss: 0.5946 run number: 2 [1, 1875] loss: 2.2945 [851, 1875] loss: 0.8078 [1701, 1875] loss: 0.5921 . train(reproducibility=True) . run number: 1 [1, 1875] loss: 2.2983 [851, 1875] loss: 0.8051 [1701, 1875] loss: 0.5927 run number: 2 [1, 1875] loss: 2.2983 [851, 1875] loss: 0.8051 [1701, 1875] loss: 0.5927 . if you check your new ideas like me | you have to always see how much is overhead of your implementation | in pytorch for giving acutual time we use synchronize | . %%timeit # stay in GPUs until it done torch.cuda.synchronize() . Keras &amp; TF 2.X . Mnist classification with Reproducibilityfrom Keras Team:when running on a GPU, some operations have non-deterministic outputs, in particular tf.reduce_sum(). This is due to the fact that GPUs run many operations in parallel, so the order of execution is not always guaranteed. Due to the limited precision of floats, even adding several numbers together may give slightly different results depending on the order in which you add them. You can try to avoid the non-deterministic operations, but some may be created automatically by TensorFlow to compute the gradients, so it is much simpler to just run the code on the CPU. For this, you can set the CUDA_VISIBLE_DEVICES environment variable to an empty string . | . they said Keras REPRODUCIBILITY works just on CPUs | but we need GPUs | after a week seach I found a possible way on GPUs . based on this work TensorFlow Determinism from NVIDIA | now we can run Keras with REPRODUCIBILITY on GPUs :) | . | Note: it works just for TF &gt;= 2.3 . also it works fine with tf.data | but you have to watch out (especially prefetch) | . | . let&#39;s check this out | . import random, os import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step . def tf_seed(seed=0): os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) # For working on GPUs from &quot;TensorFlow Determinism&quot; os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = str(seed) np.random.seed(seed) random.seed(seed) tf.random.set_seed(seed) . def train(reproducibility=True, n_run=2): for n in range(n_run): print(&#39;run number: &#39;, n+1) # set seed before create your model if reproducibility: tf_seed(seed=0) # compile model model = tf.keras.models.Sequential([Flatten(input_shape=(28, 28)), Dense(128, activation=&#39;gelu&#39;), Dense(10)]) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(optimizer=&#39;adam&#39;, loss=loss_fn) # training model.fit(x_train, y_train, epochs=1) . train(reproducibility=False) . run number: 1 1875/1875 [==============================] - 4s 1ms/step - loss: 0.4279 run number: 2 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4214 . train(reproducibility=True) . run number: 1 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4124 run number: 2 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4124 . if you want run it on CPUs see this | . def tf_seed(seed=0): os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) # if your machine has GPUs use following to off it os.environ[&#39;CUDA_VISBLE_DEVICE&#39;] = &#39;&#39; np.random.seed(seed) random.seed(seed) python_random.seed(seed) tf.random.set_seed(seed) .",
            "url": "https://sajjjadayobi.github.io/blog/tips/2021/02/24/reproducibility.html",
            "relUrl": "/tips/2021/02/24/reproducibility.html",
            "date": " • Feb 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Super Resolution With Feature Losses",
            "content": "Task Defenition: Image Reconstruction . you can do any transformation on original image and reconstruct it with this Loss for example | noise, blur, crop, draw number or text, grayscale ... | . | . Structure of model from original (Perceptual Losses Paper) we use a U-net for Image Transform Net and VGG19 for Loss Network | . | . . One of the best example of Feature Loss implementation DeOldify | . . Prepare Dataset . from glob import glob from tqdm import tqdm from pathlib import Path from PIL import Image import matplotlib.pyplot as plt import numpy as np import time, sys, random from IPython.display import clear_output . Download . !wget https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz !tar -xvzf oxford-iiit-pet.tgz clear_output() . create low resolution images . a function for create low res image from original image works for other datasets | you can do any modification that you want | . | . def create_dataset(fn, path_save, size, low=True): dest = path_save / fn.relative_to(orig_path) dest.parent.mkdir(parents=True, exist_ok=True) img = Image.open(fn) q = 100 if low: q = 35 donw_size = 110 w, h = img.size ratio = (donw_size)/min(w, h) w , h = int(w*ratio), int(h*ratio) # convert to low size img = img.resize((w, h), Image.BILINEAR).convert(&#39;RGB&#39;) # back to original size img = img.resize((size, size), Image.BILINEAR).convert(&#39;RGB&#39;) img.save(dest, quality=q) . hight_res folder: original image convert to size 224 | low_res folder: convert to size 112 and back to 224, save with 35% quality | . orig_path = &#39;./oxford-iiit-pet/images&#39; path_high = &#39;./oxford-iiit-pet/high_res&#39; path_low = &#39;./oxford-iiit-pet/low_res&#39; size = 224 # # create high res images for i in tqdm(glob(f&#39;{orig_path}/*.jpg&#39;)): create_dataset(fn=Path(i) ,path_save=path_high, size=size, low=False) # create low res images for i in tqdm(glob(f&#39;{orig_path}/*.jpg&#39;)): create_dataset(fn=Path(i) ,path_save=path_low, size=size, low=True) . 100%|██████████| 7390/7390 [01:01&lt;00:00, 120.29it/s] 100%|██████████| 7390/7390 [00:50&lt;00:00, 145.70it/s] . low_res_paths = glob(f&#39;{path_low}/*.jpg&#39;) high_res_paths = glob(f&#39;{path_high}/*.jpg&#39;) . plot an example from our data | . plt.figure(dpi=300) idx = 256 x = plt.imread(low_res_paths[idx]) plt.subplot(1, 2, 1) plt.title(&#39;low res&#39;) plt.imshow(x) plt.axis(&#39;off&#39;) plt.subplot(1, 2, 2) plt.title(&#39;original&#39;) y = plt.imread(high_res_paths[idx]) plt.imshow(y) plt.axis(&#39;off&#39;) plt.show() . PyTorch . import torch from torch import nn from torch import optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms . Dataset &amp; DataLoder . class SuperResDataset(torch.utils.data.Dataset): def __init__(self, low_res, high_res, transform): self.low = low_res self.high = high_res self.transform = transform def __getitem__(self, i): # one from high as x and one from low as y x = Image.open(self.low[i]) y = Image.open(self.high[i]) if self.transform: x = self.transform(x) y = self.transform(y) return x, y def __len__(self): return len(self.low) . normalization with imagenet stats | . mu = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mu, std)]) # create ds and dl train_ds = SuperResDataset(low_res_paths, high_res_paths, transform=transform) # if you have small GPU decrease batch_size train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, num_workers=6, shuffle=True) . denormalize function for plot image | . def denormalize(inp): inp = inp.numpy().transpose((1, 2, 0)) inp = std * inp + mu inp = np.clip(inp, 0, 1) return inp . check some example from dataset | . plt.figure(dpi=300) x, y= train_ds[random.randint(0, len(train_ds))] plt.subplot(1, 2, 1) plt.imshow(denormalize(x)) plt.axis(&#39;off&#39;) plt.title(&#39;low res image&#39;) plt.subplot(1, 2, 2) plt.imshow(denormalize(y)) plt.axis(&#39;off&#39;) plt.title(&#39;original image&#39;) plt.show() . Custom Training Loop for PyTorch | . class TorchTrainer: def __init__(self, model, train_dl, optimizer, loss, valid_dl=None, cuda=False): self.model = model self.train_dl = train_dl self.valid_dl = valid_dl self.optimizer = optimizer self.loss = loss self.cuda = cuda if cuda: self.model = model.cuda() self.loss = loss.cuda() self.loss_history = [] def fit(self, num_epochs): clear_output() valid_acc = 0.0 for epoch in range(num_epochs): print(&#39;Epoch %2d/%2d&#39; % (epoch + 1, num_epochs)) print(&#39;-&#39; * 25) t0 = time.time() train_acc = self.train_model() if self.valid_dl: valid_acc = self.valid_model() time_elapsed = time.time() - t0 print(&#39; | Metrics: | train_loss: %.3f | valid_loss: %.3f |&#39; % (train_acc, valid_acc)) print(&#39; n Epoch complete in: %.0fm %.0fs n&#39; % (time_elapsed // 60, time_elapsed % 60)) return def train_model(self): self.model.train() N = len(self.train_dl.dataset) step = N // self.train_dl.batch_size avg_loss = 0.0 for i, (x, y) in enumerate(self.train_dl): if self.cuda: x = x.cuda() y = y.cuda() # forward reconst = self.model(x) loss = self.loss(reconst, y) # backward self.optimizer.zero_grad() loss.backward() self.optimizer.step() # statistics of model training avg_loss = (avg_loss * i + loss) / (i + 1) self.loss_history.append(avg_loss) # report statistics sys.stdout.flush() sys.stdout.write(&quot; r Train_Step: %d/%d | runing_loss: %.4f&quot;%(i+1, step, avg_loss)) sys.stdout.flush() return avg_loss . show_result: random example for model test plot [low-res-image, output-of-model, original-image] | . | . def show_result(model, train_ds): model.eval() plt.figure(dpi=300) x , y = train_ds[random.randint(0, len(train_ds))] rec = model(x.unsqueeze(0).cuda()).detach().cpu()[0] plt.axis(&#39;off&#39;) plt.subplot(1, 3, 1) plt.title(&#39;low res&#39;) plt.imshow(denormalize(x)) plt.axis(&#39;off&#39;) plt.subplot(1, 3, 2) plt.title(&#39;prediction&#39;) plt.imshow(denormalize(rec)) plt.axis(&#39;off&#39;) plt.subplot(1, 3, 3) plt.title(&#39;original&#39;) plt.imshow(denormalize(y)) plt.axis(&#39;off&#39;) plt.show() . Pre-trained Model for Feature Extraction . VGG19 with BatchNorm as pre-trained model for extract feature maps | . disc = torchvision.models.vgg19_bn(pretrained=True).features # freeze model for p in disc.parameters(): p.requires_grad = False # ready for test time disc = disc.eval() . Downloading: &#34;https://download.pytorch.org/models/vgg19_bn-c79401a0.pth&#34; to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth . . find index of layers before MaxPool2d (before shrink feature maps) | . layer_ids = [] for i, layer in enumerate(disc.children()): if isinstance(layer, nn.MaxPool2d): layer_ids.append(i-1) # which layers do you want to use for Feature loss # we need 4 feature maps for Feature Loss (it&#39;s like a hyper parameter) layer_ids[-4:] . [12, 25, 38, 51] . Featuer Loss . StoreModel: in order to extract feature maps on layers 12, 25, 38, 51 from VGG19_bn | . class StoreModel(nn.Module): &quot;&quot;&quot; get a Sequential Model and index of layers from that model return output of model on those layers &quot;&quot;&quot; def __init__(self, model, layer_ids): super().__init__() self.model = list(model.children()) self.layer_ids = layer_ids def forward(self, x): storage = [] for i, layer in enumerate(self.model): x = layer(x) # keep feature maps of layers 12, 25, 38, 51 for this input and return them if i in self.layer_ids: storage.append(x) return storage . a nn.Module for compute Feature Losses | . class FeatureLoss(nn.Module): &quot;&quot;&quot; discriminator: a pretrained model for extract feature maps from layers in layer_ids in this case VGG19_bn layer_ids: which layers do you want to use for Feature loss layer_weights: how much important these layers (a list of weights) &quot;&quot;&quot; def __init__(self, disc, layer_ids, layer_weights): super().__init__() self.disc = StoreModel(disc, layer_ids) self.layer_weights = layer_weights @staticmethod def gram_matrix(x): n,c,h,w = x.size() x = x.view(n, c, -1) return (x @ x.transpose(1,2))/(c*h*w) def forward(self, reconstruction, target): target_features = self.disc(target) # get output of layers in VGG19 for original image recon_features = self.disc(reconstruction) # get output of layers in VGG19 for reconstruction image # compute normal loss for x, x&#39; in AutoEncoders loss = F.l1_loss(reconstruction, target) # plus different between target feature maps and reconstruction feature maps for yf, xf, w in zip(target_features, recon_features, self.layer_weights): loss += F.l1_loss(yf, xf)*w loss += F.l1_loss(self.gram_matrix(yf), self.gram_matrix(xf))*w**2*5e3 return loss . Generator: AutoEncoder with U-net . ConvBlock: Convlayer -&gt; BatchNorm -&gt; ReLU | . class ConvBlock(nn.Module): # Conv + Batch + Relu def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride) self.bn = nn.BatchNorm2d(out_channels) def forward(self, x): return F.relu(self.bn(self.conv(x))) . Decoder of U-net base on Resnet50 block sizes | . class UpBlockForUNetWithResNet50(nn.Module): # Upsample -&gt; ConvBlock -&gt; ConvBlock def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None): super().__init__() if up_conv_in_channels == None: up_conv_in_channels = in_channels if up_conv_out_channels == None: up_conv_out_channels = out_channels self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2) self.conv_block_1 = ConvBlock(in_channels, out_channels) self.conv_block_2 = ConvBlock(out_channels, out_channels) def forward(self, up_x, down_x): x = self.upsample(up_x) x = torch.cat([x, down_x], 1) x = self.conv_block_1(x) x = self.conv_block_2(x) return x . Encoder of U-net with pre-trianed Resnet50 | . class UNetWithResnet50Encoder(nn.Module): DEPTH = 6 def __init__(self, resnet=None, n_classes=3): super().__init__() down_blocks = [] up_blocks = [] self.input_block = nn.Sequential(*list(resnet.children()))[:3] self.input_pool = list(resnet.children())[3] for bottleneck in list(resnet.children()): if isinstance(bottleneck, nn.Sequential): down_blocks.append(bottleneck) self.down_blocks = nn.ModuleList(down_blocks) self.middle = nn.Sequential(ConvBlock(2048, 2048), ConvBlock(2048, 2048)) up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024)) up_blocks.append(UpBlockForUNetWithResNet50(1024, 512)) up_blocks.append(UpBlockForUNetWithResNet50(512, 256)) up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128, up_conv_in_channels=256, up_conv_out_channels=128)) up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64, up_conv_in_channels=128, up_conv_out_channels=64)) self.up_blocks = nn.ModuleList(up_blocks) self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1) def forward(self, x, with_output_feature_map=False): pre_pools = dict() pre_pools[f&quot;layer_0&quot;] = x x = self.input_block(x) pre_pools[f&quot;layer_1&quot;] = x x = self.input_pool(x) for i, block in enumerate(self.down_blocks, 2): x = block(x) if i == (UNetWithResnet50Encoder.DEPTH - 1): continue pre_pools[f&quot;layer_{i}&quot;] = x x = self.middle(x) for i, block in enumerate(self.up_blocks, 1): key = f&quot;layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}&quot; x = block(x, pre_pools[key]) output_feature_map = x x = self.out(x) del pre_pools if with_output_feature_map: return x, output_feature_map else: return x . complete U-net with Freeze Encoder | . encoder = torchvision.models.resnet.resnet50(pretrained=True) # freeze for p in encoder.parameters(): p.requires_grad = False # create unet unet = UNetWithResnet50Encoder(resnet=encoder, n_classes=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . Training . optimizer = optim.Adam(lr=0.001, params=unet.parameters()) loss = FeatureLoss(disc.cuda(), layer_ids[-4:], [3, 6, 12, 4]) learner = TorchTrainer(model=unet, train_dl=train_dl, optimizer=optimizer, loss=loss, cuda=True) . learner.fit(num_epochs=10) . Epoch 1/10 - Train_Step: 462/461 | runing_loss: 4.7252 | Metrics: | train_loss: 4.725 | valid_loss: 0.000 | Epoch complete in: 7m 31s Epoch 2/10 - Train_Step: 462/461 | runing_loss: 4.1393 | Metrics: | train_loss: 4.139 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 3/10 - Train_Step: 462/461 | runing_loss: 3.9681 | Metrics: | train_loss: 3.968 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 4/10 - Train_Step: 462/461 | runing_loss: 3.8685 | Metrics: | train_loss: 3.869 | valid_loss: 0.000 | Epoch complete in: 7m 41s Epoch 5/10 - Train_Step: 462/461 | runing_loss: 3.7355 | Metrics: | train_loss: 3.735 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 6/10 - Train_Step: 462/461 | runing_loss: 3.6689 | Metrics: | train_loss: 3.669 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 7/10 - Train_Step: 462/461 | runing_loss: 3.5943 | Metrics: | train_loss: 3.594 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 8/10 - Train_Step: 462/461 | runing_loss: 3.5301 | Metrics: | train_loss: 3.530 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 9/10 - Train_Step: 462/461 | runing_loss: 3.4526 | Metrics: | train_loss: 3.453 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 10/10 - Train_Step: 462/461 | runing_loss: 3.4006 | Metrics: | train_loss: 3.401 | valid_loss: 0.000 | Epoch complete in: 7m 40s . show_result(unet, train_ds) . learner.optimizer = optim.Adam(lr=0.0005, params=unet.parameters()) learner.fit(num_epochs=10) . Epoch 1/10 - Train_Step: 462/461 | runing_loss: 3.2657 | Metrics: | train_loss: 3.266 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 2/10 - Train_Step: 462/461 | runing_loss: 3.2039 | Metrics: | train_loss: 3.204 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 3/10 - Train_Step: 462/461 | runing_loss: 3.1593 | Metrics: | train_loss: 3.159 | valid_loss: 0.000 | Epoch complete in: 7m 39s Epoch 4/10 - Train_Step: 462/461 | runing_loss: 3.1093 | Metrics: | train_loss: 3.109 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 5/10 - Train_Step: 462/461 | runing_loss: 3.0788 | Metrics: | train_loss: 3.079 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 6/10 - Train_Step: 462/461 | runing_loss: 3.0413 | Metrics: | train_loss: 3.041 | valid_loss: 0.000 | Epoch complete in: 7m 39s Epoch 7/10 - Train_Step: 462/461 | runing_loss: 3.0058 | Metrics: | train_loss: 3.006 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 8/10 - Train_Step: 462/461 | runing_loss: 2.9876 | Metrics: | train_loss: 2.988 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 9/10 - Train_Step: 462/461 | runing_loss: 2.9476 | Metrics: | train_loss: 2.948 | valid_loss: 0.000 | Epoch complete in: 7m 39s Epoch 10/10 - Train_Step: 462/461 | runing_loss: 2.9053 | Metrics: | train_loss: 2.905 | valid_loss: 0.000 | Epoch complete in: 7m 40s . show_result(unet, train_ds) . learner.optimizer = optim.Adam(lr=0.0003, params=unet.parameters()) learner.fit(num_epochs=10) . Epoch 1/10 - Train_Step: 462/461 | runing_loss: 2.8315 | Metrics: | train_loss: 2.832 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 2/10 - Train_Step: 462/461 | runing_loss: 2.7991 | Metrics: | train_loss: 2.799 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 3/10 - Train_Step: 462/461 | runing_loss: 2.7697 | Metrics: | train_loss: 2.770 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 4/10 - Train_Step: 462/461 | runing_loss: 2.7457 | Metrics: | train_loss: 2.746 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 5/10 - Train_Step: 462/461 | runing_loss: 2.7218 | Metrics: | train_loss: 2.722 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 6/10 - Train_Step: 462/461 | runing_loss: 2.7079 | Metrics: | train_loss: 2.708 | valid_loss: 0.000 | Epoch complete in: 7m 39s Epoch 7/10 - Train_Step: 462/461 | runing_loss: 2.6901 | Metrics: | train_loss: 2.690 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 8/10 - Train_Step: 462/461 | runing_loss: 2.6693 | Metrics: | train_loss: 2.669 | valid_loss: 0.000 | Epoch complete in: 7m 40s Epoch 9/10 - Train_Step: 462/461 | runing_loss: 2.6529 | Metrics: | train_loss: 2.653 | valid_loss: 0.000 | Epoch complete in: 7m 39s Epoch 10/10 - Train_Step: 462/461 | runing_loss: 2.6438 | Metrics: | train_loss: 2.644 | valid_loss: 0.000 | Epoch complete in: 7m 40s . show_result(unet, train_ds) . Unfreeze Encoder and more epochs | . for p in encoder.parameters(): p.requires_grad = True learner.optimizer = optim.Adam(lr=0.00005, params=unet.parameters()) learner.fit(num_epochs=7) . Epoch 1/ 7 - Train_Step: 462/461 | runing_loss: 2.6152 | Metrics: | train_loss: 2.615 | valid_loss: 0.000 | Epoch complete in: 8m 44s Epoch 2/ 7 - Train_Step: 462/461 | runing_loss: 2.6022 | Metrics: | train_loss: 2.602 | valid_loss: 0.000 | Epoch complete in: 8m 44s Epoch 3/ 7 - Train_Step: 462/461 | runing_loss: 2.5660 | Metrics: | train_loss: 2.566 | valid_loss: 0.000 | Epoch complete in: 8m 44s Epoch 4/ 7 - Train_Step: 462/461 | runing_loss: 2.5444 | Metrics: | train_loss: 2.544 | valid_loss: 0.000 | Epoch complete in: 8m 44s Epoch 5/ 7 - Train_Step: 462/461 | runing_loss: 2.5375 | Metrics: | train_loss: 2.538 | valid_loss: 0.000 | Epoch complete in: 8m 44s Epoch 6/ 7 - Train_Step: 420/461 | runing_loss: 2.5235 . Result . show_result(unet, train_ds) . show_result(unet, train_ds) . show_result(unet, train_ds) . show_result(unet, train_ds) . Tensorflow 2.0 (Keras) . install segmentation-models for create U-net base on Pre-trianed Encoder like Resnet, Mobilenet , etc | . | . !pip install -q -U segmentation-models %env SM_FRAMEWORK=tf.keras . |████████████████████████████████| 51kB 7.4MB/s env: SM_FRAMEWORK=tf.keras . import tensorflow as tf from tensorflow import keras import segmentation_models as sm keras.backend.set_image_data_format(&#39;channels_last&#39;) auto = tf.data.experimental.AUTOTUNE . Segmentation Models: using `tf.keras` framework. . Create TF Dataset . low_res = glob(f&#39;{path_low}/*.jpg&#39;) high_res = glob(f&#39;{path_high}/*.jpg&#39;) # create dataset # high res as x and low res as y train_ds = tf.data.Dataset.from_tensor_slices((low_res, high_res)) for x, y in train_ds.take(1): print(x) print(y) . tf.Tensor(b&#39;./oxford-iiit-pet/low_res/boxer_135.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;./oxford-iiit-pet/high_res/boxer_135.jpg&#39;, shape=(), dtype=string) . preprocessing functions read image from file and convert to float32 | image normalization with divide by 255 | . | . def load_image(path): raw = tf.io.read_file(path) # read byte file img = tf.image.decode_image(raw, 3, expand_animations=False) # decode to jpg return tf.cast(img, tf.float32) def normalize(img): return tf.divide(img, 255.) # all in one def prepair(high_path, low_path): high, low = load_image(high_path), load_image(low_path) high, low = normalize(high), normalize(low) return (high, low) . create dataset for training | . train_ds = train_ds.map(prepair) train_ds = train_ds.shuffle(512) # if you have small GPU decrease batch_size train_ds = train_ds.batch(16, drop_remainder=True) train_ds = train_ds.prefetch(buffer_size=auto) # show 1 example x, y = list(train_ds.take(1))[0] . def denormalize(inp): inp = np.clip(inp, 0, 1) return inp . show an example | . plt.figure(dpi=300) plt.subplot(1, 2, 1) plt.imshow(denormalize(x[2])) plt.axis(&#39;off&#39;) plt.title(&#39;low res&#39;) plt.subplot(1, 2, 2) plt.imshow(denormalize(y[2])) plt.axis(&#39;off&#39;) plt.title(&#39;original&#39;) plt.show() . Pre-trained Model for Feature Extraction . VGG19 as pre-trained model for extract feature maps | . from tensorflow.keras.applications import VGG19 disc = VGG19(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3)) # freeze disc.trainable = False . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 80142336/80134624 [==============================] - 1s 0us/step . find index of layers before MaxPool2d (before shrink feature maps) | . layer_ids = [] for i, layer in enumerate(disc.layers): if isinstance(layer, keras.layers.MaxPool2D): layer_ids.append(i-1) # before MaxPool # which layers do you want to use for Feature loss layer_ids[-3:] # we need 4 feature maps for Feature Loss (it&#39;s like a hyper parameter) . [10, 15, 20] . StoreModel: StoreModel: in order to extract feature maps on layers 10, 15, 20 from VGG19 | . class StoreModel(keras.Model): &quot;&quot;&quot; get a Sequential Model and index of layers from that model return output of model on those layers layer_ids: which layers do you want to use for Feature loss &quot;&quot;&quot; def __init__(self, model, layer_ids): super(StoreModel, self).__init__() self.model = model self.layer_ids = layer_ids def call(self, x): storage = [] for i, layer in enumerate(self.model.layers): x = layer(x, training=False) # keep feature maps of layer 10, 15, 20 for this input and return them as a list if i in self.layer_ids: storage.append(x) return storage . Feature Loss . L1 Loss with TF | . def L1_loss(y_true,y_pre): return tf.reduce_mean(tf.abs(y_true-y_pre)) . gram matrix with TF | . def gram_matrix(x): n,h,w,c= x.shape x = tf.reshape(x, (n, c, -1)) return (x@tf.transpose(x, perm=[0, 2, 1]))/(3*size*size) . custom loss function: for comoute feature losses | . feature_extractor = StoreModel(disc, layer_ids=layer_ids[-3:]) # how much important these layers (a list of weights) layer_weights = [6, 12, 4] def feature_loss(reconstruction, target): target_features = feature_extractor(target) # get output of layers in VGG19 for original image recon_features = feature_extractor(reconstruction) # get output of layers in VGG19 for reconstruction image # compute normal loss for x, x&#39; in AutoEncoders loss = L1_loss(target, reconstruction) # plus difference between target feature maps and reconstruction feature maps for yf, xf, w in zip(target_features, recon_features, layer_weights): loss += L1_loss(yf, xf)*w loss += L1_loss(gram_matrix(yf), gram_matrix(xf))*w**2 return loss . Training . def unfreeze(model): for layer in model.layers: if not isinstance(layer, keras.layers.BatchNormalization): layer.trainable = True . show_result: random example for model test plot [low-res-image, output-of-model, original-image] | . | . def show_result(model, train_ds): batch = list(train_ds.take(1))[0] idx = random.randint(0, len(batch)) x, y = batch p = model.predict(x) plt.figure(dpi=300) plt.subplot(1, 3, 1) plt.title(&#39;low res&#39;) plt.imshow(denormalize(x[idx])) plt.axis(&#39;off&#39;) plt.subplot(1, 3, 2) plt.title(&#39;prediction&#39;) plt.imshow(denormalize(p[idx])) plt.axis(&#39;off&#39;) plt.subplot(1, 3, 3) plt.title(&#39;original&#39;) plt.imshow(denormalize(y[idx])) plt.axis(&#39;off&#39;) plt.show() . model = sm.Unet(&#39;resnet50&#39;, encoder_weights=&#39;imagenet&#39;, encoder_freeze=True, decoder_block_type=&#39;upsampling&#39;, classes=3, input_shape=(224, 224, 3)) optim = keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optim, loss=feature_loss) # model.summary() . Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5 94593024/94592056 [==============================] - 4s 0us/step . model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 251s 509ms/step - loss: 27.8639 Epoch 2/10 461/461 [==============================] - 238s 514ms/step - loss: 15.1287 Epoch 3/10 461/461 [==============================] - 238s 514ms/step - loss: 14.1798 Epoch 4/10 461/461 [==============================] - 237s 512ms/step - loss: 13.5909 Epoch 5/10 461/461 [==============================] - 237s 513ms/step - loss: 13.1782 Epoch 6/10 461/461 [==============================] - 237s 513ms/step - loss: 12.8685 Epoch 7/10 461/461 [==============================] - 237s 513ms/step - loss: 12.6065 Epoch 8/10 461/461 [==============================] - 237s 513ms/step - loss: 12.5048 Epoch 9/10 461/461 [==============================] - 237s 512ms/step - loss: 12.3209 Epoch 10/10 461/461 [==============================] - 237s 512ms/step - loss: 12.1943 . &lt;tensorflow.python.keras.callbacks.History at 0x7f3224135828&gt; . optim = keras.optimizers.Adam(learning_rate=5e-4) model.compile(optimizer=optim, loss=feature_loss) model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 242s 512ms/step - loss: 11.8937 Epoch 2/10 461/461 [==============================] - 237s 512ms/step - loss: 11.7142 Epoch 3/10 461/461 [==============================] - 237s 512ms/step - loss: 11.7355 Epoch 4/10 461/461 [==============================] - 238s 513ms/step - loss: 11.5630 Epoch 5/10 461/461 [==============================] - 237s 512ms/step - loss: 11.4652 Epoch 6/10 461/461 [==============================] - 237s 513ms/step - loss: 11.4185 Epoch 7/10 461/461 [==============================] - 237s 513ms/step - loss: 11.3118 Epoch 8/10 461/461 [==============================] - 237s 512ms/step - loss: 11.2718 Epoch 9/10 461/461 [==============================] - 237s 512ms/step - loss: 11.2404 Epoch 10/10 461/461 [==============================] - 237s 512ms/step - loss: 11.1460 . &lt;tensorflow.python.keras.callbacks.History at 0x7f3210da42e8&gt; . optim = keras.optimizers.Adam(learning_rate=3e-4) model.compile(optimizer=optim, loss=feature_loss) model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 243s 513ms/step - loss: 11.0292 Epoch 2/10 461/461 [==============================] - 237s 513ms/step - loss: 10.9282 Epoch 3/10 461/461 [==============================] - 237s 512ms/step - loss: 10.8952 Epoch 4/10 461/461 [==============================] - 237s 513ms/step - loss: 10.8448 Epoch 5/10 461/461 [==============================] - 238s 513ms/step - loss: 10.7893 Epoch 6/10 461/461 [==============================] - 237s 513ms/step - loss: 10.8093 Epoch 7/10 461/461 [==============================] - 238s 514ms/step - loss: 10.7328 Epoch 8/10 461/461 [==============================] - 237s 512ms/step - loss: 10.7469 Epoch 9/10 461/461 [==============================] - 238s 514ms/step - loss: 10.6469 Epoch 10/10 461/461 [==============================] - 237s 513ms/step - loss: 10.6894 . &lt;tensorflow.python.keras.callbacks.History at 0x7f321db3b240&gt; . show_result(model, train_ds) . optim = keras.optimizers.Adam(learning_rate=1e-4) model.compile(optimizer=optim, loss=feature_loss) model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 243s 514ms/step - loss: 10.4858 Epoch 2/10 461/461 [==============================] - 237s 513ms/step - loss: 10.4373 Epoch 3/10 461/461 [==============================] - 237s 513ms/step - loss: 10.4189 Epoch 4/10 461/461 [==============================] - 237s 513ms/step - loss: 10.4104 Epoch 5/10 461/461 [==============================] - 237s 512ms/step - loss: 10.3674 Epoch 6/10 461/461 [==============================] - 237s 512ms/step - loss: 10.3773 Epoch 7/10 461/461 [==============================] - 237s 513ms/step - loss: 10.3616 Epoch 8/10 461/461 [==============================] - 237s 512ms/step - loss: 10.3594 Epoch 9/10 461/461 [==============================] - 237s 512ms/step - loss: 10.3128 Epoch 10/10 461/461 [==============================] - 237s 512ms/step - loss: 10.3244 . &lt;tensorflow.python.keras.callbacks.History at 0x7f321ce5c0f0&gt; . show_result(model, train_ds) . unfreeze(model) optim = keras.optimizers.Adam(learning_rate=5e-5) model.compile(optimizer=optim, loss=feature_loss) model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 267s 565ms/step - loss: 10.4494 Epoch 2/10 461/461 [==============================] - 261s 565ms/step - loss: 10.3516 Epoch 3/10 461/461 [==============================] - 261s 564ms/step - loss: 10.3099 Epoch 4/10 461/461 [==============================] - 262s 566ms/step - loss: 10.3026 Epoch 5/10 461/461 [==============================] - 261s 565ms/step - loss: 10.2560 Epoch 6/10 461/461 [==============================] - 262s 566ms/step - loss: 10.2331 Epoch 7/10 461/461 [==============================] - 262s 566ms/step - loss: 10.3670 Epoch 8/10 461/461 [==============================] - 261s 565ms/step - loss: 10.2048 Epoch 9/10 461/461 [==============================] - 261s 565ms/step - loss: 10.1329 Epoch 10/10 461/461 [==============================] - 262s 566ms/step - loss: 10.0836 . &lt;tensorflow.python.keras.callbacks.History at 0x7f32103ad390&gt; . optim = keras.optimizers.Adam(learning_rate=2e-5) model.compile(optimizer=optim, loss=feature_loss) model.fit(train_ds, epochs=10) . Epoch 1/10 461/461 [==============================] - 267s 566ms/step - loss: 9.9702 Epoch 2/10 461/461 [==============================] - 262s 566ms/step - loss: 9.9443 Epoch 3/10 461/461 [==============================] - 262s 566ms/step - loss: 9.8974 Epoch 4/10 461/461 [==============================] - 262s 566ms/step - loss: 9.8965 Epoch 5/10 461/461 [==============================] - 262s 566ms/step - loss: 9.8268 Epoch 6/10 461/461 [==============================] - 262s 567ms/step - loss: 9.8079 Epoch 7/10 461/461 [==============================] - 262s 566ms/step - loss: 9.7809 Epoch 8/10 461/461 [==============================] - 262s 566ms/step - loss: 9.7671 Epoch 9/10 461/461 [==============================] - 262s 566ms/step - loss: 9.8018 Epoch 10/10 461/461 [==============================] - 262s 567ms/step - loss: 9.7488 . &lt;tensorflow.python.keras.callbacks.History at 0x7f2fee9a9da0&gt; . Result . show_result(model, train_ds) . show_result(model, train_ds) . show_result(model, train_ds) . show_result(model, train_ds) . Contribution . thanks for your attention | write my mistake or your idea in the comments section | .",
            "url": "https://sajjjadayobi.github.io/blog/vision/implementation/2021/01/27/Image-Super-Res.html",
            "relUrl": "/vision/implementation/2021/01/27/Image-Super-Res.html",
            "date": " • Jan 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Text Classification with Albert-Persian",
            "content": "What&#39;s ALBERT-Persian? . A Lite BERT for Self-supervised Learning of Language Representations for the Persian Language . thanks Mehrdad Farahani &amp; Hoshvare for sharing this albert-persian repo | . ALBERT-Persian trained on a massive amount of public corpora (Persian Wikidumps, MirasText) and six other manually crawled text data from a various type of websites (BigBang Page scientific, Chetor lifestyle, Eligasht itinerary, Digikala, Ted Talks general conversational, Books novels, storybooks, short stories from old to the contemporary era). . Dataset (DigiMag) . For this notebook, I&#39;m going to use DigiMag dataset for text classification train len: 6865 , valid len:767 , test len: 852 | it has 7 types for Magazines (7 classes) | thanks Hooshvare for sharing this | . | . this is an example of how to use (You can use whatever dataset you have) | . the training was on Google Colab | . !nvidia-smi . Thu Feb 18 05:45:45 2021 +--+ | NVIDIA-SMI 460.39 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 32C P8 27W / 149W | 0MiB / 11441MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !pip install -q sentencepiece !pip install -q transformers !pip install -q tokenizers # for text processing !pip -q install hazm !pip -q install clean-text[gpl] . |████████████████████████████████| 1.2MB 5.9MB/s |████████████████████████████████| 1.8MB 6.1MB/s |████████████████████████████████| 3.2MB 40.2MB/s |████████████████████████████████| 890kB 38.7MB/s Building wheel for sacremoses (setup.py) ... done |████████████████████████████████| 317kB 5.0MB/s |████████████████████████████████| 1.4MB 5.4MB/s |████████████████████████████████| 235kB 17.9MB/s Building wheel for nltk (setup.py) ... done Building wheel for libwapiti (setup.py) ... done |████████████████████████████████| 71kB 3.3MB/s |████████████████████████████████| 133kB 7.0MB/s |████████████████████████████████| 245kB 6.9MB/s Building wheel for ftfy (setup.py) ... done . import pandas as pd import numpy as np import plotly.graph_objects as go import matplotlib.pyplot as plt import seaborn as sns from tqdm.notebook import tqdm from hazm import Normalizer from hazm import WordTokenizer from cleantext import clean import re . Load . the dataset is here on Drive (open it with VPN) | add it on your drive and use it like the following | . from google.colab import drive drive.mount(&#39;/gdrive&#39;) %cd /gdrive/MyDrive . Mounted at /gdrive /gdrive/MyDrive . !unzip digimag.zip . Archive: digimag.zip creating: digimag/ inflating: digimag/dev.csv inflating: digimag/train.csv inflating: digimag/test.csv . load train, test, and valid with Pandas | . train_df = pd.read_csv(&#39;digimag/train.csv&#39;, delimiter=&quot; &quot;, index_col=False) eval_df = pd.read_csv(&#39;digimag/dev.csv&#39;, delimiter=&quot; &quot;, index_col=False) test_df = pd.read_csv(&#39;digimag/test.csv&#39;, delimiter=&quot; &quot;, index_col=False) # drop the label columns train_df.drop(columns=[&#39;Unnamed: 0&#39;, &#39;label&#39;], inplace=True) eval_df.drop(columns=[&#39;Unnamed: 0&#39;, &#39;label&#39;], inplace=True) test_df.drop(columns=[&#39;Unnamed: 0&#39;, &#39;label&#39;], inplace=True) train_df.head() . content label_id . 0 نمایش تبلیغ در لاک‌اسکرین تعدادی از گوشی‌های ه... | 3 | . 1 شکست Justice League در باکس آفیس پس از بازخورد... | 5 | . 2 کلاسیک بینی؛ همه چیز در یک شب اتفاق افتاد فیلم... | 5 | . 3 اپل دوباره سراغ رنده رفته چراکه آپگرید کردن سط... | 3 | . 4 بررسی جزء به جزء بهترین بخش Ori and the Blind ... | 0 | . Normalization (Preprocessing) . Our cleaning method includes these steps: . fixing unicodes | removing specials like a phone number, email, url, new lines, ... | cleaning HTMLs | normalizer | tokenize and detokenize (for adding space) | . tokenizer = WordTokenizer(join_verb_parts=False, replace_hashtags=False, replace_IDs=False) normalizer = Normalizer(remove_extra_spaces=True, persian_numbers=False, persian_style=True, punctuation_spacing=False, remove_diacritics=True, affix_spacing=False, token_based=True) def cleaning(text): text = clean(text, fix_unicode=True, to_ascii=False, lower=True, no_urls=True, no_emails=True, no_phone_numbers=True, no_numbers=True, no_digits=True, no_currency_symbols=True, no_punct=True, replace_with_url=&quot;&quot;, replace_with_email=&quot;&quot;, replace_with_phone_number=&quot;&quot;, replace_with_number=&quot;&quot;, replace_with_currency_symbol=&quot;&quot;) text = re.sub(r&#39;([ا-ی]) 1{2,}&#39;, r&#39; 1&#39;, text) # bbb+ -&gt; b at least 2 char return text.strip() def text_preprocessor(t, tokenize=False): tokens = tokenizer.tokenize(cleaning(normalizer.normalize(t))) return tokens if tokenize else &#39; &#39;.join(tokens) . test_df[&#39;cleaned&#39;] = test_df.content.apply(text_preprocessor) print(&#39;test cleaned&#39;) eval_df[&#39;cleaned&#39;] = eval_df.content.apply(text_preprocessor) print(&#39;valid cleaned&#39;) . test cleaned valid cleaned . train_df[&#39;cleaned&#39;] = train_df.content.apply(text_preprocessor) print(&#39;train cleaned&#39;) train_df[&#39;tokens&#39;] = train_df.content.apply(text_preprocessor, tokenize=True) train_df.head(3) . train cleaned . content label_id cleaned tokens . 0 نمایش تبلیغ در لاک‌اسکرین تعدادی از گوشی‌های ه... | 3 | نمایش تبلیغ در لاک اسکرین تعدادی از گوشی های ه... | [نمایش, تبلیغ, در, لاک, اسکرین, تعدادی, از, گو... | . 1 شکست Justice League در باکس آفیس پس از بازخورد... | 5 | شکست justice league در باکس آفیس پس از بازخورد... | [شکست, justice, league, در, باکس, آفیس, پس, از... | . 2 کلاسیک بینی؛ همه چیز در یک شب اتفاق افتاد فیلم... | 5 | کلاسیک بینی همه چیز در یک شب اتفاق افتاد فیلم ... | [کلاسیک, بینی, همه, چیز, در, یک, شب, اتفاق, اف... | . display the frequent words in the train set . word_list = [] for s in train_df.tokens: for w in s: word_list.append(w) words = pd.DataFrame(word_list, columns=[&#39;words&#39;]) top = words.groupby([&#39;words&#39;]).size().sort_values(ascending=False) print(&#39;number of words&#39;, len(top.index)) plt.figure(figsize = (8,12)) sns.barplot(x=top.values[:30], y=top.index[:30]); . number of words 80154 . Finding the max len . The contents have different lengths based on words! Detecting the most normal range could help us find the maximum length of the sequences for the preprocessing step. On the other hand, we suppose that the minimum word combination for having a meaningful article for our learning process is 50. . train_df[&#39;words_len&#39;] = train_df.tokens.apply(lambda t: len(t)) . def data_gl_than(data, less_than=100.0, greater_than=0.0, col=&#39;words_len&#39;): data_length = data[col].values data_glt = sum([1 for length in data_length if greater_than &lt; length &lt;= less_than]) data_glt_rate = (data_glt / len(data_length)) * 100 print(f&#39;Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!&#39;) data_gl_than(train_df, 1e4, 50) . Texts with word length of greater than 50 and less than 10000.0 includes 99.55% of the whole! . fig = go.Figure() fig.add_trace(go.Histogram(x=train_df[&#39;words_len&#39;])) fig.update_layout( title_text=&#39;Distribution of word counts within comments&#39;, xaxis_title_text=&#39;Word Count&#39;, yaxis_title_text=&#39;Frequency&#39;, bargap=0.2, bargroupgap=0.2) fig.show() . . . minlim, maxlim = 50, 1e4 # remove comments with the length of fewer than 50 words print(&#39;size of the data before remove: &#39;, len(train_df)) train_df[&#39;words_len&#39;] = train_df[&#39;words_len&#39;].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else None) train_df = train_df.dropna(subset=[&#39;words_len&#39;]) train_df = train_df.reset_index(drop=True) print(&#39;size of the data after remove: &#39;, len(train_df)) . size of the data before remove: 6896 size of the data after remove: 6865 . Distribution of Classes . fig = go.Figure() topic_freq = train_df.label_id.value_counts() fig.add_trace(go.Bar(y=topic_freq, x=topic_freq.index.to_numpy())) fig.update_layout( title_text=&#39;Distribution of each class&#39;, xaxis_title_text=&#39;classes&#39;, yaxis_title_text=&#39;Frequency&#39;, bargap=0.2, bargroupgap=0.2) fig.show() . . . Prepare data for Albert-LM . using transformers tokenizer | tiny and fast BERT-ish Language Model (just 70MB) the pre-trained model is from Merhdad Farhani | . | . this is the latest version from official repository | the new version removes half-space and newlines in its tokenizer (you don&#39;t have to handle it in your cleaning) | . batch_size = 16 max_len = 256 class_number = 7 # this is the latest version from official repository model_name = &quot;HooshvareLab/albert-fa-zwnj-base-v2&quot; . from transformers import AlbertTokenizerFast tokenizer = AlbertTokenizerFast.from_pretrained(model_name) . Tokenization | . train_encodings = tokenizer(train_df.cleaned.tolist(), max_length=max_len, truncation=True, padding=True) print(&#39;train tokenized&#39;) test_encodings = tokenizer(test_df.cleaned.tolist(), max_length=max_len, truncation=True, padding=True) print(&#39;test tokenized&#39;) valid_encodings = tokenizer(eval_df.cleaned.tolist(), max_length=max_len, truncation=True, padding=True) print(&#39;valid tokenized&#39;) . train tokenized test tokenized valid tokenized . create config for our classification task | . from transformers import AutoConfig # create a dict for classes label2id = {label: i for i, label in enumerate(range(class_number))} id2label = {v: k for k, v in label2id.items()} config = AutoConfig.from_pretrained(model_name,**{&#39;label2id&#39;: label2id, &#39;id2label&#39;: id2label}) print(f&#39;label2id: {label2id}&#39;) print(f&#39;id2label: {id2label}&#39;) . label2id: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6} id2label: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6} . Training . PyTorch . import torch from torch import optim from torch.utils.data import DataLoader from transformers import AlbertForSequenceClassification # set device device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;) . class DigiMagDs(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) # create datasets train_ds = DigiMagDs(train_encodings, train_df.label_id) valid_ds = DigiMagDs(valid_encodings, eval_df.label_id) test_ds = DigiMagDs(test_encodings, test_df.label_id) # create Dataloders train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4) valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True, num_workers=4) test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4) . My PyTorch Trainer | . import time, sys from IPython.display import clear_output class TorchTrainer: def __init__(self, model, train_dl, valid_dl, optimizer): self.model = model self.train_dl = train_dl self.valid_dl = valid_dl self.optimizer = optimizer self.loss_history = [] def fit(self, num_epochs): clear_output() valid_acc = 0 for epoch in range(num_epochs): print(&#39;Epoch %2d/%2d&#39; % (epoch + 1, num_epochs)) print(&#39;-&#39; * 20) t0 = time.time() train_acc = self.train_model() valid_acc = self.valid_model() time_elapsed = time.time() - t0 print(&#39; n Metrics: | train_acc: %.3f | valid_acc: %.3f |&#39; % (train_acc[0], valid_acc[0])) print(&#39; n Epoch complete in: %.0fm %.0fs n&#39; % (time_elapsed // 60, time_elapsed % 60)) return def train_model(self): self.model.train() N = len(self.train_dl.dataset) step = N // self.train_dl.batch_size avg_loss = 0.0 avg_acc = 0.0 for i, batch in enumerate(self.train_dl): self.optimizer.zero_grad() # forward input_ids = batch[&#39;input_ids&#39;].to(device) attention_mask = batch[&#39;attention_mask&#39;].to(device) labels = batch[&#39;labels&#39;].to(device) outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels) predictions = torch.argmax(outputs[&#39;logits&#39;], dim=1) # loss loss = outputs[&#39;loss&#39;] # backward loss.backward() self.optimizer.step() # statistics of model training and print avg_loss = (avg_loss * i + loss) / (i + 1) avg_acc += torch.sum(predictions == labels) self.loss_history.append(avg_loss) sys.stdout.flush() sys.stdout.write(&quot; r Train_Step: %d/%d | runing_loss: %.4f&quot; % (i + 1, step, avg_loss)) sys.stdout.flush() return torch.tensor([avg_acc]) / N def valid_model(self): print() self.model.eval() N = len(self.valid_dl.dataset) step = N // self.valid_dl.batch_size avg_loss = 0.0 avg_acc = 0.0 with torch.no_grad(): for i, batch in enumerate(self.valid_dl): input_ids = batch[&#39;input_ids&#39;].to(device) attention_mask = batch[&#39;attention_mask&#39;].to(device) labels = batch[&#39;labels&#39;].to(device) outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels) predictions = torch.argmax(outputs[&#39;logits&#39;], dim=1) loss = outputs[&#39;loss&#39;] avg_loss = (avg_loss * i + loss) / (i + 1) avg_acc += torch.sum(predictions == labels) sys.stdout.flush() sys.stdout.write(&quot; r Valid_Step: %d/%d | runing_loss: %.4f&quot; % (i, step, avg_loss)) sys.stdout.flush() return torch.tensor([avg_acc]) /N . model = AlbertForSequenceClassification.from_pretrained(model_name, config=config) model.to(device) # AdamW is just adam with fixing on weight decay (don&#39;t worry about it) optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.0) . Some weights of the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 were not used when initializing AlbertForSequenceClassification: [&#39;predictions.bias&#39;, &#39;predictions.LayerNorm.weight&#39;, &#39;predictions.LayerNorm.bias&#39;, &#39;predictions.dense.weight&#39;, &#39;predictions.dense.bias&#39;, &#39;predictions.decoder.weight&#39;, &#39;predictions.decoder.bias&#39;] - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 and are newly initialized: [&#39;albert.pooler.weight&#39;, &#39;albert.pooler.bias&#39;, &#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . trainer = TorchTrainer(model, train_dl, valid_dl, optimizer=optimizer) trainer.fit(num_epochs=2) . Epoch 1/ 2 -- Train_Step: 427/426 | runing_loss: 0.3749 Valid_Step: 47/47 | runing_loss: 0.2160 Metrics: | train_acc: 0.893 | valid_acc: 0.941 | Epoch complete in: 11m 4s Epoch 2/ 2 -- Train_Step: 427/426 | runing_loss: 0.2136 Valid_Step: 47/47 | runing_loss: 0.2005 Metrics: | train_acc: 0.942 | valid_acc: 0.935 | Epoch complete in: 10m 55s . We achieve 94% just in 2 epochs and without any hyperparameters optimization | Not bad ha? | . | . Tensorflow . from transformers import TFAlbertForSequenceClassification import tensorflow as tf . creat tf_dataset | . train_ds = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_df.label_id)) train_ds = train_ds.shuffle(1024).batch(batch_size, drop_remainder=True) print(&#39;train dataset created&#39;) test_ds = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_df.label_id)) test_ds = test_ds.shuffle(512).batch(batch_size, drop_remainder=True) print(&#39;test dataset created&#39;) valid_ds = tf.data.Dataset.from_tensor_slices((dict(valid_encodings), eval_df.label_id)) valid_ds = valid_ds.shuffle(512).batch(batch_size, drop_remainder=True) print(&#39;valid dataset created&#39;) . train dataset created test dataset created valid dataset created . the original model is based on PyTorch | If you need use Albert-Persian in TF you have to use from_pt=True | . model = TFAlbertForSequenceClassification.from_pretrained(model_name, config=config, from_pt=True) # compile model optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, clipnorm=1.0) model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[&#39;acc&#39;]) # you can also use any keras loss fn model.summary() . Model: &#34;tf_albert_for_sequence_classification_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= albert (TFAlbertMainLayer) multiple 11683584 _________________________________________________________________ dropout_18 (Dropout) multiple 0 _________________________________________________________________ classifier (Dense) multiple 5383 ================================================================= Total params: 11,688,967 Trainable params: 11,688,967 Non-trainable params: 0 _________________________________________________________________ . H = model.fit(train_ds, validation_data=valid_ds, epochs=2, batch_size=batch_size) . Epoch 1/2 426/426 [==============================] - 920s 2s/step - loss: 0.5785 - acc: 0.8232 - val_loss: 0.2331 - val_acc: 0.9295 Epoch 2/2 426/426 [==============================] - 875s 2s/step - loss: 0.2243 - acc: 0.9400 - val_loss: 0.1981 - val_acc: 0.9348 . metrics = model.evaluate(test_ds) print(&#39;test loss: &#39;, metrics[0]) print(&#39;test acc: &#39;, metrics[1]) . 53/53 [==============================] - 30s 562ms/step - loss: 0.2437 - acc: 0.9387 test loss: 0.2437356561422348 test acc: 0.9386792182922363 . How can we achieve better results . with hyper parameters tuning more epoch | different batch size | bigger max len | ... | . | . HuggingFace &#129303; . you can also train it with HuggingFace Trainer &amp; TFTrainer | for more details see this | . In The End . thanks for your attention | sorry, my English is not great but you can get the idea | write your comments and ideas below and say about my mistakes or ... | . | .",
            "url": "https://sajjjadayobi.github.io/blog/nlp/jupyter/2021/01/23/Text-Classification-Albert.html",
            "relUrl": "/nlp/jupyter/2021/01/23/Text-Classification-Albert.html",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Custom Training in Keras & TF 2.X",
            "content": "with this new syntax from Keras, you can write a complex training loop using model subclassing | . Example . Cifar10 with Resnetish Model | . import tensorflow as tf from tensorflow.keras import layers . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 y_train = tf.keras.utils.to_categorical(y_train) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 58s 0us/step . create new layer in tf | . class ResBlock(tf.keras.layers.Layer): def __init__(self): super(ResBlock, self).__init__() self.c1 = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;) self.c2 = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;) # forward step def call(self, inputs): x1 = self.c1(inputs) x2 = self.c2(x1+inputs) return x2 . create new model in tf | you need to override the train_step function | . class Resnet18(tf.keras.Model): def __init__(self, n_class=10): super(Resnet18, self).__init__() self.first_conv = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;) self.blocks = tf.keras.Sequential([ResBlock(), ResBlock()]) self.faltten = layers.Flatten() self.fc = layers.Dense(n_class, activation=&#39;softmax&#39;) # forward step def call(self, x): x = self.first_conv(x) x = self.blocks(x) x = self.fc(self.faltten(x)) return x # one batch train def train_step(self, data): x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.compiled_metrics.update_state(y, y_pred) return {m.name: m.result() for m in self.metrics} # one batch test def test_step(self, data): x, y = data y_pred = self(x, training=False) self.compiled_loss(y, y_pred, regularization_losses=self.losses) self.compiled_metrics.update_state(y, y_pred) return {m.name: m.result() for m in self.metrics} # other usful functions def compile(self): pass def metrics(self): pass . training | . model = Resnet18() model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) H = model.fit(x_train, y_train, epochs=5, batch_size=64) . Epoch 1/5 782/782 [==============================] - 427s 545ms/step - loss: 1.6727 - accuracy: 0.4035 Epoch 2/5 782/782 [==============================] - 415s 531ms/step - loss: 1.0652 - accuracy: 0.6288 Epoch 3/5 782/782 [==============================] - 415s 531ms/step - loss: 0.8133 - accuracy: 0.7210 Epoch 4/5 782/782 [==============================] - 417s 533ms/step - loss: 0.6389 - accuracy: 0.7817 Epoch 5/5 782/782 [==============================] - 417s 534ms/step - loss: 0.4888 - accuracy: 0.8329 . Template . you don&#39;t have to write your training loop from scrach | . import tenserflow as tf class Learner(tf.keras.Model): def train_step(self, data): # you have anything that passed to tf.Data x, y = data with tf.GradientTape() as tape: y_pred = self(x, training=True) loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # often you don&#39;t change the rest gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.compiled_metrics.update_state(y, y_pred) return {m.name: m.result() for m in self.metrics} def test_step(self, data): x, y = data y_pred = self(x, training=False) self.compiled_loss(y, y_pred, regularization_losses=self.losses) self.compiled_metrics.update_state(y, y_pred) return {m.name: m.result() for m in self.metrics} def predict_step(self, x): pass model = # builded model(Sequential, Functional, Application, Model-Subclassing) learner = Learner(model.inputs, model.outputs) learner.compile(optimizer=, loss=, metrics=) # compile model learner.fit() # enjoy the abilitis of keras.fit :) .",
            "url": "https://sajjjadayobi.github.io/blog/tips/2020/11/23/Custom-Training-Keras.html",
            "relUrl": "/tips/2020/11/23/Custom-Training-Keras.html",
            "date": " • Nov 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m Sajjad Ayoubi | a Deep Learning Practitioner and Little computer geek | from Tehran/Iran | I’m currently working on Chatbots at Hamtech | . Connect me . Telegram: @Sajjadayobi | Twitter: @ayoubi_sajjad | . .",
          "url": "https://sajjjadayobi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sajjjadayobi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}